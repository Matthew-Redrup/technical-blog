# RBE Core


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Probability Utilities

Basic probability distribution functions following fast.ai style with
short, clear names.

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L40"
target="_blank" style="float:right; font-size:smaller">source</a>

### prob_kl_div

>  prob_kl_div (p, q)

*KL divergence from `q` to `p`*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L34"
target="_blank" style="float:right; font-size:smaller">source</a>

### prob_entropy

>  prob_entropy (probs)

*Calculate entropy of `probs` distribution*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L26"
target="_blank" style="float:right; font-size:smaller">source</a>

### prob_sample

>  prob_sample (probs, n=1, rng=None)

*Sample `n` indices from `probs` distribution*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L19"
target="_blank" style="float:right; font-size:smaller">source</a>

### prob_normalize

>  prob_normalize (probs)

*Normalize `probs` to sum to 1*

``` python
# Test probability utilities
probs = [1, 2, 3]
normalized = prob_normalize(probs)
test_close(np.sum(normalized), 1.0)
test_close(normalized, [1/6, 2/6, 3/6])

# Test sampling
rng = np.random.default_rng(42)
samples = prob_sample([0.1, 0.7, 0.2], n=1000, rng=rng)
assert len(samples) == 1000
assert np.all((samples >= 0) & (samples <= 2))

# Test entropy
uniform = [0.5, 0.5]
certain = [1.0, 0.0]
assert prob_entropy(uniform) > prob_entropy(certain)

# Test KL divergence
p = [0.5, 0.5]
q = [0.5, 0.5]
test_close(prob_kl_div(p, q), 0.0, eps=1e-10)  # Same distributions
```

## Bayesian Core

Core Bayesian inference functions for updating beliefs with evidence.

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L69"
target="_blank" style="float:right; font-size:smaller">source</a>

### bayes_posterior_predictive

>  bayes_posterior_predictive (posterior, likelihood_fn, n_samples=1000,
>                                  rng=None)

*Sample from posterior predictive distribution*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L55"
target="_blank" style="float:right; font-size:smaller">source</a>

### bayes_sequential

>  bayes_sequential (priors, likelihoods, evidences=None)

*Sequential updating of `priors` with `likelihoods` and optional
`evidences`*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L48"
target="_blank" style="float:right; font-size:smaller">source</a>

### bayes_update

>  bayes_update (prior, likelihood, evidence=None)

*Update `prior` with `likelihood` and optional `evidence`*

``` python
# Test Bayesian core functions
prior = np.array([0.3, 0.7])
likelihood = np.array([0.8, 0.2])
posterior = bayes_update(prior, likelihood)
test_close(np.sum(posterior), 1.0)
assert posterior[0] > prior[0]  # First hypothesis should increase

# Test sequential updating
priors = [0.5, 0.5]
likelihoods = [[0.9, 0.1], [0.8, 0.2], [0.7, 0.3]]
posteriors = bayes_sequential(priors, likelihoods)
assert posteriors.shape == (4, 2)  # Initial + 3 updates
test_close(np.sum(posteriors, axis=1), 1.0)  # All normalized

# Test posterior predictive (simple case)
posterior = [0.6, 0.4]
def simple_likelihood(param_idx):
    if param_idx == 0:
        return [0.8, 0.2]  # Biased toward observation 0
    else:
        return [0.3, 0.7]  # Biased toward observation 1

rng = np.random.default_rng(42)
predictions = bayes_posterior_predictive(posterior, simple_likelihood, n_samples=100, rng=rng)
assert len(predictions) == 100
assert np.all((predictions >= 0) & (predictions <= 1))
```

## Particle Filter Foundation

Core particle filter functions for Monte Carlo-based Bayesian inference.

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L152"
target="_blank" style="float:right; font-size:smaller">source</a>

### pf_step

>  pf_step (particles, weights, observation, transition_fn, likelihood_fn,
>               resample_threshold=0.5, rng=None)

*Complete particle filter step: predict, update, and conditionally
resample*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L147"
target="_blank" style="float:right; font-size:smaller">source</a>

### pf_effective_size

>  pf_effective_size (weights)

*Calculate effective sample size of normalized `weights`*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L126"
target="_blank" style="float:right; font-size:smaller">source</a>

### pf_resample

>  pf_resample (particles, weights, method='systematic', rng=None)

*Resample `particles` using `weights` with specified `method`*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L110"
target="_blank" style="float:right; font-size:smaller">source</a>

### pf_update

>  pf_update (particles, weights, observation, likelihood_fn)

*Update step: weight `particles` using `observation` and
`likelihood_fn`*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L100"
target="_blank" style="float:right; font-size:smaller">source</a>

### pf_predict

>  pf_predict (particles, weights, transition_fn, rng=None)

*Prediction step: apply `transition_fn` to `particles`*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L87"
target="_blank" style="float:right; font-size:smaller">source</a>

### pf_init

>  pf_init (n_particles, state_dim, init_fn=None, rng=None)

*Initialize particle filter with `n_particles` and `state_dim`*

``` python
# Test particle filter functions
rng = np.random.default_rng(42)

# Test initialization
particles, weights = pf_init(100, 2, rng=rng)
assert particles.shape == (100, 2)
assert len(weights) == 100
test_close(np.sum(weights), 1.0)

# Test prediction with simple transition
def simple_transition(particle, rng):
    return particle + rng.normal(0, 0.1, size=particle.shape)

new_particles, new_weights = pf_predict(particles, weights, simple_transition, rng)
assert new_particles.shape == particles.shape
test_close(new_weights, weights)  # Weights unchanged

# Test update with simple likelihood
def simple_likelihood(particle, observation):
    # Gaussian likelihood
    diff = np.linalg.norm(particle - observation)
    return np.exp(-0.5 * diff**2)

observation = np.array([0.5, 0.5])
particles, weights = pf_update(particles, weights, observation, simple_likelihood)
test_close(np.sum(weights), 1.0)

# Test resampling
particles, weights = pf_resample(particles, weights, rng=rng)
test_close(np.sum(weights), 1.0)
test_close(weights, np.ones(100)/100)  # Should be uniform after resampling

# Test effective sample size
uniform_weights = np.ones(100) / 100
skewed_weights = np.zeros(100)
skewed_weights[0] = 1.0
assert pf_effective_size(uniform_weights) > pf_effective_size(skewed_weights)
```

## RBE Estimator

Main recursive Bayesian estimator implementations.

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L225"
target="_blank" style="float:right; font-size:smaller">source</a>

### rbe_metrics

>  rbe_metrics (true_states, estimates)

*Calculate performance metrics for RBE estimates*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L204"
target="_blank" style="float:right; font-size:smaller">source</a>

### rbe_adaptive

>  rbe_adaptive (observations, transition_fn, likelihood_fn,
>                    adapt_rate=0.01, n_particles=1000, init_fn=None, rng=None)

*Adaptive RBE estimator that adjusts parameters based on performance*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L169"
target="_blank" style="float:right; font-size:smaller">source</a>

### rbe_estimator

>  rbe_estimator (observations, transition_fn, likelihood_fn,
>                     n_particles=1000, init_fn=None, rng=None)

*Main RBE estimator for `observations` with particle filter*

``` python
# Test RBE estimator functions
rng = np.random.default_rng(42)

# Generate synthetic data
true_states = [np.array([i * 0.1, i * 0.05]) for i in range(10)]
observations = [state + rng.normal(0, 0.1, 2) for state in true_states]

# Define simple transition and likelihood
def test_transition(particle, rng):
    return particle + rng.normal(0, 0.05, particle.shape)

def test_likelihood(particle, observation):
    diff = np.linalg.norm(particle - observation)
    return np.exp(-0.5 * (diff / 0.1)**2)

# Test basic estimator
result = rbe_estimator(observations, test_transition, test_likelihood, 
                      n_particles=100, rng=rng)
assert result['estimates'].shape == (10, 2)
assert len(result['particles']) == 11  # Initial + 10 steps
assert len(result['weights']) == 11

# Test adaptive estimator
adaptive_result = rbe_adaptive(observations, test_transition, test_likelihood,
                              n_particles=100, rng=rng)
assert 'adaptation_info' in adaptive_result
assert 'avg_effective_size' in adaptive_result['adaptation_info']

# Test metrics
metrics = rbe_metrics(true_states, result['estimates'])
assert 'mse' in metrics
assert 'rmse' in metrics
assert 'mae' in metrics
assert metrics['n_samples'] == 10
assert metrics['rmse'] == np.sqrt(metrics['mse'])
```

## Visualization Helpers

Helper functions for visualizing RBE results and particle filters.

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L346"
target="_blank" style="float:right; font-size:smaller">source</a>

### viz_rbe_summary

>  viz_rbe_summary (rbe_result, true_states=None, title='RBE Summary',
>                       figsize=(15, 10))

*Create comprehensive summary visualization of RBE results*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L304"
target="_blank" style="float:right; font-size:smaller">source</a>

### viz_comparison

>  viz_comparison (methods_data, time_steps=None, title='Method Comparison',
>                      figsize=(12, 8), metrics=['mse', 'mae'])

*Compare multiple methods with `methods_data` dict*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L279"
target="_blank" style="float:right; font-size:smaller">source</a>

### viz_beliefs

>  viz_beliefs (beliefs, time_steps=None, title='Belief Evolution',
>                   figsize=(10, 6), labels=None)

*Visualize evolution of `beliefs` over `time_steps`*

------------------------------------------------------------------------

<a
href="https://github.com/Matthew-Redrup/technical-blog/blob/main/technical_blog/rbe/core.py#L251"
target="_blank" style="float:right; font-size:smaller">source</a>

### viz_particles

>  viz_particles (particles, weights, title='Particle Distribution',
>                     figsize=(8, 6), alpha=0.6)

*Visualize `particles` with `weights`*

``` python
# Test visualization functions (basic functionality)
rng = np.random.default_rng(42)

# Create test data
particles = rng.normal(0, 1, (100, 2))
weights = rng.exponential(1, 100)
weights = prob_normalize(weights)

# Test particle visualization
fig, ax = viz_particles(particles, weights)
assert fig is not None
plt.close(fig)

# Test belief visualization
beliefs = np.random.random((10, 3))
fig, ax = viz_beliefs(beliefs)
assert fig is not None
plt.close(fig)

# Test comparison visualization
methods_data = {
    'Method A': {
        'estimates': np.random.random(10), 
        'mse': np.random.random(10), 
        'mae': np.random.random(10)
    },
    'Method B': {
        'estimates': np.random.random(10), 
        'mse': np.random.random(10), 
        'mae': np.random.random(10)
    }
}
fig, axes = viz_comparison(methods_data)
assert fig is not None
plt.close(fig)

# Test RBE summary visualization
# Use the RBE result from previous test
true_states = [np.array([i * 0.1, i * 0.05]) for i in range(10)]
observations = [state + rng.normal(0, 0.1, 2) for state in true_states]
result = rbe_estimator(observations, test_transition, test_likelihood, 
                      n_particles=50, rng=rng)  # Smaller for faster test

fig = viz_rbe_summary(result, true_states)
assert fig is not None
plt.close(fig)
```

## Export Functions

Define all functions to be exported from this module.

## Summary

This module provides the complete foundation for Recursive Bayesian
Estimation following fast.ai coding principles:

- **Short, clear names**:
  [`prob_normalize`](https://Matthew-Redrup.github.io/technical-blog/rbe/rbe_core.html#prob_normalize),
  [`bayes_update`](https://Matthew-Redrup.github.io/technical-blog/rbe/rbe_core.html#bayes_update),
  [`pf_init`](https://Matthew-Redrup.github.io/technical-blog/rbe/rbe_core.html#pf_init)
- **Function-first approach**: Minimal classes, comprehensive functions
- **Liberal imports**: Full `__all__` definition for
  `from rbe.core import *`
- **Inline testing**: Comprehensive tests using `test_eq` and
  `test_close`
- **Mathematical focus**: Functions optimized for interactive
  exploration
- **Modular design**: Mix-and-match functionality for different use
  cases

The module supports the entire RBE blog series with: - Core probability
and Bayesian inference functions - Complete particle filter
implementation - Ready-to-use RBE estimators - Comprehensive
visualization tools - Performance metrics and analysis functions
