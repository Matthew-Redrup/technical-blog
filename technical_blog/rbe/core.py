"""Core functions for Recursive Bayesian Estimation - probability utilities, Bayesian inference, particle filters, and visualization helpers"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/rbe/00_rbe_core.ipynb.

# %% auto 0
__all__ = ['prob_normalize', 'prob_sample', 'prob_entropy', 'prob_kl_div', 'bayes_update', 'bayes_sequential',
           'bayes_posterior_predictive', 'pf_init', 'pf_predict', 'pf_update', 'pf_resample', 'pf_effective_size',
           'pf_step', 'rbe_estimator', 'rbe_adaptive', 'rbe_metrics', 'viz_particles', 'viz_beliefs', 'viz_comparison',
           'viz_rbe_summary']

# %% ../../nbs/rbe/00_rbe_core.ipynb 3
import numpy as np
import matplotlib.pyplot as plt
from typing import Optional, Callable, Tuple, List, Union
from fastcore.test import test_eq, test_close
from fastcore.all import *

# %% ../../nbs/rbe/00_rbe_core.ipynb 5
def prob_normalize(probs):
    "Normalize `probs` to sum to 1"
    probs = np.asarray(probs)
    return probs / np.sum(probs)

def prob_sample(probs, n=1, rng=None):
    "Sample `n` indices from `probs` distribution"
    if rng is None: rng = np.random.default_rng()
    probs = prob_normalize(probs)
    return rng.choice(len(probs), size=n, p=probs)

def prob_entropy(probs):
    "Calculate entropy of `probs` distribution"
    probs = prob_normalize(probs)
    probs = probs[probs > 0]  # Remove zeros to avoid log(0)
    return -np.sum(probs * np.log2(probs))

def prob_kl_div(p, q):
    "KL divergence from `q` to `p`"
    p, q = prob_normalize(p), prob_normalize(q)
    # Add small epsilon to avoid log(0)
    eps = 1e-10
    return np.sum(p * np.log((p + eps) / (q + eps)))

# %% ../../nbs/rbe/00_rbe_core.ipynb 8
def bayes_update(prior, likelihood, evidence=None):
    "Update `prior` with `likelihood` and optional `evidence`"
    prior, likelihood = np.array(prior), np.array(likelihood)
    if evidence is None: evidence = (prior * likelihood).sum()
    if evidence == 0: raise ValueError("Impossible observation")
    return (prior * likelihood) / evidence

def bayes_sequential(priors, likelihoods, evidences=None):
    "Sequential updating of `priors` with `likelihoods` and optional `evidences`"
    if evidences is None:
        evidences = [None] * len(likelihoods)
    
    posterior = np.array(priors)
    posteriors = [posterior.copy()]
    
    for likelihood, evidence in zip(likelihoods, evidences):
        posterior = bayes_update(posterior, likelihood, evidence)
        posteriors.append(posterior.copy())
    
    return np.array(posteriors)

def bayes_posterior_predictive(posterior, likelihood_fn, n_samples=1000, rng=None):
    "Sample from posterior predictive distribution"
    if rng is None: rng = np.random.default_rng()
    
    # Sample parameter values from posterior
    param_samples = prob_sample(posterior, n_samples, rng)
    
    # Generate predictions for each parameter sample
    predictions = []
    for param_idx in param_samples:
        # likelihood_fn should return a distribution over observations
        obs_dist = likelihood_fn(param_idx)
        obs_sample = prob_sample(obs_dist, 1, rng)[0]
        predictions.append(obs_sample)
    
    return np.array(predictions)

# %% ../../nbs/rbe/00_rbe_core.ipynb 11
def pf_init(n_particles, state_dim, init_fn=None, rng=None):
    "Initialize particle filter with `n_particles` and `state_dim`"
    if rng is None: rng = np.random.default_rng()
    
    if init_fn is None:
        # Default: uniform initialization in [0, 1]
        particles = rng.uniform(0, 1, size=(n_particles, state_dim))
    else:
        particles = init_fn(n_particles, state_dim, rng)
    
    weights = np.ones(n_particles) / n_particles
    return particles, weights

def pf_predict(particles, weights, transition_fn, rng=None):
    "Prediction step: apply `transition_fn` to `particles`"
    if rng is None: rng = np.random.default_rng()
    
    new_particles = np.zeros_like(particles)
    for i, particle in enumerate(particles):
        new_particles[i] = transition_fn(particle, rng)
    
    return new_particles, weights  # Weights unchanged in prediction

def pf_update(particles, weights, observation, likelihood_fn):
    "Update step: weight `particles` using `observation` and `likelihood_fn`"
    new_weights = np.zeros_like(weights)
    
    for i, particle in enumerate(particles):
        new_weights[i] = weights[i] * likelihood_fn(particle, observation)
    
    # Normalize weights
    if np.sum(new_weights) > 0:
        new_weights = prob_normalize(new_weights)
    else:
        # If all weights are zero, reset to uniform
        new_weights = np.ones_like(weights) / len(weights)
    
    return particles, new_weights

def pf_resample(particles, weights, method='systematic', rng=None):
    "Resample `particles` using `weights` with specified `method`"
    if rng is None: rng = np.random.default_rng()
    n_particles = len(particles)
    
    if method == 'systematic':
        # Systematic resampling
        positions = (np.arange(n_particles) + rng.uniform()) / n_particles
        cum_weights = np.cumsum(weights)
        indices = np.searchsorted(cum_weights, positions)
    elif method == 'multinomial':
        # Multinomial resampling
        indices = prob_sample(weights, n_particles, rng)
    else:
        raise ValueError(f"Unknown resampling method: {method}")
    
    new_particles = particles[indices]
    new_weights = np.ones(n_particles) / n_particles
    
    return new_particles, new_weights

def pf_effective_size(weights):
    "Calculate effective sample size of `weights`"
    return 1.0 / np.sum(weights**2)

def pf_step(particles, weights, observation, transition_fn, likelihood_fn, 
           resample_threshold=0.5, rng=None):
    "Complete particle filter step: predict, update, and conditionally resample"
    # Prediction
    particles, weights = pf_predict(particles, weights, transition_fn, rng)
    
    # Update
    particles, weights = pf_update(particles, weights, observation, likelihood_fn)
    
    # Conditional resampling
    eff_size = pf_effective_size(weights)
    if eff_size < resample_threshold * len(particles):
        particles, weights = pf_resample(particles, weights, rng=rng)
    
    return particles, weights

# %% ../../nbs/rbe/00_rbe_core.ipynb 14
def rbe_estimator(observations, transition_fn, likelihood_fn, 
                 n_particles=1000, init_fn=None, rng=None):
    "Main RBE estimator for `observations` with particle filter"
    if rng is None: rng = np.random.default_rng()
    
    # Infer state dimension from first observation
    state_dim = len(observations[0]) if hasattr(observations[0], '__len__') else 1
    
    # Initialize particles
    particles, weights = pf_init(n_particles, state_dim, init_fn, rng)
    
    # Store results
    estimates = []
    particle_history = [particles.copy()]
    weight_history = [weights.copy()]
    
    for obs in observations:
        # Particle filter step
        particles, weights = pf_step(particles, weights, obs, 
                                   transition_fn, likelihood_fn, rng=rng)
        
        # Estimate (weighted mean)
        estimate = np.average(particles, weights=weights, axis=0)
        estimates.append(estimate)
        
        # Store history
        particle_history.append(particles.copy())
        weight_history.append(weights.copy())
    
    return {
        'estimates': np.array(estimates),
        'particles': particle_history,
        'weights': weight_history
    }

def rbe_adaptive(observations, transition_fn, likelihood_fn, 
                adapt_rate=0.01, n_particles=1000, init_fn=None, rng=None):
    "Adaptive RBE estimator that adjusts parameters based on performance"
    if rng is None: rng = np.random.default_rng()
    
    # Start with base estimator
    result = rbe_estimator(observations, transition_fn, likelihood_fn, 
                          n_particles, init_fn, rng)
    
    # Simple adaptation: adjust resampling threshold based on effective size
    # This is a placeholder for more sophisticated adaptation
    avg_eff_size = np.mean([pf_effective_size(w) for w in result['weights']])
    adapted_threshold = max(0.1, min(0.9, 0.5 + adapt_rate * (avg_eff_size - n_particles/2)))
    
    result['adaptation_info'] = {
        'avg_effective_size': avg_eff_size,
        'adapted_threshold': adapted_threshold
    }
    
    return result

def rbe_metrics(true_states, estimates):
    "Calculate performance metrics for RBE estimates"
    true_states = np.array(true_states)
    estimates = np.array(estimates)
    
    # Mean squared error
    mse = np.mean((true_states - estimates)**2)
    
    # Root mean squared error
    rmse = np.sqrt(mse)
    
    # Mean absolute error
    mae = np.mean(np.abs(true_states - estimates))
    
    # Maximum absolute error
    max_ae = np.max(np.abs(true_states - estimates))
    
    return {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'max_absolute_error': max_ae,
        'n_samples': len(estimates)
    }

# %% ../../nbs/rbe/00_rbe_core.ipynb 17
def viz_particles(particles, weights, title='Particle Distribution', 
                 figsize=(8, 6), alpha=0.6):
    "Visualize `particles` with `weights`"
    fig, ax = plt.subplots(figsize=figsize)
    
    if particles.shape[1] == 1:
        # 1D case: histogram
        ax.hist(particles.flatten(), weights=weights, bins=30, alpha=alpha)
        ax.set_xlabel('State')
        ax.set_ylabel('Probability Density')
    elif particles.shape[1] == 2:
        # 2D case: scatter plot
        scatter = ax.scatter(particles[:, 0], particles[:, 1], 
                           s=weights*1000, alpha=alpha)
        ax.set_xlabel('State Dimension 1')
        ax.set_ylabel('State Dimension 2')
    else:
        # Higher dimensions: just show first two
        scatter = ax.scatter(particles[:, 0], particles[:, 1], 
                           s=weights*1000, alpha=alpha)
        ax.set_xlabel('State Dimension 1')
        ax.set_ylabel('State Dimension 2')
        title += f' (showing dims 1-2 of {particles.shape[1]})'
    
    ax.set_title(title)
    ax.grid(True, alpha=0.3)
    return fig, ax

def viz_beliefs(beliefs, time_steps=None, title='Belief Evolution', 
               figsize=(10, 6), labels=None):
    "Visualize evolution of `beliefs` over `time_steps`"
    beliefs = np.array(beliefs)
    if time_steps is None:
        time_steps = np.arange(len(beliefs))
    
    fig, ax = plt.subplots(figsize=figsize)
    
    if beliefs.ndim == 2:
        # Multiple belief dimensions
        for i in range(beliefs.shape[1]):
            label = f'Belief {i+1}' if labels is None else labels[i]
            ax.plot(time_steps, beliefs[:, i], label=label, marker='o')
        ax.legend()
    else:
        # Single belief dimension
        ax.plot(time_steps, beliefs, marker='o')
    
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Belief Value')
    ax.set_title(title)
    ax.grid(True, alpha=0.3)
    return fig, ax

def viz_comparison(methods_data, time_steps=None, title='Method Comparison',
                  figsize=(12, 8), metrics=['mse', 'mae']):
    "Compare multiple methods with `methods_data` dict"
    if time_steps is None:
        # Try to infer time_steps from data
        first_method = list(methods_data.keys())[0]
        first_data = methods_data[first_method]
        
        # Look for estimates first, then fall back to any available metric
        if 'estimates' in first_data:
            time_steps = np.arange(len(first_data['estimates']))
        else:
            # Use the first available metric to infer length
            available_metrics = [m for m in metrics if m in first_data]
            if available_metrics:
                time_steps = np.arange(len(first_data[available_metrics[0]]))
            else:
                # Default fallback
                time_steps = np.arange(10)
    
    n_metrics = len(metrics)
    fig, axes = plt.subplots(n_metrics, 1, figsize=figsize)
    if n_metrics == 1:
        axes = [axes]
    
    for i, metric in enumerate(metrics):
        ax = axes[i]
        
        for method_name, method_data in methods_data.items():
            if metric in method_data:
                ax.plot(time_steps, method_data[metric], 
                       label=method_name, marker='o')
        
        ax.set_xlabel('Time Step')
        ax.set_ylabel(metric.upper())
        ax.set_title(f'{title} - {metric.upper()}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig, axes

def viz_rbe_summary(rbe_result, true_states=None, title='RBE Summary',
                   figsize=(15, 10)):
    "Create comprehensive summary visualization of RBE results"
    fig = plt.figure(figsize=figsize)
    
    # Layout: 2x2 grid
    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)
    
    # Top left: Final particle distribution
    ax1 = fig.add_subplot(gs[0, 0])
    final_particles = rbe_result['particles'][-1]
    final_weights = rbe_result['weights'][-1]
    
    if final_particles.shape[1] >= 2:
        ax1.scatter(final_particles[:, 0], final_particles[:, 1], 
                   s=final_weights*1000, alpha=0.6)
        ax1.set_xlabel('State Dim 1')
        ax1.set_ylabel('State Dim 2')
    else:
        ax1.hist(final_particles.flatten(), weights=final_weights, bins=30, alpha=0.6)
        ax1.set_xlabel('State')
        ax1.set_ylabel('Density')
    ax1.set_title('Final Particle Distribution')
    ax1.grid(True, alpha=0.3)
    
    # Top right: Estimates over time
    ax2 = fig.add_subplot(gs[0, 1])
    estimates = rbe_result['estimates']
    time_steps = np.arange(len(estimates))
    
    if estimates.ndim == 2 and estimates.shape[1] >= 2:
        ax2.plot(time_steps, estimates[:, 0], 'b-', label='Dim 1', marker='o')
        ax2.plot(time_steps, estimates[:, 1], 'r-', label='Dim 2', marker='s')
        if true_states is not None:
            true_states = np.array(true_states)
            ax2.plot(time_steps, true_states[:, 0], 'b--', alpha=0.7, label='True Dim 1')
            ax2.plot(time_steps, true_states[:, 1], 'r--', alpha=0.7, label='True Dim 2')
        ax2.legend()
    else:
        ax2.plot(time_steps, estimates.flatten(), 'b-', marker='o', label='Estimate')
        if true_states is not None:
            ax2.plot(time_steps, np.array(true_states).flatten(), 'r--', alpha=0.7, label='True')
        ax2.legend()
    
    ax2.set_xlabel('Time Step')
    ax2.set_ylabel('State Value')
    ax2.set_title('Estimates Over Time')
    ax2.grid(True, alpha=0.3)
    
    # Bottom left: Effective sample size
    ax3 = fig.add_subplot(gs[1, 0])
    eff_sizes = [pf_effective_size(w) for w in rbe_result['weights']]
    ax3.plot(np.arange(len(eff_sizes)), eff_sizes, 'g-', marker='o')
    ax3.axhline(len(rbe_result['weights'][0])/2, color='r', linestyle='--', alpha=0.7, label='N/2')
    ax3.set_xlabel('Time Step')
    ax3.set_ylabel('Effective Sample Size')
    ax3.set_title('Particle Filter Health')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Bottom right: Error metrics (if true states provided)
    ax4 = fig.add_subplot(gs[1, 1])
    if true_states is not None:
        errors = np.array(true_states) - estimates
        if errors.ndim == 2:
            error_norms = np.linalg.norm(errors, axis=1)
        else:
            error_norms = np.abs(errors)
        
        ax4.plot(time_steps, error_norms, 'r-', marker='o')
        ax4.set_xlabel('Time Step')
        ax4.set_ylabel('Estimation Error')
        ax4.set_title('Error Over Time')
    else:
        ax4.text(0.5, 0.5, 'No true states\nprovided', ha='center', va='center', 
                transform=ax4.transAxes, fontsize=12)
        ax4.set_title('Error Analysis')
    ax4.grid(True, alpha=0.3)
    
    fig.suptitle(title, fontsize=16)
    return fig

# %% ../../nbs/rbe/00_rbe_core.ipynb 20
__all__ = [
    # Probability utilities
    'prob_normalize', 'prob_sample', 'prob_entropy', 'prob_kl_div',
    
    # Bayesian core
    'bayes_update', 'bayes_sequential', 'bayes_posterior_predictive',
    
    # Particle filter foundation
    'pf_init', 'pf_predict', 'pf_update', 'pf_resample', 'pf_effective_size', 'pf_step',
    
    # RBE estimator
    'rbe_estimator', 'rbe_adaptive', 'rbe_metrics',
    
    # Visualization helpers
    'viz_particles', 'viz_beliefs', 'viz_comparison', 'viz_rbe_summary'
]
