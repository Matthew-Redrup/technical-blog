"""Monte Carlo methods for recursive Bayesian filtering in non-linear, non-Gaussian systems"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/rbe/02_particle_filter.ipynb.

# %% auto 0
__all__ = ['init', 'predict', 'update', 'resample', 'step', 'run', 'auxiliary_pf_step']

# %% ../../nbs/rbe/02_particle_filter.ipynb 3
import numpy as np
from typing import Optional, Callable, Tuple, List
from fastcore.test import test_eq, test_close
from fastcore.all import *
from .probability import normalize, sample, eff_size

# %% ../../nbs/rbe/02_particle_filter.ipynb 5
def init(n_particles, state_dim, init_fn=None, rng=None):
    "Initialize particle filter with `n_particles` and `state_dim`"
    if rng is None: rng = np.random.default_rng()
    
    if init_fn is None:
        # Default: uniform initialization in [0, 1]
        particles = rng.uniform(0, 1, size=(n_particles, state_dim))
    else:
        particles = init_fn(n_particles, state_dim, rng)
    
    weights = np.ones(n_particles) / n_particles
    return particles, weights

# %% ../../nbs/rbe/02_particle_filter.ipynb 8
def predict(particles, weights, transition_fn, rng=None):
    "Prediction step: apply `transition_fn` to `particles`"
    if rng is None: rng = np.random.default_rng()
    
    new_particles = np.zeros_like(particles)
    for i, particle in enumerate(particles):
        new_particles[i] = transition_fn(particle, rng)
    
    return new_particles, weights  # Weights unchanged in prediction

def update(particles, weights, observation, likelihood_fn):
    "Update step: weight `particles` using `observation` and `likelihood_fn`"
    new_weights = np.zeros_like(weights)
    
    for i, particle in enumerate(particles):
        new_weights[i] = weights[i] * likelihood_fn(particle, observation)
    
    # Normalize weights
    if np.sum(new_weights) > 0:
        new_weights = normalize(new_weights)
    else:
        # If all weights are zero, reset to uniform
        new_weights = np.ones_like(weights) / len(weights)
    
    return particles, new_weights

# %% ../../nbs/rbe/02_particle_filter.ipynb 11
def resample(particles, weights, method='systematic', rng=None):
    "Resample `particles` using `weights` with specified `method`"
    if rng is None: rng = np.random.default_rng()
    n_particles = len(particles)
    
    if method == 'systematic':
        # Systematic resampling
        positions = (np.arange(n_particles) + rng.uniform()) / n_particles
        cum_weights = np.cumsum(weights)
        indices = np.searchsorted(cum_weights, positions)
    elif method == 'multinomial':
        # Multinomial resampling
        indices = sample(weights, n_particles, rng)
    elif method == 'stratified':
        # Stratified resampling
        positions = (np.arange(n_particles) + rng.uniform(size=n_particles)) / n_particles
        cum_weights = np.cumsum(weights)
        indices = np.searchsorted(cum_weights, positions)
    else:
        raise ValueError(f"Unknown resampling method: {method}")
    
    new_particles = particles[indices]
    new_weights = np.ones(n_particles) / n_particles
    
    return new_particles, new_weights

# %% ../../nbs/rbe/02_particle_filter.ipynb 14
def step(particles, weights, observation, transition_fn, likelihood_fn, 
         resample_threshold=0.5, rng=None):
    "Complete particle filter step: predict, update, and conditionally resample"
    # Prediction
    particles, weights = predict(particles, weights, transition_fn, rng)
    
    # Update
    particles, weights = update(particles, weights, observation, likelihood_fn)
    
    # Conditional resampling
    if eff_size(weights) < resample_threshold * len(particles):
        particles, weights = resample(particles, weights, rng=rng)
    
    return particles, weights

# %% ../../nbs/rbe/02_particle_filter.ipynb 17
def run(observations, transition_fn, likelihood_fn, 
        n_particles=1000, init_fn=None, resample_threshold=0.5, rng=None):
    "Run particle filter on sequence of `observations`"
    if rng is None: rng = np.random.default_rng()
    
    # Infer state dimension from first observation
    state_dim = len(observations[0]) if hasattr(observations[0], '__len__') else 1
    
    # Initialize particles
    particles, weights = init(n_particles, state_dim, init_fn, rng)
    
    # Store results
    estimates = []
    particle_history = [particles.copy()]
    weight_history = [weights.copy()]
    eff_sizes = [eff_size(weights)]
    
    for obs in observations:
        # Particle filter step
        particles, weights = step(particles, weights, obs, 
                                 transition_fn, likelihood_fn, 
                                 resample_threshold, rng)
        
        # Estimate (weighted mean)
        estimate = np.average(particles, weights=weights, axis=0)
        estimates.append(estimate)
        
        # Store history
        particle_history.append(particles.copy())
        weight_history.append(weights.copy())
        eff_sizes.append(eff_size(weights))
    
    return {
        'estimates': np.array(estimates),
        'particles': particle_history,
        'weights': weight_history,
        'eff_sizes': np.array(eff_sizes)
    }

# %% ../../nbs/rbe/02_particle_filter.ipynb 20
def auxiliary_pf_step(particles, weights, observation, transition_fn, 
                     likelihood_fn, auxiliary_fn=None, rng=None):
    "Auxiliary particle filter step for improved proposal distribution"
    if rng is None: rng = np.random.default_rng()
    if auxiliary_fn is None:
        auxiliary_fn = likelihood_fn  # Default to standard PF
    
    n_particles = len(particles)
    
    # Compute auxiliary weights
    aux_weights = np.zeros(n_particles)
    for i, particle in enumerate(particles):
        # Look-ahead: what would the likelihood be after transition?
        predicted = transition_fn(particle, rng)
        aux_weights[i] = weights[i] * auxiliary_fn(predicted, observation)
    
    # Resample based on auxiliary weights
    if np.sum(aux_weights) > 0:
        aux_weights = normalize(aux_weights)
        indices = sample(aux_weights, n_particles, rng)
        particles = particles[indices]
    
    # Standard predict and update
    return step(particles, np.ones(n_particles)/n_particles, observation,
                transition_fn, likelihood_fn, resample_threshold=2.0, rng=rng)

# %% ../../nbs/rbe/02_particle_filter.ipynb 23
__all__ = [
    # Core operations
    'init', 'predict', 'update', 'resample', 'step',
    
    # High-level interface
    'run',
    
    # Advanced filters
    'auxiliary_pf_step'
]
