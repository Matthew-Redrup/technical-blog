"""From single updates to recursive Bayesian filtering - the temporal dimension of belief evolution"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/rbe/03_recursive_updating.ipynb.

# %% auto 0
__all__ = ['prior', 'evidence_sequence', 'likelihoods', 'belief_evolution', 'rng', 'true_positions', 'observations', 'pos', 'vel',
           'initial_state', 'tracking_result', 'security_transitions', 'markov_result', 'fig', 'axes', 'perf_results',
           'long_evidence', 'long_likelihoods', 'memory_results', 'baseline', 'traffic_data', 'monitoring_results',
           'attack_patterns', 'event_sequence', 'attack_results', 'detected_patterns', 'change_points',
           'segment_patterns', 'obs_data', 'true_regimes', 'strategies', 'adaptation_results', 'time_steps',
           'recursive_bayes_demo', 'state_space_tracker', 'simple_motion_model', 'noisy_position_observation',
           'markov_chain_demo', 'belief_evolution_visualizer', 'recursive_update_component',
           'batch_vs_recursive_comparison', 'plot_performance_comparison', 'memory_analysis', 'plot_memory_analysis',
           'update_baseline', 'adaptive_threat_monitor', 'multi_step_attack_detection', 'non_stationary_demo',
           'compare_adaptation_strategies', 'forgetting_factor_filter', 'windowed_filter', 'standard_recursive_filter']

# %% ../../nbs/rbe/03_recursive_updating.ipynb 3
import numpy as np
import matplotlib.pyplot as plt
from fastcore.test import test_eq, test_close
from fastcore.all import *
from .core import bayes_update, bayes_sequential, pf_init, pf_step, pf_effective_size, prob_normalize, prob_sample, prob_entropy, prob_kl_div, viz_beliefs
from .bayes import bayes_theorem_step_by_step
from fasthtml.common import *
from typing import List, Dict, Tuple, Optional, Callable
import seaborn as sns
import time
from collections import defaultdict

# %% ../../nbs/rbe/03_recursive_updating.ipynb 6
def recursive_bayes_demo(prior, evidence_sequence, likelihoods, labels=None):
    """Demonstrate recursive Bayesian updating with evidence sequence"""
    if labels is None:
        labels = [f'H{i+1}' for i in range(len(prior))]
    
    print("=== RECURSIVE BAYESIAN UPDATING ===")
    print(f"\nInitial Prior: {dict(zip(labels, prior))}")
    
    # Track beliefs over time
    beliefs = [np.array(prior)]
    current_belief = np.array(prior)
    
    for i, (evidence, likelihood) in enumerate(zip(evidence_sequence, likelihoods)):
        print(f"\n--- Time Step {i+1}: Evidence = {evidence} ---")
        
        # Recursive update: current belief becomes prior for this step
        updated_belief = bayes_update(current_belief, np.array(likelihood))
        
        print(f"Likelihood: {dict(zip(labels, likelihood))}")
        print(f"Updated Belief: {dict(zip(labels, updated_belief))}")
        
        # Store and update
        beliefs.append(updated_belief.copy())
        current_belief = updated_belief
    
    return np.array(beliefs)

# Example: Network intrusion detection over time
print("Example: Sequential Network Monitoring")
prior = [0.1, 0.9]  # 10% initial attack probability
evidence_sequence = ['suspicious_port', 'failed_login', 'privilege_escalation']
likelihoods = [
    [0.7, 0.1],  # Suspicious port: 70% if attack, 10% if normal
    [0.9, 0.05], # Failed login: 90% if attack, 5% if normal  
    [0.95, 0.01] # Privilege escalation: 95% if attack, 1% if normal
]

belief_evolution = recursive_bayes_demo(prior, evidence_sequence, likelihoods, 
                                       labels=['Attack', 'Normal'])

# %% ../../nbs/rbe/03_recursive_updating.ipynb 8
def state_space_tracker(initial_state, observations, transition_fn, 
                       observation_fn, n_particles=1000, rng=None):
    """Generic state space tracker using particle filtering"""
    if rng is None: rng = np.random.default_rng()
    
    # Initialize particles around initial state
    state_dim = len(initial_state) if hasattr(initial_state, '__len__') else 1
    particles = np.array([initial_state] * n_particles)
    
    # Add some initial uncertainty
    if state_dim > 1:
        particles += rng.normal(0, 0.1, (n_particles, state_dim))
    else:
        particles += rng.normal(0, 0.1, n_particles)
    
    weights = np.ones(n_particles) / n_particles
    
    # Track results
    estimates = []
    particle_history = [particles.copy()]
    weight_history = [weights.copy()]
    
    for obs in observations:
        # Predict step: evolve particles
        for i in range(n_particles):
            particles[i] = transition_fn(particles[i], rng)
        
        # Update step: weight particles based on observation
        for i in range(n_particles):
            weights[i] *= observation_fn(particles[i], obs)
        
        # Normalize weights
        if np.sum(weights) > 0:
            weights = prob_normalize(weights)
        else:
            weights = np.ones(n_particles) / n_particles
        
        # Resample if needed
        eff_size = pf_effective_size(weights)
        if eff_size < n_particles / 2:
            indices = prob_sample(weights, n_particles, rng)
            particles = particles[indices]
            weights = np.ones(n_particles) / n_particles
        
        # Estimate (weighted mean)
        if state_dim > 1:
            estimate = np.average(particles, weights=weights, axis=0)
        else:
            estimate = np.average(particles, weights=weights)
        estimates.append(estimate)
        
        # Store history
        particle_history.append(particles.copy())
        weight_history.append(weights.copy())
    
    return {
        'estimates': np.array(estimates),
        'particles': particle_history,
        'weights': weight_history
    }

# Example: Track moving target
def simple_motion_model(state, rng):
    """Simple constant velocity model with noise"""
    # State: [position, velocity]
    position, velocity = state
    new_position = position + velocity + rng.normal(0, 0.05)
    new_velocity = velocity + rng.normal(0, 0.02)
    return np.array([new_position, new_velocity])

def noisy_position_observation(state, observation):
    """Likelihood of observing position given true state"""
    position = state[0]
    diff = abs(position - observation)
    return np.exp(-0.5 * (diff / 0.1)**2)  # Gaussian likelihood

# Generate synthetic tracking data
rng = np.random.default_rng(42)
true_positions = []
observations = []
pos, vel = 0.0, 0.1

for t in range(20):
    pos += vel + rng.normal(0, 0.05)
    vel += rng.normal(0, 0.02)
    true_positions.append(pos)
    observations.append(pos + rng.normal(0, 0.1))  # Noisy observation

# Track the target
initial_state = np.array([0.0, 0.1])  # [position, velocity]
tracking_result = state_space_tracker(initial_state, observations, 
                                     simple_motion_model, noisy_position_observation,
                                     n_particles=500, rng=rng)

print(f"\nTracking Results:")
print(f"Final position estimate: {tracking_result['estimates'][-1][0]:.3f}")
print(f"True final position: {true_positions[-1]:.3f}")
print(f"Tracking error: {abs(tracking_result['estimates'][-1][0] - true_positions[-1]):.3f}")

# %% ../../nbs/rbe/03_recursive_updating.ipynb 10
def markov_chain_demo(transition_matrix, n_steps, initial_dist=None, 
                     state_labels=None, rng=None):
    """Demonstrate Markov chain evolution and properties"""
    if rng is None: rng = np.random.default_rng()
    n_states = len(transition_matrix)
    
    if initial_dist is None:
        initial_dist = np.ones(n_states) / n_states
    
    if state_labels is None:
        state_labels = [f'State {i}' for i in range(n_states)]
    
    print("=== MARKOV CHAIN DEMONSTRATION ===")
    print(f"Transition Matrix:")
    for i, row in enumerate(transition_matrix):
        print(f"  {state_labels[i]}: {row}")
    
    # Track distribution evolution
    distributions = [initial_dist.copy()]
    current_dist = initial_dist.copy()
    
    print(f"\nDistribution Evolution:")
    print(f"Step 0: {dict(zip(state_labels, current_dist))}")
    
    for step in range(n_steps):
        # Matrix multiplication for distribution evolution
        current_dist = current_dist @ transition_matrix
        distributions.append(current_dist.copy())
        print(f"Step {step+1}: {dict(zip(state_labels, current_dist))}")
    
    # Sample trajectory
    print(f"\nSample Trajectory:")
    state = prob_sample(initial_dist, 1, rng)[0]
    trajectory = [state]
    print(f"Start: {state_labels[state]}")
    
    for step in range(min(10, n_steps)):
        # Sample next state based on current state
        state = prob_sample(transition_matrix[state], 1, rng)[0]
        trajectory.append(state)
        print(f"Step {step+1}: {state_labels[state]}")
    
    return {
        'distributions': np.array(distributions),
        'trajectory': trajectory,
        'steady_state': current_dist
    }

# Example: Network security states
print("Example: Network Security State Evolution")
security_transitions = np.array([
    [0.8, 0.15, 0.05],  # Normal -> [Normal, Suspicious, Compromised]
    [0.3, 0.6, 0.1],    # Suspicious -> [Normal, Suspicious, Compromised] 
    [0.1, 0.2, 0.7]     # Compromised -> [Normal, Suspicious, Compromised]
])

markov_result = markov_chain_demo(
    security_transitions, 10, 
    initial_dist=[0.9, 0.1, 0.0],
    state_labels=['Normal', 'Suspicious', 'Compromised']
)

# %% ../../nbs/rbe/03_recursive_updating.ipynb 12
def belief_evolution_visualizer(beliefs, time_steps=None, title="Belief Evolution",
                               labels=None, figsize=(12, 6)):
    """Visualize how beliefs evolve over time with interactive features"""
    beliefs = np.array(beliefs)
    if time_steps is None:
        time_steps = np.arange(len(beliefs))
    
    if labels is None:
        labels = [f'Hypothesis {i+1}' for i in range(beliefs.shape[1])]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    
    # Left plot: Belief evolution over time
    colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))
    for i, (label, color) in enumerate(zip(labels, colors)):
        ax1.plot(time_steps, beliefs[:, i], label=label, marker='o', 
                linewidth=2, color=color)
    
    ax1.set_xlabel('Time Step')
    ax1.set_ylabel('Belief Probability')
    ax1.set_title('Belief Evolution Over Time')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)
    
    # Right plot: Final belief distribution
    final_beliefs = beliefs[-1]
    bars = ax2.bar(labels, final_beliefs, color=colors, alpha=0.7)
    ax2.set_ylabel('Final Probability')
    ax2.set_title('Final Belief State')
    ax2.set_ylim(0, 1)
    
    # Add value labels on bars
    for bar, val in zip(bars, final_beliefs):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{val:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    return fig, (ax1, ax2)

# Visualize our network monitoring example
fig, axes = belief_evolution_visualizer(
    belief_evolution, 
    labels=['Attack Probability', 'Normal Probability'],
    title="Network Threat Assessment Over Time"
)
plt.show()

# %% ../../nbs/rbe/03_recursive_updating.ipynb 13
def recursive_update_component():
    """Interactive FastHTML component for exploring recursive updates"""
    
    # Component styling
    style = """
    .update-controls { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 8px; 
        margin: 10px 0;
    }
    .belief-display {
        font-family: 'Courier New', monospace;
        background: #e9ecef;
        padding: 15px;
        border-radius: 5px;
        margin: 10px 0;
    }
    .step-button {
        background: #007bff;
        color: white;
        border: none;
        padding: 10px 20px;
        border-radius: 5px;
        cursor: pointer;
        margin: 5px;
    }
    .step-button:hover { background: #0056b3; }
    """
    
    # JavaScript for interactivity
    script = """
    let currentStep = 0;
    let beliefs = [0.1, 0.9]; // [Attack, Normal]
    const evidenceSequence = [
        {name: 'Suspicious Port', likelihood: [0.7, 0.1]},
        {name: 'Failed Login', likelihood: [0.9, 0.05]},
        {name: 'Privilege Escalation', likelihood: [0.95, 0.01]},
        {name: 'Data Exfiltration', likelihood: [0.98, 0.005]}
    ];
    
    function updateDisplay() {
        document.getElementById('current-beliefs').innerHTML = 
            `Attack: ${beliefs[0].toFixed(3)}, Normal: ${beliefs[1].toFixed(3)}`;
        document.getElementById('step-counter').innerHTML = `Step: ${currentStep}`;
        
        if (currentStep < evidenceSequence.length) {
            document.getElementById('next-evidence').innerHTML = 
                `Next: ${evidenceSequence[currentStep].name}`;
        } else {
            document.getElementById('next-evidence').innerHTML = 'No more evidence';
            document.getElementById('step-btn').disabled = true;
        }
    }
    
    function bayesUpdate(prior, likelihood) {
        const evidence = prior[0] * likelihood[0] + prior[1] * likelihood[1];
        return [
            (prior[0] * likelihood[0]) / evidence,
            (prior[1] * likelihood[1]) / evidence
        ];
    }
    
    function nextStep() {
        if (currentStep < evidenceSequence.length) {
            const evidence = evidenceSequence[currentStep];
            beliefs = bayesUpdate(beliefs, evidence.likelihood);
            currentStep++;
            updateDisplay();
        }
    }
    
    function reset() {
        currentStep = 0;
        beliefs = [0.1, 0.9];
        document.getElementById('step-btn').disabled = false;
        updateDisplay();
    }
    """
    
    return Div(
        Style(style),
        Script(script),
        H3("Interactive Recursive Bayesian Updating"),
        P("Watch how beliefs evolve as evidence arrives sequentially."),
        
        Div(
            H4("Current State"),
            Div(id="step-counter", cls="belief-display"),
            Div(id="current-beliefs", cls="belief-display"),
            Div(id="next-evidence", cls="belief-display"),
            cls="update-controls"
        ),
        
        Div(
            Button("Next Update", id="step-btn", cls="step-button", 
                  onclick="nextStep()"),
            Button("Reset", cls="step-button", onclick="reset()"),
            cls="update-controls"
        ),
        
        Script("updateDisplay();")  # Initialize display
    )

# Display the component (this would be used in FastHTML app)
print("Interactive component created - would be displayed in FastHTML app")
print("Features: Step-by-step Bayesian updating with network security scenario")
from IPython.display import HTML
HTML(str(recursive_update_component()))

# %% ../../nbs/rbe/03_recursive_updating.ipynb 15
def batch_vs_recursive_comparison(data_sizes, n_trials=10, rng=None):
    """Compare performance of batch vs recursive Bayesian updating"""
    if rng is None: rng = np.random.default_rng()
    
    results = {
        'data_sizes': data_sizes,
        'batch_times': [],
        'recursive_times': [],
        'batch_memory': [],
        'recursive_memory': [],
        'accuracy_difference': []
    }
    
    for n_data in data_sizes:
        print(f"\nTesting with {n_data} data points...")
        
        batch_times = []
        recursive_times = []
        accuracy_diffs = []
        
        for trial in range(n_trials):
            # Generate synthetic data
            prior = np.array([0.3, 0.7])
            likelihoods = rng.random((n_data, 2))
            likelihoods = likelihoods / likelihoods.sum(axis=1, keepdims=True)
            
            # Batch processing
            start_time = time.time()
            
            # Simulate batch processing: recompute from scratch each time
            batch_posterior = prior.copy()
            for i in range(n_data):
                # Inefficient: recompute using all data up to point i
                temp_posterior = prior.copy()
                for j in range(i + 1):
                    temp_posterior = bayes_update(temp_posterior, likelihoods[j])
                batch_posterior = temp_posterior
            
            batch_time = time.time() - start_time
            batch_times.append(batch_time)
            
            # Recursive processing
            start_time = time.time()
            
            recursive_posterior = prior.copy()
            for likelihood in likelihoods:
                recursive_posterior = bayes_update(recursive_posterior, likelihood)
            
            recursive_time = time.time() - start_time
            recursive_times.append(recursive_time)
            
            # Check accuracy difference
            accuracy_diff = np.linalg.norm(batch_posterior - recursive_posterior)
            accuracy_diffs.append(accuracy_diff)
        
        # Store average results
        results['batch_times'].append(np.mean(batch_times))
        results['recursive_times'].append(np.mean(recursive_times))
        results['accuracy_difference'].append(np.mean(accuracy_diffs))
        
        # Memory usage (approximate)
        results['batch_memory'].append(n_data ** 2)  # O(n²) for batch
        results['recursive_memory'].append(n_data)   # O(n) for recursive
        
        print(f"  Batch time: {results['batch_times'][-1]:.4f}s")
        print(f"  Recursive time: {results['recursive_times'][-1]:.4f}s")
        print(f"  Speedup: {results['batch_times'][-1] / results['recursive_times'][-1]:.1f}x")
        print(f"  Accuracy difference: {results['accuracy_difference'][-1]:.2e}")
    
    return results

def plot_performance_comparison(results):
    """Plot performance comparison results"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    
    data_sizes = results['data_sizes']
    
    # Computation time comparison
    ax1.plot(data_sizes, results['batch_times'], 'r-o', label='Batch', linewidth=2)
    ax1.plot(data_sizes, results['recursive_times'], 'b-s', label='Recursive', linewidth=2)
    ax1.set_xlabel('Number of Data Points')
    ax1.set_ylabel('Computation Time (seconds)')
    ax1.set_title('Computation Time Comparison')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')
    
    # Speedup factor
    speedups = np.array(results['batch_times']) / np.array(results['recursive_times'])
    ax2.plot(data_sizes, speedups, 'g-^', linewidth=2, markersize=8)
    ax2.set_xlabel('Number of Data Points')
    ax2.set_ylabel('Speedup Factor')
    ax2.set_title('Recursive Speedup vs Batch')
    ax2.grid(True, alpha=0.3)
    
    # Memory usage comparison
    ax3.plot(data_sizes, results['batch_memory'], 'r-o', label='Batch O(n²)', linewidth=2)
    ax3.plot(data_sizes, results['recursive_memory'], 'b-s', label='Recursive O(n)', linewidth=2)
    ax3.set_xlabel('Number of Data Points')
    ax3.set_ylabel('Memory Usage (relative units)')
    ax3.set_title('Memory Usage Comparison')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale('log')
    
    # Accuracy difference
    ax4.plot(data_sizes, results['accuracy_difference'], 'm-d', linewidth=2)
    ax4.set_xlabel('Number of Data Points')
    ax4.set_ylabel('Accuracy Difference')
    ax4.set_title('Numerical Accuracy (Batch vs Recursive)')
    ax4.grid(True, alpha=0.3)
    ax4.set_yscale('log')
    
    plt.tight_layout()
    return fig

# Run performance comparison
print("Running performance comparison...")
perf_results = batch_vs_recursive_comparison([10, 25, 50, 100], n_trials=3)
fig = plot_performance_comparison(perf_results)
plt.show()

# %% ../../nbs/rbe/03_recursive_updating.ipynb 17
def memory_analysis(evidence_sequence, likelihoods, lookback_windows, 
                   initial_prior=None):
    """Analyze how much historical evidence affects current beliefs"""
    if initial_prior is None:
        initial_prior = np.array([0.5, 0.5])
    
    n_evidence = len(evidence_sequence)
    results = {
        'lookback_windows': lookback_windows,
        'final_beliefs': [],
        'belief_differences': [],
        'information_loss': []
    }
    
    # Full recursive update (baseline)
    full_belief = initial_prior.copy()
    for likelihood in likelihoods:
        full_belief = bayes_update(full_belief, np.array(likelihood))
    
    print(f"Full recursive belief: {full_belief}")
    
    # Test different lookback windows
    for window in lookback_windows:
        if window >= n_evidence:
            # Use all evidence
            windowed_belief = full_belief.copy()
        else:
            # Use only last 'window' pieces of evidence
            windowed_belief = initial_prior.copy()
            start_idx = max(0, n_evidence - window)
            
            for i in range(start_idx, n_evidence):
                windowed_belief = bayes_update(windowed_belief, 
                                             np.array(likelihoods[i]))
        
        # Calculate differences
        belief_diff = np.linalg.norm(full_belief - windowed_belief)
        info_loss = prob_kl_div(full_belief, windowed_belief)
        
        results['final_beliefs'].append(windowed_belief)
        results['belief_differences'].append(belief_diff)
        results['information_loss'].append(info_loss)
        
        print(f"Window {window}: belief = {windowed_belief}, "
              f"diff = {belief_diff:.4f}, KL = {info_loss:.4f}")
    
    return results

def plot_memory_analysis(results):
    """Plot memory analysis results"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    windows = results['lookback_windows']
    
    # Belief difference vs lookback window
    ax1.plot(windows, results['belief_differences'], 'b-o', linewidth=2, markersize=8)
    ax1.set_xlabel('Lookback Window Size')
    ax1.set_ylabel('Belief Difference from Full History')
    ax1.set_title('Effect of Limited Memory on Beliefs')
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')
    
    # Information loss (KL divergence)
    ax2.plot(windows, results['information_loss'], 'r-s', linewidth=2, markersize=8)
    ax2.set_xlabel('Lookback Window Size')
    ax2.set_ylabel('Information Loss (KL Divergence)')
    ax2.set_title('Information Loss with Limited Memory')
    ax2.grid(True, alpha=0.3)
    ax2.set_yscale('log')
    
    plt.tight_layout()
    return fig

# Example: How much history do we need?
print("Memory Analysis: How much history matters?")
long_evidence = ['event'] * 20
long_likelihoods = [[0.8, 0.3]] * 10 + [[0.9, 0.1]] * 10  # Pattern change

memory_results = memory_analysis(
    long_evidence, long_likelihoods, 
    lookback_windows=[1, 2, 5, 10, 15, 20, 25],
    initial_prior=[0.2, 0.8]
)

fig = plot_memory_analysis(memory_results)
plt.show()

# %% ../../nbs/rbe/03_recursive_updating.ipynb 19
def update_baseline(current_baseline, observation, adaptation_rate):
    """Update baseline using exponential moving average"""
    return (1 - adaptation_rate) * current_baseline + adaptation_rate * observation

def adaptive_threat_monitor(baseline_behavior, 
                            time_series_data,
                            adaptation_rate=0.1, 
                            threshold=0.7):    
    """Adaptive threat monitoring with evolving baseline"""
    # Initialize beliefs: [normal, anomalous]   
    threat_belief = np.array([0.9, 0.1])    
    # Handle both numeric and array baselines   
    if hasattr(baseline_behavior, 'copy'):
        current_baseline = baseline_behavior.copy()
    else:
        current_baseline = float(baseline_behavior)
            
    results = {
        'time_steps': [],
        'threat_probabilities': [],
        'baselines': [],
        'alerts': [],
        'observations': []
        }
    
    print("=== ADAPTIVE THREAT MONITORING ===")
    for t, observation in enumerate(time_series_data):
        # Calculate deviation from current baseline
        deviation = abs(observation - current_baseline)
        
        # Likelihood based on deviation (more reasonable scaling)
        # Scale deviation by baseline to get relative change
        relative_deviation = deviation / max(current_baseline, 1.0)
        
        # More moderate likelihood functions
        if relative_deviation < 0.1:  # Small deviation
            normal_likelihood = 0.9
            anomaly_likelihood = 0.1
        elif relative_deviation < 0.3:  # Medium deviation
            normal_likelihood = 0.6
            anomaly_likelihood = 0.4
        else:  # Large deviation
            normal_likelihood = 0.2
            anomaly_likelihood = 0.8
            
        likelihood = np.array([normal_likelihood, anomaly_likelihood])
        
        # Recursive Bayesian update
        threat_belief = bayes_update(threat_belief, likelihood)
        
        # Alert if threat probability exceeds threshold
        alert = threat_belief[1] > threshold
        
        # Update baseline
        current_baseline = update_baseline(current_baseline, observation, adaptation_rate)
        
        # Store results
        results['time_steps'].append(t)
        results['threat_probabilities'].append(threat_belief[1])
        results['baselines'].append(current_baseline)
        results['alerts'].append(alert)
        results['observations'].append(observation)
        
        # Print progress
        print(f"Time step {t}: Obs={observation:.1f}, Baseline={current_baseline:.1f}, " +
              f"RelDev={relative_deviation:.2f}, Threat={threat_belief[1]:.3f}, Alert={alert}")
        
    return results

def multi_step_attack_detection(event_sequence, attack_patterns, 
                               window_size=5, threshold=0.8):
    """Detect multi-step attacks using pattern matching with recursive updates"""
    # Initialize beliefs for each attack pattern
    pattern_beliefs = {}
    for pattern_name in attack_patterns:
        pattern_beliefs[pattern_name] = np.array([0.9, 0.1])  # [benign, attack] - start less certain
    
    results = {
        'time_steps': [],
        'pattern_probabilities': {name: [] for name in attack_patterns},
        'alerts': [],
        'detected_patterns': []
    }
    
    print("=== MULTI-STEP ATTACK DETECTION ===")
    
    for t, event in enumerate(event_sequence):
        alerts_this_step = []
        detected_this_step = []
        
        print(f"\nTime step {t}: Event = {event}")
        
        for pattern_name, pattern_steps in attack_patterns.items():
            # Check if current event matches any step in this pattern
            current_belief = pattern_beliefs[pattern_name]
            
            if event in pattern_steps:
                # Event matches pattern - increase attack probability
                # More moderate likelihood that still provides evidence
                likelihood = np.array([0.3, 0.7])  # Moderate evidence for attack
            else:
                # Event doesn't match - slightly decrease attack probability  
                likelihood = np.array([0.7, 0.3])  # Moderate evidence for benign
            
            # Recursive Bayesian update
            updated_belief = bayes_update(current_belief, likelihood)
            pattern_beliefs[pattern_name] = updated_belief
            
            # Check for detection
            attack_prob = updated_belief[1]
            alert = attack_prob > threshold
            
            if alert:
                alerts_this_step.append(pattern_name)
                detected_this_step.append(pattern_name)
            
            print(f"  {pattern_name}: Match={event in pattern_steps}, P(attack)={attack_prob:.3f}, Alert={alert}")
            
            # Store results
            results['pattern_probabilities'][pattern_name].append(attack_prob)
        
        results['time_steps'].append(t)
        results['alerts'].append(alerts_this_step)
        results['detected_patterns'].append(detected_this_step)
    
    return results

# Demonstrate adaptive threat monitoring
print("Example: Network Traffic Monitoring")
baseline = 100  # Baseline network traffic (packets/sec)
traffic_data = [95, 102, 98, 105, 150, 180, 160, 110, 95, 200, 220, 190, 100, 98]

monitoring_results = adaptive_threat_monitor(
    baseline, traffic_data, 
    adaptation_rate=0.2, threshold=0.6
)

print(f"\nSummary: {sum(monitoring_results['alerts'])} alerts generated")

# Demonstrate multi-step attack detection  
print("\n" + "="*50)
print("Example: Multi-step Attack Detection")

attack_patterns = {
    'Reconnaissance': ['port_scan', 'dns_lookup', 'service_enum'],
    'Lateral_Movement': ['credential_theft', 'remote_login', 'privilege_escalation'], 
    'Data_Exfiltration': ['database_access', 'file_compression', 'network_transfer']
}

event_sequence = ['port_scan', 'normal_traffic', 'dns_lookup', 'credential_theft', 
                 'service_enum', 'remote_login', 'normal_traffic', 'database_access']

attack_results = multi_step_attack_detection(
    event_sequence, attack_patterns, threshold=0.7
)

detected_patterns = set([p for patterns in attack_results['detected_patterns'] for p in patterns])
print(f"\nDetected attack patterns: {detected_patterns if detected_patterns else 'None'}")

# %% ../../nbs/rbe/03_recursive_updating.ipynb 21
def non_stationary_demo(change_points, segment_patterns, n_observations=100):
    """Demonstrate challenges with non-stationary data"""
    rng = np.random.default_rng(42)
    
    # Generate non-stationary data
    observations = []
    true_states = []
    current_segment = 0
    
    for t in range(n_observations):
        # Check for regime change
        if current_segment < len(change_points) and t >= change_points[current_segment]:
            current_segment += 1
        
        # Generate observation from current pattern
        pattern = segment_patterns[min(current_segment, len(segment_patterns) - 1)]
        obs = rng.normal(pattern['mean'], pattern['std'])
        observations.append(obs)
        true_states.append(current_segment)
    
    return observations, true_states

def compare_adaptation_strategies(observations, true_states, strategies):
    """Compare different adaptation strategies for non-stationary data"""
    results = {name: {'estimates': [], 'errors': []} for name in strategies}
    
    for strategy_name, strategy_fn in strategies.items():
        estimates = strategy_fn(observations)
        errors = [abs(est - true) for est, true in zip(estimates, true_states)]
        
        results[strategy_name]['estimates'] = estimates
        results[strategy_name]['errors'] = errors
        
        print(f"{strategy_name}: Mean error = {np.mean(errors):.3f}")
    
    return results

def forgetting_factor_filter(observations, forgetting_factor=0.95):
    """Simple filter with exponential forgetting"""
    estimates = []
    current_estimate = observations[0] if observations else 0
    
    for obs in observations:
        current_estimate = forgetting_factor * current_estimate + (1 - forgetting_factor) * obs
        estimates.append(current_estimate)
    
    return estimates

def windowed_filter(observations, window_size=20):
    """Simple windowed mean filter"""
    estimates = []
    
    for i, obs in enumerate(observations):
        start_idx = max(0, i - window_size + 1)
        window_data = observations[start_idx:i+1]
        estimates.append(np.mean(window_data))
    
    return estimates

def standard_recursive_filter(observations):
    """Standard recursive mean (no adaptation)"""
    estimates = []
    running_mean = 0
    
    for i, obs in enumerate(observations):
        running_mean = (running_mean * i + obs) / (i + 1)
        estimates.append(running_mean)
    
    return estimates

# Demonstrate non-stationarity challenges
print("=== NON-STATIONARITY CHALLENGES ===")

# Define regime changes
change_points = [30, 60]
segment_patterns = [
    {'mean': 10, 'std': 2},   # Regime 1
    {'mean': 25, 'std': 3},   # Regime 2  
    {'mean': 5, 'std': 1}     # Regime 3
]

obs_data, true_regimes = non_stationary_demo(change_points, segment_patterns, 90)

# Compare adaptation strategies
strategies = {
    'Standard Recursive': standard_recursive_filter,
    'Forgetting Factor': lambda x: forgetting_factor_filter(x, 0.9),
    'Sliding Window': lambda x: windowed_filter(x, 15)
}

adaptation_results = compare_adaptation_strategies(obs_data, [p['mean'] for p in segment_patterns for _ in range(30)], strategies)

# Plot comparison
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

# Top: Observations and true regime changes
time_steps = np.arange(len(obs_data))
ax1.plot(time_steps, obs_data, 'k.', alpha=0.5, label='Observations')

# Mark regime changes
for cp in change_points:
    ax1.axvline(cp, color='red', linestyle='--', alpha=0.7)

ax1.set_ylabel('Observation Value')
ax1.set_title('Non-stationary Data with Regime Changes')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Bottom: Estimation errors
for strategy_name, strategy_data in adaptation_results.items():
    ax2.plot(time_steps, strategy_data['errors'], label=strategy_name, alpha=0.8)

for cp in change_points:
    ax2.axvline(cp, color='red', linestyle='--', alpha=0.7)

ax2.set_xlabel('Time Step')
ax2.set_ylabel('Estimation Error')
ax2.set_title('Adaptation Strategy Comparison')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nKey insights:")
print("- Standard recursive filtering fails with regime changes")
print("- Forgetting factors help adapt to new regimes")
print("- Sliding windows provide good balance of adaptation and stability")
print("- Choice depends on expected change frequency and noise levels")

# %% ../../nbs/rbe/03_recursive_updating.ipynb 23
__all__ = [
    # Core recursive functions
    'recursive_bayes_demo', 'state_space_tracker', 'markov_chain_demo',
    
    # Interactive components
    'belief_evolution_visualizer', 'recursive_update_component',
    
    # Performance analysis
    'batch_vs_recursive_comparison', 'plot_performance_comparison',
    
    # Memory analysis
    'memory_analysis', 'plot_memory_analysis', 
    
    # Cybersecurity applications
    'adaptive_threat_monitor', 'multi_step_attack_detection',
    
    # Non-stationarity handling
    'non_stationary_demo', 'compare_adaptation_strategies',
    'forgetting_factor_filter', 'windowed_filter', 'standard_recursive_filter'
]
