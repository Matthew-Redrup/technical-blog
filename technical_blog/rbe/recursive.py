"""From single updates to recursive Bayesian filtering - the temporal dimension of belief evolution"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/rbe/03_recursive_updating.ipynb.

# %% auto 0
__all__ = ['prior', 'evidence_sequence', 'likelihoods', 'belief_evolution', 'fig', 'axes', 'current_step', 'beliefs', 'app', 'rt',
           'mixed_evidence', 'balanced_likelihoods', 'corrected_results', 'insights_fig', 'decision_fig',
           'change_points', 'segment_patterns', 'obs_data', 'true_regimes', 'strategies', 'adaptation_results',
           'time_steps', 'recursive_bayes_demo', 'particle_filter', 'motion_model', 'position_likelihood',
           'markov_chain_demo', 'belief_evolution_visualizer', 'belief_state_component', 'control_buttons',
           'recursive_update_component', 'get', 'post', 'batch_vs_recursive_comparison', 'memory_analysis_corrected',
           'rolling_memory_analysis', 'plot_memory_insights', 'plot_memory_decision_guide', 'update_baseline',
           'multi_step_attack_detection', 'adaptive_threat_monitor', 'non_stationary_demo',
           'compare_adaptation_strategies', 'forgetting_factor_filter', 'windowed_filter', 'standard_recursive_filter']

# %% ../../nbs/rbe/03_recursive_updating.ipynb 3
import numpy as np
import matplotlib.pyplot as plt
from fastcore.test import test_eq, test_close
from fastcore.all import *
from .core import bayes_update, bayes_sequential, pf_init, pf_step, pf_effective_size, prob_normalize, prob_sample, prob_entropy, prob_kl_div, viz_beliefs
from .bayes import bayes_theorem_step_by_step
from fasthtml.common import *
from monsterui.all import *
from typing import List, Dict, Tuple, Optional, Callable
import seaborn as sns
import time
from collections import defaultdict

# %% ../../nbs/rbe/03_recursive_updating.ipynb 6
def recursive_bayes_demo(prior, evidence_seq, likelihoods, labels=None):
    "Demonstrate recursive Bayesian updating"
    if labels is None: labels = [f'H{i}' for i in range(len(prior))]
    
    beliefs, current = [np.array(prior)], np.array(prior)
    
    for i, (ev, like) in enumerate(zip(evidence_seq, likelihoods)):
        print(f"\nStep {i+1}: {ev}")
        current = bayes_update(current, like)
        print(f"  {dict(zip(labels, current.round(3)))}")
        beliefs.append(current.copy())
    
    return np.array(beliefs)

# Example: Network intrusion detection over time
print("Example: Sequential Network Monitoring")
prior = [0.1, 0.9]  # 10% initial attack probability
evidence_sequence = ['suspicious_port', 'failed_login', 'privilege_escalation']
likelihoods = [
    [0.7, 0.1],  # Suspicious port: 70% if attack, 10% if normal
    [0.9, 0.05], # Failed login: 90% if attack, 5% if normal  
    [0.95, 0.01] # Privilege escalation: 95% if attack, 1% if normal
]

belief_evolution = recursive_bayes_demo(prior, evidence_sequence, likelihoods, 
                                       labels=['Attack', 'Normal'])

# %% ../../nbs/rbe/03_recursive_updating.ipynb 9
def particle_filter(initial_state,  # Initial state estimate
                    observations,  # Sequence of observations
                    transition_fn,  # Function (state, rng) -> new_state
                    observation_fn,  # Function (state, obs) -> likelihood
                    n_particles=1000,  # Number of particles
                    initial_cov=0.1,  # Initial covariance for particle initialization
                    resample_threshold=0.5,  # Resample when eff_size < threshold * n_particles
                    rng=None  # Random number generator
                    ): 
    """Particle filter for state space tracking"""
    if rng is None: rng = np.random.default_rng()
    
    # Initialize particles with proper diversity
    initial_state = np.asarray(initial_state)
    if initial_state.ndim == 0: initial_state = np.array([initial_state])
    
    if len(initial_state) == 1:
        particles = rng.normal(initial_state[0], initial_cov, (n_particles, 1))
    else:
        cov_matrix = initial_cov * np.eye(len(initial_state))
        particles = rng.multivariate_normal(initial_state, cov_matrix, n_particles)
    
    estimates = []
    
    for obs in observations:
        # Predict: evolve all particles
        for i in range(n_particles):
            particles[i] = transition_fn(particles[i].flatten(), rng)
        
        # Update: compute likelihoods (not cumulative weights)
        log_weights = np.array([np.log(max(observation_fn(p.flatten(), obs), 1e-300)) 
                               for p in particles])
        
        # Normalize in log space for numerical stability
        log_weights -= np.max(log_weights)  # Prevent overflow
        weights = np.exp(log_weights)
        weights = prob_normalize(weights)
        
        # Estimate current state
        estimate = np.average(particles, weights=weights, axis=0)
        estimates.append(estimate.copy())
        
        # Resample if needed
        eff_size = pf_effective_size(weights)
        if eff_size < resample_threshold * n_particles:
            indices = prob_sample(weights, n_particles, rng)
            particles = particles[indices]
    
    return np.array(estimates)

# Simplified tracking example with better motion model
def motion_model(state, rng, dt=1.0, process_noise=0.05):
    """Constant velocity model: [pos, vel] -> [pos+vel*dt, vel+noise]"""
    pos, vel = state
    new_pos = pos + vel * dt + rng.normal(0, process_noise)
    new_vel = vel + rng.normal(0, process_noise * 0.5)  # Less velocity noise
    return np.array([new_pos, new_vel])

def position_likelihood(state, obs, obs_noise=0.1):
    """Gaussian likelihood for position observation"""
    pos = state[0]
    return np.exp(-0.5 * ((pos - obs) / obs_noise)**2)


# %% ../../nbs/rbe/03_recursive_updating.ipynb 14
def markov_chain_demo(P, # transition matrix
                      n_steps, # number of steps to simulate
                      π0=None, # initial distribution (uniform if None)
                      labels=None, # state names (auto-generated if None)
                      seed=None # random seed for reproducibility
                      ):
    """Demonstrate Markov chain evolution and properties"""
    P = np.array(P, dtype=np.float64)
    n_states = len(P)
    rng = np.random.default_rng(seed)
    
    # Validate transition matrix
    row_sums = P.sum(axis=1)
    if not np.allclose(row_sums, 1.0, rtol=1e-10):
        bad_rows = np.where(~np.isclose(row_sums, 1.0, rtol=1e-10))[0]
        raise ValueError(f"Transition matrix rows must sum to 1. "
                        f"Rows {bad_rows} sum to {row_sums[bad_rows]}")
    
    # Set defaults
    if π0 is None: π0 = np.ones(n_states) / n_states
    if labels is None: labels = [f'S{i}' for i in range(n_states)]
    
    π0 = np.array(π0, dtype=np.float64)
    
    print("=== MARKOV CHAIN ===")
    print("Transition Matrix P:")
    for i, (label, row) in enumerate(zip(labels, P)):
        print(f"  {label}: {row}")
    
    # Evolution: π_{t+1} = π_t @ P  
    πs = [π0.copy()]
    π = π0.copy()
    
    print(f"\nDistribution Evolution:")
    print(f"t=0: {dict(zip(labels, π))}")
    
    for t in range(n_steps):
        π = π @ P
        πs.append(π.copy())
        print(f"t={t+1}: {dict(zip(labels, π.round(4)))}")
    
    # Sample path
    print(f"\nSample Path:")
    s = rng.choice(n_states, p=π0)
    path = [s]
    print(f"t=0: {labels[s]}")
    
    for t in range(min(10, n_steps)):
        s = rng.choice(n_states, p=P[s])
        path.append(s)
        print(f"t={t+1}: {labels[s]}")
    
    return {
        'distributions': np.array(πs),
        'path': path,
        'steady_state': π,
        'P': P
    }

# %% ../../nbs/rbe/03_recursive_updating.ipynb 17
def belief_evolution_visualizer(beliefs, time_steps=None, title="Belief Evolution",
                               labels=None, figsize=(12, 6)):
    """Visualize how beliefs evolve over time with interactive features"""
    beliefs = np.array(beliefs)
    if time_steps is None:
        time_steps = np.arange(len(beliefs))
    
    if labels is None:
        labels = [f'Hypothesis {i+1}' for i in range(beliefs.shape[1])]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    
    # Left plot: Belief evolution over time
    colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))
    for i, (label, color) in enumerate(zip(labels, colors)):
        ax1.plot(time_steps, beliefs[:, i], label=label, marker='o', 
                linewidth=2, color=color)
    
    ax1.set_xlabel('Time Step')
    ax1.set_ylabel('Belief Probability')
    ax1.set_title('Belief Evolution Over Time')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)
    
    # Right plot: Final belief distribution
    final_beliefs = beliefs[-1]
    bars = ax2.bar(labels, final_beliefs, color=colors, alpha=0.7)
    ax2.set_ylabel('Final Probability')
    ax2.set_title('Final Belief State')
    ax2.set_ylim(0, 1)
    
    # Add value labels on bars
    for bar, val in zip(bars, final_beliefs):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{val:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    return fig, (ax1, ax2)

# Visualize our network monitoring example
fig, axes = belief_evolution_visualizer(
    belief_evolution, 
    labels=['Attack Probability', 'Normal Probability'],
    title="Network Threat Assessment Over Time"
)
plt.show()

# %% ../../nbs/rbe/03_recursive_updating.ipynb 18
# Global state for the demo (in a real app, you'd use sessions or database)
current_step = 0
beliefs = [0.1, 0.9]  # [Attack, Normal]

evidence_sequence = [
    {'name': 'Suspicious Port', 'likelihood': [0.7, 0.1]},
    {'name': 'Failed Login', 'likelihood': [0.9, 0.05]},
    {'name': 'Privilege Escalation', 'likelihood': [0.95, 0.01]},
    {'name': 'Data Exfiltration', 'likelihood': [0.98, 0.005]}
]

def belief_state_component():
    """Component showing current belief state"""
    global current_step, beliefs, evidence_sequence
    
    # Progress indicator
    progress = Span(f"Step {current_step} of {len(evidence_sequence)}", 
                   cls="uk-badge uk-badge-primary")
    
    # Current beliefs
    belief_display = Div(
        Strong("Probabilities: "),
        Span(f"Attack: {beliefs[0]:.3f} | Normal: {beliefs[1]:.3f}", 
             cls="monospace-text uk-text-primary uk-text-bold"),
        cls="uk-background-muted uk-padding-small uk-border-rounded uk-margin-small-bottom"
    )
    
    # Next evidence or completion message
    if current_step < len(evidence_sequence):
        next_evidence = Span(f"Next Evidence: {evidence_sequence[current_step]['name']}", 
                           cls="uk-text-muted uk-text-italic")
        update_btn = Button("Update Beliefs", 
                           cls="uk-button uk-button-primary uk-margin-small-right",
                           hx_post="/update_beliefs",
                           hx_target="#belief-state",
                           hx_swap="outerHTML")
    else:
        next_evidence = Span("Analysis Complete", cls="uk-text-muted uk-text-italic")
        update_btn = Button("Update Beliefs", 
                           cls="uk-button uk-button-primary uk-margin-small-right",
                           disabled=True)
    
    return Div(
        Div(progress, cls="uk-margin-small-bottom"),
        belief_display,
        Div(next_evidence, cls="uk-margin-small-bottom"),
        id="belief-state"
    )

def control_buttons():
    """Control buttons component"""
    return Div(
        Button("Update Beliefs", 
               cls="uk-button uk-button-primary uk-margin-small-right",
               hx_post="/update_beliefs",
               hx_target="#belief-state",
               hx_swap="outerHTML",
               hx_also="#controls"),  # Also update buttons
        Button("Reset Demo", 
               cls="uk-button uk-button-default",
               hx_post="/reset_demo",
               hx_target="#belief-state",
               hx_swap="outerHTML",
               hx_also="#controls"),
        cls="uk-text-center uk-margin-medium-top",
        id="controls"
    )

def recursive_update_component():
    """Main component using HTMX"""
    
    style = """
    .monospace-text {
        font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
        font-size: 0.95rem;
    }
    """
    
    return Div(
        Style(style),
        
        # Header
        H3("Interactive Recursive Bayesian Updating", 
           cls="uk-heading-line uk-text-center uk-margin-large-bottom"),
        P("Watch how beliefs evolve as evidence arrives sequentially in network security monitoring.", 
          cls="uk-text-muted uk-text-center uk-margin-medium-bottom"),
        
        # Main content card
        Div(
            Div(
                H4("Current Belief State", cls="uk-card-title"),
                cls="uk-card-header"
            ),
            Div(
                belief_state_component(),
                cls="uk-card-body"
            ),
            cls="uk-card uk-card-default uk-margin-medium-bottom"
        ),
        
        # Control buttons
        control_buttons(),
        
        cls="uk-container uk-container-small uk-margin-large-top"
    )

# FastHTML app setup
app, rt = fast_app()

@rt("/")
def get():
    return Titled("Bayesian Demo", recursive_update_component())

@rt("/update_beliefs")
def post():
    """HTMX endpoint to update beliefs"""
    global current_step, beliefs, evidence_sequence
    
    if current_step < len(evidence_sequence):
        evidence = evidence_sequence[current_step]
        beliefs = bayes_update(beliefs, evidence['likelihood'])
        current_step += 1
    
    return belief_state_component()

@rt("/reset_demo")  
def post():
    """HTMX endpoint to reset demo"""
    global current_step, beliefs
    
    current_step = 0
    beliefs = [0.1, 0.9]
    
    return belief_state_component()

# For Jupyter display
from IPython.display import HTML
HTML(str(recursive_update_component()))


# %% ../../nbs/rbe/03_recursive_updating.ipynb 21
def batch_vs_recursive_comparison(data_sizes, n_trials=10, rng=None):
    """Compare true batch processing vs recursive updating"""
    if rng is None: rng = np.random.default_rng()
    
    results = {
        'data_sizes': data_sizes,
        'batch_times': [],
        'recursive_times': [],
        'accuracy_difference': []
    }
    
    for n_data in data_sizes:
        print(f"\nTesting with {n_data} data points...")
        
        batch_times = []
        recursive_times = []
        accuracy_diffs = []
        
        for trial in range(n_trials):
            # Generate synthetic data
            prior = np.array([0.3, 0.7])
            likelihoods = rng.random((n_data, 2))
            likelihoods = likelihoods / likelihoods.sum(axis=1, keepdims=True)
            
            # TRUE batch processing: compute joint likelihood of all data
            start_time = time.time()
            
            # Compute product of all likelihoods
            joint_likelihood = np.ones(2)
            for likelihood in likelihoods:
                joint_likelihood *= likelihood
            
            # Single Bayes update with joint likelihood
            batch_posterior = bayes_update(prior, joint_likelihood)
            batch_time = time.time() - start_time
            batch_times.append(batch_time)
            
            # Recursive processing (same as before)
            start_time = time.time()
            recursive_posterior = prior.copy()
            for likelihood in likelihoods:
                recursive_posterior = bayes_update(recursive_posterior, likelihood)
            recursive_time = time.time() - start_time
            recursive_times.append(recursive_time)
            
            # Now there should be NO difference if evidence is independent
            accuracy_diff = np.linalg.norm(batch_posterior - recursive_posterior)
            accuracy_diffs.append(accuracy_diff)
        
        results['batch_times'].append(np.mean(batch_times))
        results['recursive_times'].append(np.mean(recursive_times))
        results['accuracy_difference'].append(np.mean(accuracy_diffs))
        
        print(f"  Batch time: {results['batch_times'][-1]:.6f}s")
        print(f"  Recursive time: {results['recursive_times'][-1]:.6f}s")
        print(f"  Accuracy difference: {results['accuracy_difference'][-1]:.2e}")
    
    return results

# %% ../../nbs/rbe/03_recursive_updating.ipynb 25
mixed_evidence = ['event'] * 25
balanced_likelihoods = (
        [[0.7, 0.4]] * 6 +    # Moderate evidence for H1
        [[0.4, 0.7]] * 8 +    # Moderate evidence for H2  
        [[0.6, 0.45]] * 6 +   # Weak evidence for H1
        [[0.45, 0.6]] * 5     # Weak evidence for H2
    )

def memory_analysis_corrected(evidence_sequence, likelihoods, lookback_windows, 
                            initial_prior=None):
    """Corrected memory analysis - simulate true memory limitations"""
    if initial_prior is None:
        initial_prior = np.array([0.5, 0.5])
    
    n_evidence = len(evidence_sequence)
    
    # Full memory baseline - use ALL evidence from initial prior
    full_belief = initial_prior.copy()
    for likelihood in likelihoods:
        full_belief = bayes_update(full_belief, np.array(likelihood))
    
    results = {
        'lookback_windows': lookback_windows,
        'final_beliefs': [],
        'belief_differences': [],
        'information_loss': []
    }
    
    print(f"Full memory final belief: {full_belief}")
    
    # Test different memory limitations
    for window in lookback_windows:
        if window >= n_evidence:
            # Use all evidence - same as full case
            windowed_belief = full_belief.copy()
        else:
            # TRUE memory limitation: start from initial prior,
            # but only use the LAST 'window' pieces of evidence
            windowed_belief = initial_prior.copy()
            start_idx = n_evidence - window
            
            # Only apply the last 'window' pieces of evidence
            for i in range(start_idx, n_evidence):
                windowed_belief = bayes_update(windowed_belief, np.array(likelihoods[i]))
        
        # Calculate differences from full-memory result
        belief_diff = np.linalg.norm(full_belief - windowed_belief)
        info_loss = prob_kl_div(full_belief, windowed_belief)
        
        results['final_beliefs'].append(windowed_belief)
        results['belief_differences'].append(belief_diff)
        results['information_loss'].append(info_loss)
        
        print(f"Window {window}: belief = {windowed_belief.round(6)}, "
              f"diff = {belief_diff:.6f}, KL = {info_loss:.6f}")
    
    return results

# Test the corrected version
print("=== CORRECTED MEMORY ANALYSIS ===")
corrected_results = memory_analysis_corrected(
    ['event'] * len(balanced_likelihoods), 
    balanced_likelihoods,
    lookback_windows=[1, 3, 5, 8, 12, 16, 20, 25],
    initial_prior=[0.5, 0.5]
)


# %% ../../nbs/rbe/03_recursive_updating.ipynb 26
def rolling_memory_analysis(evidence_sequence, likelihoods, window_size, 
                          initial_prior=None):
    """Analyze performance with a fixed rolling memory window"""
    if initial_prior is None:
        initial_prior = np.array([0.5, 0.5])
    
    # Full memory baseline
    full_belief = initial_prior.copy()
    for likelihood in likelihoods:
        full_belief = bayes_update(full_belief, np.array(likelihood))
    
    # Rolling window simulation
    rolling_belief = initial_prior.copy()
    recent_evidence = []
    
    for i, likelihood in enumerate(likelihoods):
        recent_evidence.append(likelihood)
        
        # Keep only last 'window_size' pieces of evidence
        if len(recent_evidence) > window_size:
            recent_evidence.pop(0)
        
        # Recompute belief from scratch using only recent evidence
        rolling_belief = initial_prior.copy()
        for recent_like in recent_evidence:
            rolling_belief = bayes_update(rolling_belief, np.array(recent_like))
    
    return {
        'full_belief': full_belief,
        'rolling_belief': rolling_belief,
        'difference': np.linalg.norm(full_belief - rolling_belief),
        'kl_divergence': prob_kl_div(full_belief, rolling_belief)
    }


# %% ../../nbs/rbe/03_recursive_updating.ipynb 28
def plot_memory_insights(results, likelihoods):
    """Create insightful visualizations of memory effects"""
    
    fig = plt.figure(figsize=(16, 12))
    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1], hspace=0.3, wspace=0.3)
    
    windows = results['lookback_windows']
    final_beliefs = np.array(results['final_beliefs'])
    belief_diffs = results['belief_differences']
    info_losses = results['information_loss']
    
    # 1. Memory window effects on final beliefs
    ax1 = fig.add_subplot(gs[0, :])
    ax1.plot(windows, final_beliefs[:, 0], 'bo-', label='Hypothesis 1', markersize=8, linewidth=2)
    ax1.plot(windows, final_beliefs[:, 1], 'ro-', label='Hypothesis 2', markersize=8, linewidth=2)
    
    # Add full memory reference lines
    full_memory_h1 = final_beliefs[-1, 0]  # Last entry should be full memory
    full_memory_h2 = final_beliefs[-1, 1]
    ax1.axhline(y=full_memory_h1, color='blue', linestyle='--', alpha=0.7, label='Full Memory H1')
    ax1.axhline(y=full_memory_h2, color='red', linestyle='--', alpha=0.7, label='Full Memory H2')
    
    ax1.set_xlabel('Memory Window Size')
    ax1.set_ylabel('Final Belief Probability')
    ax1.set_title('How Memory Limitations Affect Final Beliefs')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)
    
    # 2. Accuracy vs Memory Trade-off
    ax2 = fig.add_subplot(gs[1, 0])
    ax2.semilogy(windows, belief_diffs, 'g-o', label='L2 Difference', markersize=6, linewidth=2)
    ax2.semilogy(windows, info_losses, 'purple', marker='s', label='KL Divergence', 
                markersize=6, linewidth=2)
    ax2.set_xlabel('Memory Window Size')
    ax2.set_ylabel('Error from Full Memory (log scale)')
    ax2.set_title('Memory vs Accuracy Trade-off')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Memory efficiency analysis
    ax3 = fig.add_subplot(gs[1, 1])
    # Efficiency = accuracy per unit of memory
    # Higher is better (low error per memory unit)
    efficiency = []
    for i, window in enumerate(windows):
        if window == 0:
            eff = 0
        else:
            # Inverse of error per memory unit (higher = more efficient)
            error = max(belief_diffs[i], 1e-10)  # Avoid division by zero
            eff = 1 / (error * window)
        efficiency.append(eff)
    
    ax3.plot(windows, efficiency, 'orange', marker='D', markersize=6, linewidth=2)
    ax3.set_xlabel('Memory Window Size')
    ax3.set_ylabel('Efficiency (Accuracy/Memory)')
    ax3.set_title('Memory Efficiency Analysis')
    ax3.grid(True, alpha=0.3)
    
    # Find and highlight the most efficient window
    max_eff_idx = np.argmax(efficiency[:-1])  # Exclude full memory
    ax3.axvline(x=windows[max_eff_idx], color='red', linestyle='--', alpha=0.7)
    ax3.text(windows[max_eff_idx], max(efficiency) * 0.8, 
             f'Most Efficient\nWindow: {windows[max_eff_idx]}', 
             ha='center', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
    
    # 4. Evidence pattern analysis
    ax4 = fig.add_subplot(gs[2, 0])
    evidence_strength = [l[0] - l[1] for l in likelihoods]
    colors = ['red' if x < 0 else 'blue' for x in evidence_strength]
    
    bars = ax4.bar(range(1, len(evidence_strength) + 1), evidence_strength, 
                   color=colors, alpha=0.7)
    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax4.set_xlabel('Evidence Step')
    ax4.set_ylabel('Evidence Strength (H1 - H2)')
    ax4.set_title('Evidence Pattern: Blue=Pro-H1, Red=Pro-H2')
    ax4.grid(True, alpha=0.3)
    
    # 5. Memory sensitivity heatmap
    ax5 = fig.add_subplot(gs[2, 1])
    
    # Create a sensitivity matrix: how much does each piece of evidence matter
    # for different memory windows?
    n_evidence = len(likelihoods)
    sensitivity_matrix = np.zeros((len(windows[:-1]), n_evidence))  # Exclude full memory
    
    for i, window in enumerate(windows[:-1]):  # Exclude full memory
        if window >= n_evidence:
            continue
        start_idx = n_evidence - window
        
        # Test removing each piece of evidence within the window
        for j in range(start_idx, n_evidence):
            # Compute belief without evidence j
            test_belief = np.array([0.5, 0.5])
            for k in range(start_idx, n_evidence):
                if k != j:  # Skip evidence j
                    test_belief = bayes_update(test_belief, np.array(likelihoods[k]))
            
            # Sensitivity = how much removing this evidence changes the result
            original_belief = final_beliefs[i]
            sensitivity = np.linalg.norm(original_belief - test_belief)
            sensitivity_matrix[i, j] = sensitivity
    
    im = ax5.imshow(sensitivity_matrix, cmap='YlOrRd', aspect='auto')
    ax5.set_xlabel('Evidence Step')
    ax5.set_ylabel('Memory Window Size')
    ax5.set_title('Evidence Sensitivity by Memory Window')
    ax5.set_yticks(range(len(windows[:-1])))
    ax5.set_yticklabels([str(w) for w in windows[:-1]])
    
    # Add colorbar
    cbar = plt.colorbar(im, ax=ax5)
    cbar.set_label('Sensitivity (Impact if Removed)')
    
    plt.suptitle('Comprehensive Memory Analysis: Key Insights', fontsize=16, y=0.98)
    return fig

# Create the comprehensive insights plot
insights_fig = plot_memory_insights(corrected_results, balanced_likelihoods)
plt.show()


# %% ../../nbs/rbe/03_recursive_updating.ipynb 29
def plot_memory_decision_guide(results):
    """Create a practical decision guide for choosing memory window size"""
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    
    windows = results['lookback_windows']
    belief_diffs = results['belief_differences']
    info_losses = results['information_loss']
    
    # 1. Error tolerance guide
    ax1.plot(windows, belief_diffs, 'b-o', linewidth=2, markersize=6)
    
    # Add tolerance threshold lines
    tolerances = [0.01, 0.05, 0.1, 0.2]
    colors = ['green', 'yellow', 'orange', 'red']
    
    for tol, color in zip(tolerances, colors):
        ax1.axhline(y=tol, color=color, linestyle='--', alpha=0.7, 
                   label=f'{tol:.0%} tolerance')
        
        # Find minimum window size for this tolerance
        valid_windows = [w for w, diff in zip(windows, belief_diffs) if diff <= tol]
        if valid_windows:
            min_window = min(valid_windows)
            ax1.axvline(x=min_window, color=color, linestyle=':', alpha=0.5)
    
    ax1.set_xlabel('Memory Window Size')
    ax1.set_ylabel('L2 Error from Full Memory')
    ax1.set_title('Memory Requirements by Error Tolerance')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')
    
    # 2. Diminishing returns analysis
    ax2.plot(windows[1:], np.diff(belief_diffs), 'r-s', linewidth=2, markersize=6)
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax2.set_xlabel('Memory Window Size')
    ax2.set_ylabel('Marginal Error Reduction')
    ax2.set_title('Diminishing Returns of Additional Memory')
    ax2.grid(True, alpha=0.3)
    
    # 3. Cost-benefit analysis
    ax3.scatter(windows, belief_diffs, s=[w*10 for w in windows], 
               c=windows, cmap='viridis', alpha=0.7)
    ax3.set_xlabel('Memory Window Size')
    ax3.set_ylabel('Error from Full Memory')
    ax3.set_title('Cost-Benefit: Bubble Size = Memory Cost')
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale('log')
    
    # Add pareto frontier
    pareto_points = []
    for i, (w, err) in enumerate(zip(windows, belief_diffs)):
        is_pareto = True
        for j, (w2, err2) in enumerate(zip(windows, belief_diffs)):
            if i != j and w2 <= w and err2 <= err and (w2 < w or err2 < err):
                is_pareto = False
                break
        if is_pareto:
            pareto_points.append((w, err))
    
    if pareto_points:
        pareto_points.sort()
        pareto_w, pareto_err = zip(*pareto_points)
        ax3.plot(pareto_w, pareto_err, 'r--', linewidth=2, alpha=0.8, label='Pareto Frontier')
        ax3.legend()
    
    # 4. Practical recommendations
    ax4.axis('off')
    
    # Generate recommendations based on the data
    recommendations = []
    
    # Find minimum viable window (< 5% error)
    viable_windows = [w for w, diff in zip(windows, belief_diffs) if diff <= 0.05]
    if viable_windows:
        min_viable = min(viable_windows)
        recommendations.append(f"• Minimum viable memory: {min_viable} steps (< 5% error)")
    
    # Find efficiency sweet spot
    efficiency = [1/(d*w) if d > 0 else 0 for d, w in zip(belief_diffs[:-1], windows[:-1])]
    if efficiency:
        best_eff_idx = np.argmax(efficiency)
        recommendations.append(f"• Most efficient: {windows[best_eff_idx]} steps")
    
    # Find diminishing returns point
    if len(belief_diffs) > 1:
        improvements = [-np.diff(belief_diffs)[i] for i in range(len(windows)-1)]
        if improvements:
            # Find where improvement drops below 10% of maximum
            max_improvement = max(improvements)
            threshold = max_improvement * 0.1
            diminishing_idx = next((i for i, imp in enumerate(improvements) 
                                  if imp < threshold), len(improvements)-1)
            recommendations.append(f"• Diminishing returns after: {windows[diminishing_idx+1]} steps")
    
    recommendations.append(f"• Full accuracy requires: {windows[-1]} steps")
    
    # Display recommendations
    rec_text = "PRACTICAL RECOMMENDATIONS:\n\n" + "\n".join(recommendations)
    rec_text += "\n\nCHOOSE BASED ON:\n"
    rec_text += "• Real-time systems: Use minimum viable\n"
    rec_text += "• Resource-constrained: Use most efficient\n" 
    rec_text += "• High-accuracy needs: Use full memory\n"
    rec_text += "• Balanced systems: Stop at diminishing returns"
    
    ax4.text(0.05, 0.95, rec_text, transform=ax4.transAxes, fontsize=11,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    plt.suptitle('Memory Window Decision Guide', fontsize=16)
    plt.tight_layout()
    return fig

# Create the decision guide
decision_fig = plot_memory_decision_guide(corrected_results)
plt.show()


# %% ../../nbs/rbe/03_recursive_updating.ipynb 31
def update_baseline(current_baseline, observation, adaptation_rate):
    """Update baseline using exponential moving average"""
    return (1 - adaptation_rate) * current_baseline + adaptation_rate * observation

# %% ../../nbs/rbe/03_recursive_updating.ipynb 32
def multi_step_attack_detection(event_sequence, attack_patterns, 
                                       window_size=5, threshold=0.6):
    """Improved multi-step attack detection with better calibration"""
    
    # Start with more neutral beliefs - less confident about benign state
    pattern_beliefs = {}
    for pattern_name in attack_patterns:
        pattern_beliefs[pattern_name] = np.array([0.7, 0.3])  # Less confident start
    
    results = {
        'time_steps': [],
        'pattern_probabilities': {name: [] for name in attack_patterns},
        'alerts': [],
        'detected_patterns': []
    }
    
    print("=== IMPROVED MULTI-STEP ATTACK DETECTION ===")
    
    for t, event in enumerate(event_sequence):
        alerts_this_step = []
        detected_this_step = []
        
        print(f"\nTime step {t}: Event = {event}")
        
        for pattern_name, pattern_steps in attack_patterns.items():
            current_belief = pattern_beliefs[pattern_name]
            
            if event in pattern_steps:
                # Strong evidence for attack when event matches pattern
                likelihood = np.array([0.1, 0.9])  # 90% evidence for attack
            else:
                # Moderate evidence against attack when no match
                likelihood = np.array([0.8, 0.2])  # 80% evidence for benign
            
            # Recursive Bayesian update
            updated_belief = bayes_update(current_belief, likelihood)
            pattern_beliefs[pattern_name] = updated_belief
            
            # Check for detection
            attack_prob = updated_belief[1]
            alert = attack_prob > threshold
            
            if alert:
                alerts_this_step.append(pattern_name)
                detected_this_step.append(pattern_name)
            
            print(f"  {pattern_name}: Match={event in pattern_steps}, P(attack)={attack_prob:.3f}, Alert={alert}")
            
            results['pattern_probabilities'][pattern_name].append(attack_prob)
        
        results['time_steps'].append(t)
        results['alerts'].append(alerts_this_step)
        results['detected_patterns'].append(detected_this_step)
    
    return results

# %% ../../nbs/rbe/03_recursive_updating.ipynb 33
def adaptive_threat_monitor(baseline_behavior, time_series_data, 
                         adaptation_rate=0.01, threshold=0.6,
                         decay_rate=0.1):  # Add memory decay
    """Refined version with better memory management"""
    
    threat_belief = np.array([0.6, 0.4])  # Slightly less neutral start
    original_baseline = float(baseline_behavior)
    
    results = {'time_steps': [], 'threat_probabilities': [], 'alerts': [], 'observations': []}
    
    print("=== REFINED THREAT MONITORING ===")
    
    for t, observation in enumerate(time_series_data):
        absolute_deviation = abs(observation - original_baseline)
        deviation_percentage = (absolute_deviation / original_baseline) * 100
        
        # More balanced likelihood functions
        if deviation_percentage < 5:     # Very close to baseline
            likelihood = np.array([0.9, 0.1])
        elif deviation_percentage < 15:  # Small deviation
            likelihood = np.array([0.7, 0.3])
        elif deviation_percentage < 40:  # Moderate deviation
            likelihood = np.array([0.3, 0.7])
        else:  # Large deviation
            likelihood = np.array([0.1, 0.9])
        
        # Bayesian update
        threat_belief = bayes_update(threat_belief, likelihood)
        
        # Add memory decay - gradually return toward neutral when no strong evidence
        if deviation_percentage < 10:  # Only decay during normal periods
            threat_belief[1] = threat_belief[1] * (1 - decay_rate) + 0.4 * decay_rate
            threat_belief[0] = 1 - threat_belief[1]
        
        alert = threat_belief[1] > threshold
        
        results['time_steps'].append(t)
        results['threat_probabilities'].append(threat_belief[1])
        results['alerts'].append(alert)
        results['observations'].append(observation)
        
        print(f"Time {t}: Obs={observation:.1f}, DevPct={deviation_percentage:.1f}%, " +
              f"Threat={threat_belief[1]:.3f}, Alert={alert}")
    
    return results

# %% ../../nbs/rbe/03_recursive_updating.ipynb 37
def non_stationary_demo(change_points, segment_patterns, n_observations=100):
    """Demonstrate challenges with non-stationary data"""
    rng = np.random.default_rng(42)
    
    # Generate non-stationary data
    observations = []
    true_states = []
    current_segment = 0
    
    for t in range(n_observations):
        # Check for regime change
        if current_segment < len(change_points) and t >= change_points[current_segment]:
            current_segment += 1
        
        # Generate observation from current pattern
        pattern = segment_patterns[min(current_segment, len(segment_patterns) - 1)]
        obs = rng.normal(pattern['mean'], pattern['std'])
        observations.append(obs)
        true_states.append(current_segment)
    
    return observations, true_states

def compare_adaptation_strategies(observations, true_states, strategies):
    """Compare different adaptation strategies for non-stationary data"""
    results = {name: {'estimates': [], 'errors': []} for name in strategies}
    
    for strategy_name, strategy_fn in strategies.items():
        estimates = strategy_fn(observations)
        errors = [abs(est - true) for est, true in zip(estimates, true_states)]
        
        results[strategy_name]['estimates'] = estimates
        results[strategy_name]['errors'] = errors
        
        print(f"{strategy_name}: Mean error = {np.mean(errors):.3f}")
    
    return results

def forgetting_factor_filter(observations, forgetting_factor=0.95):
    """Simple filter with exponential forgetting"""
    estimates = []
    current_estimate = observations[0] if observations else 0
    
    for obs in observations:
        current_estimate = forgetting_factor * current_estimate + (1 - forgetting_factor) * obs
        estimates.append(current_estimate)
    
    return estimates

def windowed_filter(observations, window_size=20):
    """Simple windowed mean filter"""
    estimates = []
    
    for i, obs in enumerate(observations):
        start_idx = max(0, i - window_size + 1)
        window_data = observations[start_idx:i+1]
        estimates.append(np.mean(window_data))
    
    return estimates

def standard_recursive_filter(observations):
    """Standard recursive mean (no adaptation)"""
    estimates = []
    running_mean = 0
    
    for i, obs in enumerate(observations):
        running_mean = (running_mean * i + obs) / (i + 1)
        estimates.append(running_mean)
    
    return estimates

# Demonstrate non-stationarity challenges
print("=== NON-STATIONARITY CHALLENGES ===")

# Define regime changes
change_points = [30, 60]
segment_patterns = [
    {'mean': 10, 'std': 2},   # Regime 1
    {'mean': 25, 'std': 3},   # Regime 2  
    {'mean': 5, 'std': 1}     # Regime 3
]

obs_data, true_regimes = non_stationary_demo(change_points, segment_patterns, 90)

# Compare adaptation strategies
strategies = {
    'Standard Recursive': standard_recursive_filter,
    'Forgetting Factor': lambda x: forgetting_factor_filter(x, 0.9),
    'Sliding Window': lambda x: windowed_filter(x, 15)
}

adaptation_results = compare_adaptation_strategies(obs_data, [p['mean'] for p in segment_patterns for _ in range(30)], strategies)

# Plot comparison
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

# Top: Observations and true regime changes
time_steps = np.arange(len(obs_data))
ax1.plot(time_steps, obs_data, 'k.', alpha=0.5, label='Observations')

# Mark regime changes
for cp in change_points:
    ax1.axvline(cp, color='red', linestyle='--', alpha=0.7)

ax1.set_ylabel('Observation Value')
ax1.set_title('Non-stationary Data with Regime Changes')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Bottom: Estimation errors
for strategy_name, strategy_data in adaptation_results.items():
    ax2.plot(time_steps, strategy_data['errors'], label=strategy_name, alpha=0.8)

for cp in change_points:
    ax2.axvline(cp, color='red', linestyle='--', alpha=0.7)

ax2.set_xlabel('Time Step')
ax2.set_ylabel('Estimation Error')
ax2.set_title('Adaptation Strategy Comparison')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nKey insights:")
print("- Standard recursive filtering fails with regime changes")
print("- Forgetting factors help adapt to new regimes")
print("- Sliding windows provide good balance of adaptation and stability")
print("- Choice depends on expected change frequency and noise levels")

# %% ../../nbs/rbe/03_recursive_updating.ipynb 39
__all__ = [
    # Core recursive functions
    'recursive_bayes_demo', 'particle_filter', 'markov_chain_demo',
    
    # Interactive components
    'belief_evolution_visualizer', 'recursive_update_component',
    
    # Performance analysis
    'batch_vs_recursive_comparison', 'plot_performance_comparison',
    
    # Memory analysis
    'memory_analysis', 'plot_memory_analysis', 
    
    # Cybersecurity applications
    'adaptive_threat_monitor', 'multi_step_attack_detection',
    
    # Non-stationarity handling
    'non_stationary_demo', 'compare_adaptation_strategies',
    'forgetting_factor_filter', 'windowed_filter', 'standard_recursive_filter'
]
