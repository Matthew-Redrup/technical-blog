"""Core probability utilities for RBE - normalization, sampling, entropy, and divergence measures"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/rbe/00_probability.ipynb.

# %% auto 0
__all__ = ['normalize', 'sample', 'entropy', 'kl_div', 'js_div', 'eff_size', 'categorical', 'uniform', 'from_counts']

# %% ../../nbs/rbe/00_probability.ipynb 4
import numpy as np
from typing import Optional, Union, List
from fastcore.all import *
from scipy.special import entr

# %% ../../nbs/rbe/00_probability.ipynb 8
def normalize(probs):
    """Normalize probabilities to sum to 1."""
    probs = np.asarray(probs, dtype=np.float64)  # Ensure float64 for precision
    if probs.size == 0: raise ValueError("Cannot normalize empty array")
    if np.any(probs < 0): raise ValueError("Probabilities must be non-negative") 
    s = np.sum(probs)
    if s == 0: raise ValueError("Cannot normalize zero probabilities")
    return probs / s

# %% ../../nbs/rbe/00_probability.ipynb 13
def sample(probs, # probability distribution
           n=1, # number of samples
           rng=None # random number generator
           ):
    """Sample indices from probability distribution."""
    if rng is None: rng = np.random.default_rng()
    probs = normalize(probs)  # This handles all validation
    if n == 1:
        return rng.choice(len(probs), p=probs)  # Return scalar
    else:
        return rng.choice(len(probs), size=n, p=probs)  # Return array

# %% ../../nbs/rbe/00_probability.ipynb 21
def entropy(probs, # probability distribution
            base=2 # base of the logarithm
            ):
    """Calculate entropy using scipy's numerically stable implementation."""
    probs = normalize(probs)
    h = np.sum(entr(probs))  # Uses x*log(x) with proper handling of x=0
    
    if base == 2:
        return h / np.log(2)
    elif base == 'e':
        return h
    else:
        return h / np.log(base)


# %% ../../nbs/rbe/00_probability.ipynb 24
def kl_div(p, q, eps=1e-10):
    "KL divergence from `q` to `p`"
    p, q = normalize(p), normalize(q)
    # Add epsilon to avoid log(0)
    return np.sum(p * np.log((p + eps) / (q + eps)))


# %% ../../nbs/rbe/00_probability.ipynb 26
def js_div(p, q):
    "Jensen-Shannon divergence between `p` and `q`"
    p, q = normalize(p), normalize(q)
    m = 0.5 * (p + q)
    return 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)

# %% ../../nbs/rbe/00_probability.ipynb 29
def eff_size(weights):
    "Calculate effective sample size of normalized `weights`"
    weights = normalize(weights)
    return 1.0 / np.sum(weights**2)

# %% ../../nbs/rbe/00_probability.ipynb 32
def categorical(probs, labels=None):
    "Create categorical distribution from `probs` with optional `labels`"
    probs = normalize(probs)
    if labels is None:
        labels = list(range(len(probs)))
    return dict(zip(labels, probs))

def uniform(n):
    "Create uniform distribution over `n` outcomes"
    return np.ones(n) / n

def from_counts(counts):
    "Create probability distribution from `counts`"
    counts = np.asarray(counts)
    if np.any(counts < 0):
        raise ValueError("Counts must be non-negative")
    return normalize(counts)

# %% ../../nbs/rbe/00_probability.ipynb 35
__all__ = [
    # Basic operations
    'normalize', 'sample',
    
    # Information measures
    'entropy', 'kl_div', 'js_div',
    
    # Effective sample size
    'eff_size',
    
    # Categorical utilities
    'categorical', 'uniform', 'from_counts'
]
