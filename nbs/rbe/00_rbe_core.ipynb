{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBE Core\n",
    "\n",
    "> Core functions for Recursive Bayesian Estimation - probability utilities, Bayesian inference, particle filters, and visualization helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rbe.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Callable, Tuple, List, Union\n",
    "from fastcore.test import test_eq, test_close\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Utilities\n",
    "\n",
    "Basic probability distribution functions following fast.ai style with short, clear names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prob_normalize(probs):\n",
    "    \"Normalize `probs` to sum to 1\"\n",
    "    probs = np.asarray(probs)\n",
    "    s = np.sum(probs)\n",
    "    if s == 0: raise ValueError(\"Cannot normalize zero probabilities\")\n",
    "    return probs / s\n",
    "\n",
    "def prob_sample(probs, n=1, rng=None):\n",
    "    \"Sample `n` indices from `probs` distribution\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    probs = np.asarray(probs)\n",
    "    if np.any(probs < 0): raise ValueError(\"Probabilities must be non-negative\")\n",
    "    probs = prob_normalize(probs)  # Handle normalization and zero-sum check\n",
    "    return rng.choice(len(probs), size=n, p=probs)\n",
    "\n",
    "def prob_entropy(probs):\n",
    "    \"Calculate entropy of `probs` distribution\"\n",
    "    probs = prob_normalize(probs)\n",
    "    probs = probs[probs > 0]  # Remove zeros to avoid log(0)\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "def prob_kl_div(p, q):\n",
    "    \"KL divergence from `q` to `p`\"\n",
    "    p, q = prob_normalize(p), prob_normalize(q)\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    return np.sum(p * np.log((p + eps) / (q + eps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test probability utilities\n",
    "probs = [1, 2, 3]\n",
    "normalized = prob_normalize(probs)\n",
    "test_close(np.sum(normalized), 1.0)\n",
    "test_close(normalized, [1/6, 2/6, 3/6])\n",
    "\n",
    "# Test sampling\n",
    "rng = np.random.default_rng(42)\n",
    "samples = prob_sample([0.1, 0.7, 0.2], n=1000, rng=rng)\n",
    "assert len(samples) == 1000\n",
    "assert np.all((samples >= 0) & (samples <= 2))\n",
    "\n",
    "# Test entropy\n",
    "uniform = [0.5, 0.5]\n",
    "certain = [1.0, 0.0]\n",
    "assert prob_entropy(uniform) > prob_entropy(certain)\n",
    "\n",
    "# Test KL divergence\n",
    "p = [0.5, 0.5]\n",
    "q = [0.5, 0.5]\n",
    "test_close(prob_kl_div(p, q), 0.0, eps=1e-10)  # Same distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Core\n",
    "\n",
    "Core Bayesian inference functions for updating beliefs with evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def bayes_update(prior, likelihood, evidence=None):\n",
    "    \"Update `prior` with `likelihood` and optional `evidence`\"\n",
    "    prior, likelihood = np.array(prior), np.array(likelihood)\n",
    "    if evidence is None: evidence = (prior * likelihood).sum()\n",
    "    if evidence == 0: raise ValueError(\"Impossible observation\")\n",
    "    return (prior * likelihood) / evidence\n",
    "\n",
    "def bayes_sequential(priors, likelihoods, evidences=None):\n",
    "    \"Sequential updating of `priors` with `likelihoods` and optional `evidences`\"\n",
    "    if evidences is None:\n",
    "        evidences = [None] * len(likelihoods)\n",
    "    \n",
    "    posterior = np.array(priors)\n",
    "    posteriors = [posterior.copy()]\n",
    "    \n",
    "    for likelihood, evidence in zip(likelihoods, evidences):\n",
    "        posterior = bayes_update(posterior, likelihood, evidence)\n",
    "        posteriors.append(posterior.copy())\n",
    "    \n",
    "    return np.array(posteriors)\n",
    "\n",
    "def bayes_posterior_predictive(posterior, likelihood_fn, n_samples=1000, rng=None):\n",
    "    \"Sample from posterior predictive distribution\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    # Sample parameter values from posterior\n",
    "    param_samples = prob_sample(posterior, n_samples, rng)\n",
    "    \n",
    "    # Generate predictions for each parameter sample\n",
    "    predictions = []\n",
    "    for param_idx in param_samples:\n",
    "        # likelihood_fn should return a distribution over observations\n",
    "        obs_dist = likelihood_fn(param_idx)\n",
    "        obs_sample = prob_sample(obs_dist, 1, rng)[0]\n",
    "        predictions.append(obs_sample)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Bayesian core functions\n",
    "prior = np.array([0.3, 0.7])\n",
    "likelihood = np.array([0.8, 0.2])\n",
    "posterior = bayes_update(prior, likelihood)\n",
    "test_close(np.sum(posterior), 1.0)\n",
    "assert posterior[0] > prior[0]  # First hypothesis should increase\n",
    "\n",
    "# Test sequential updating\n",
    "priors = [0.5, 0.5]\n",
    "likelihoods = [[0.9, 0.1], [0.8, 0.2], [0.7, 0.3]]\n",
    "posteriors = bayes_sequential(priors, likelihoods)\n",
    "assert posteriors.shape == (4, 2)  # Initial + 3 updates\n",
    "test_close(np.sum(posteriors, axis=1), 1.0)  # All normalized\n",
    "\n",
    "# Test posterior predictive (simple case)\n",
    "posterior = [0.6, 0.4]\n",
    "def simple_likelihood(param_idx):\n",
    "    if param_idx == 0:\n",
    "        return [0.8, 0.2]  # Biased toward observation 0\n",
    "    else:\n",
    "        return [0.3, 0.7]  # Biased toward observation 1\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "predictions = bayes_posterior_predictive(posterior, simple_likelihood, n_samples=100, rng=rng)\n",
    "assert len(predictions) == 100\n",
    "assert np.all((predictions >= 0) & (predictions <= 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter Foundation\n",
    "\n",
    "Core particle filter functions for Monte Carlo-based Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pf_init(n_particles, state_dim, init_fn=None, rng=None):\n",
    "    \"Initialize particle filter with `n_particles` and `state_dim`\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    if init_fn is None:\n",
    "        # Default: uniform initialization in [0, 1]\n",
    "        particles = rng.uniform(0, 1, size=(n_particles, state_dim))\n",
    "    else:\n",
    "        particles = init_fn(n_particles, state_dim, rng)\n",
    "    \n",
    "    weights = np.ones(n_particles) / n_particles\n",
    "    return particles, weights\n",
    "\n",
    "def pf_predict(particles, weights, transition_fn, rng=None):\n",
    "    \"Prediction step: apply `transition_fn` to `particles`\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    new_particles = np.zeros_like(particles)\n",
    "    for i, particle in enumerate(particles):\n",
    "        new_particles[i] = transition_fn(particle, rng)\n",
    "    \n",
    "    return new_particles, weights  # Weights unchanged in prediction\n",
    "\n",
    "def pf_update(particles, weights, observation, likelihood_fn):\n",
    "    \"Update step: weight `particles` using `observation` and `likelihood_fn`\"\n",
    "    new_weights = np.zeros_like(weights)\n",
    "    \n",
    "    for i, particle in enumerate(particles):\n",
    "        new_weights[i] = weights[i] * likelihood_fn(particle, observation)\n",
    "    \n",
    "    # Normalize weights\n",
    "    if np.sum(new_weights) > 0:\n",
    "        new_weights = prob_normalize(new_weights)\n",
    "    else:\n",
    "        # If all weights are zero, reset to uniform\n",
    "        new_weights = np.ones_like(weights) / len(weights)\n",
    "    \n",
    "    return particles, new_weights\n",
    "\n",
    "def pf_resample(particles, weights, method='systematic', rng=None):\n",
    "    \"Resample `particles` using `weights` with specified `method`\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    n_particles = len(particles)\n",
    "    \n",
    "    if method == 'systematic':\n",
    "        # Systematic resampling\n",
    "        positions = (np.arange(n_particles) + rng.uniform()) / n_particles\n",
    "        cum_weights = np.cumsum(weights)\n",
    "        indices = np.searchsorted(cum_weights, positions)\n",
    "    elif method == 'multinomial':\n",
    "        # Multinomial resampling\n",
    "        indices = prob_sample(weights, n_particles, rng)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown resampling method: {method}\")\n",
    "    \n",
    "    new_particles = particles[indices]\n",
    "    new_weights = np.ones(n_particles) / n_particles\n",
    "    \n",
    "    return new_particles, new_weights\n",
    "\n",
    "def pf_effective_size(weights):\n",
    "    \"Calculate effective sample size of normalized `weights`\"\n",
    "    weights = prob_normalize(weights)  # Ensure weights are normalized\n",
    "    return 1.0 / np.sum(weights**2)\n",
    "\n",
    "def pf_step(particles, weights, observation, transition_fn, likelihood_fn, \n",
    "           resample_threshold=0.5, rng=None):\n",
    "    \"Complete particle filter step: predict, update, and conditionally resample\"\n",
    "    # Prediction\n",
    "    particles, weights = pf_predict(particles, weights, transition_fn, rng)\n",
    "    \n",
    "    # Update\n",
    "    particles, weights = pf_update(particles, weights, observation, likelihood_fn)\n",
    "    \n",
    "    # Conditional resampling\n",
    "    eff_size = pf_effective_size(weights)\n",
    "    if eff_size < resample_threshold * len(particles):\n",
    "        particles, weights = pf_resample(particles, weights, rng=rng)\n",
    "    \n",
    "    return particles, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test particle filter functions\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Test initialization\n",
    "particles, weights = pf_init(100, 2, rng=rng)\n",
    "assert particles.shape == (100, 2)\n",
    "assert len(weights) == 100\n",
    "test_close(np.sum(weights), 1.0)\n",
    "\n",
    "# Test prediction with simple transition\n",
    "def simple_transition(particle, rng):\n",
    "    return particle + rng.normal(0, 0.1, size=particle.shape)\n",
    "\n",
    "new_particles, new_weights = pf_predict(particles, weights, simple_transition, rng)\n",
    "assert new_particles.shape == particles.shape\n",
    "test_close(new_weights, weights)  # Weights unchanged\n",
    "\n",
    "# Test update with simple likelihood\n",
    "def simple_likelihood(particle, observation):\n",
    "    # Gaussian likelihood\n",
    "    diff = np.linalg.norm(particle - observation)\n",
    "    return np.exp(-0.5 * diff**2)\n",
    "\n",
    "observation = np.array([0.5, 0.5])\n",
    "particles, weights = pf_update(particles, weights, observation, simple_likelihood)\n",
    "test_close(np.sum(weights), 1.0)\n",
    "\n",
    "# Test resampling\n",
    "particles, weights = pf_resample(particles, weights, rng=rng)\n",
    "test_close(np.sum(weights), 1.0)\n",
    "test_close(weights, np.ones(100)/100)  # Should be uniform after resampling\n",
    "\n",
    "# Test effective sample size\n",
    "uniform_weights = np.ones(100) / 100\n",
    "skewed_weights = np.zeros(100)\n",
    "skewed_weights[0] = 1.0\n",
    "assert pf_effective_size(uniform_weights) > pf_effective_size(skewed_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBE Estimator\n",
    "\n",
    "Main recursive Bayesian estimator implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rbe_estimator(observations, transition_fn, likelihood_fn, \n",
    "                 n_particles=1000, init_fn=None, rng=None):\n",
    "    \"Main RBE estimator for `observations` with particle filter\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    # Infer state dimension from first observation\n",
    "    state_dim = len(observations[0]) if hasattr(observations[0], '__len__') else 1\n",
    "    \n",
    "    # Initialize particles\n",
    "    particles, weights = pf_init(n_particles, state_dim, init_fn, rng)\n",
    "    \n",
    "    # Store results\n",
    "    estimates = []\n",
    "    particle_history = [particles.copy()]\n",
    "    weight_history = [weights.copy()]\n",
    "    \n",
    "    for obs in observations:\n",
    "        # Particle filter step\n",
    "        particles, weights = pf_step(particles, weights, obs, \n",
    "                                   transition_fn, likelihood_fn, rng=rng)\n",
    "        \n",
    "        # Estimate (weighted mean)\n",
    "        estimate = np.average(particles, weights=weights, axis=0)\n",
    "        estimates.append(estimate)\n",
    "        \n",
    "        # Store history\n",
    "        particle_history.append(particles.copy())\n",
    "        weight_history.append(weights.copy())\n",
    "    \n",
    "    return {\n",
    "        'estimates': np.array(estimates),\n",
    "        'particles': particle_history,\n",
    "        'weights': weight_history\n",
    "    }\n",
    "\n",
    "def rbe_adaptive(observations, transition_fn, likelihood_fn, \n",
    "                adapt_rate=0.01, n_particles=1000, init_fn=None, rng=None):\n",
    "    \"Adaptive RBE estimator that adjusts parameters based on performance\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    # Start with base estimator\n",
    "    result = rbe_estimator(observations, transition_fn, likelihood_fn, \n",
    "                          n_particles, init_fn, rng)\n",
    "    \n",
    "    # Simple adaptation: adjust resampling threshold based on effective size\n",
    "    # This is a placeholder for more sophisticated adaptation\n",
    "    avg_eff_size = np.mean([pf_effective_size(w) for w in result['weights']])\n",
    "    adapted_threshold = max(0.1, min(0.9, 0.5 + adapt_rate * (avg_eff_size - n_particles/2)))\n",
    "    \n",
    "    result['adaptation_info'] = {\n",
    "        'avg_effective_size': avg_eff_size,\n",
    "        'adapted_threshold': adapted_threshold\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def rbe_metrics(true_states, estimates):\n",
    "    \"Calculate performance metrics for RBE estimates\"\n",
    "    true_states = np.array(true_states)\n",
    "    estimates = np.array(estimates)\n",
    "    \n",
    "    # Mean squared error\n",
    "    mse = np.mean((true_states - estimates)**2)\n",
    "    \n",
    "    # Root mean squared error\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Mean absolute error\n",
    "    mae = np.mean(np.abs(true_states - estimates))\n",
    "    \n",
    "    # Maximum absolute error\n",
    "    max_ae = np.max(np.abs(true_states - estimates))\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'max_absolute_error': max_ae,\n",
    "        'n_samples': len(estimates)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RBE estimator functions\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "true_states = [np.array([i * 0.1, i * 0.05]) for i in range(10)]\n",
    "observations = [state + rng.normal(0, 0.1, 2) for state in true_states]\n",
    "\n",
    "# Define simple transition and likelihood\n",
    "def test_transition(particle, rng):\n",
    "    return particle + rng.normal(0, 0.05, particle.shape)\n",
    "\n",
    "def test_likelihood(particle, observation):\n",
    "    diff = np.linalg.norm(particle - observation)\n",
    "    return np.exp(-0.5 * (diff / 0.1)**2)\n",
    "\n",
    "# Test basic estimator\n",
    "result = rbe_estimator(observations, test_transition, test_likelihood, \n",
    "                      n_particles=100, rng=rng)\n",
    "assert result['estimates'].shape == (10, 2)\n",
    "assert len(result['particles']) == 11  # Initial + 10 steps\n",
    "assert len(result['weights']) == 11\n",
    "\n",
    "# Test adaptive estimator\n",
    "adaptive_result = rbe_adaptive(observations, test_transition, test_likelihood,\n",
    "                              n_particles=100, rng=rng)\n",
    "assert 'adaptation_info' in adaptive_result\n",
    "assert 'avg_effective_size' in adaptive_result['adaptation_info']\n",
    "\n",
    "# Test metrics\n",
    "metrics = rbe_metrics(true_states, result['estimates'])\n",
    "assert 'mse' in metrics\n",
    "assert 'rmse' in metrics\n",
    "assert 'mae' in metrics\n",
    "assert metrics['n_samples'] == 10\n",
    "assert metrics['rmse'] == np.sqrt(metrics['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Helpers\n",
    "\n",
    "Helper functions for visualizing RBE results and particle filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def viz_particles(particles, weights, title='Particle Distribution', \n",
    "                 figsize=(8, 6), alpha=0.6):\n",
    "    \"Visualize `particles` with `weights`\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if particles.shape[1] == 1:\n",
    "        # 1D case: histogram\n",
    "        ax.hist(particles.flatten(), weights=weights, bins=30, alpha=alpha)\n",
    "        ax.set_xlabel('State')\n",
    "        ax.set_ylabel('Probability Density')\n",
    "    elif particles.shape[1] == 2:\n",
    "        # 2D case: scatter plot\n",
    "        scatter = ax.scatter(particles[:, 0], particles[:, 1], \n",
    "                           s=weights*1000, alpha=alpha)\n",
    "        ax.set_xlabel('State Dimension 1')\n",
    "        ax.set_ylabel('State Dimension 2')\n",
    "    else:\n",
    "        # Higher dimensions: just show first two\n",
    "        scatter = ax.scatter(particles[:, 0], particles[:, 1], \n",
    "                           s=weights*1000, alpha=alpha)\n",
    "        ax.set_xlabel('State Dimension 1')\n",
    "        ax.set_ylabel('State Dimension 2')\n",
    "        title += f' (showing dims 1-2 of {particles.shape[1]})'\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    return fig, ax\n",
    "\n",
    "def viz_beliefs(beliefs, time_steps=None, title='Belief Evolution', \n",
    "               figsize=(10, 6), labels=None):\n",
    "    \"Visualize evolution of `beliefs` over `time_steps`\"\n",
    "    beliefs = np.array(beliefs)\n",
    "    if time_steps is None:\n",
    "        time_steps = np.arange(len(beliefs))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if beliefs.ndim == 2:\n",
    "        # Multiple belief dimensions\n",
    "        for i in range(beliefs.shape[1]):\n",
    "            label = f'Belief {i+1}' if labels is None else labels[i]\n",
    "            ax.plot(time_steps, beliefs[:, i], label=label, marker='o')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        # Single belief dimension\n",
    "        ax.plot(time_steps, beliefs, marker='o')\n",
    "    \n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Belief Value')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    return fig, ax\n",
    "\n",
    "def viz_comparison(methods_data, time_steps=None, title='Method Comparison',\n",
    "                  figsize=(12, 8), metrics=['mse', 'mae']):\n",
    "    \"Compare multiple methods with `methods_data` dict\"\n",
    "    if time_steps is None:\n",
    "        # Try to infer time_steps from data\n",
    "        first_method = list(methods_data.keys())[0]\n",
    "        first_data = methods_data[first_method]\n",
    "        \n",
    "        # Look for estimates first, then fall back to any available metric\n",
    "        if 'estimates' in first_data:\n",
    "            time_steps = np.arange(len(first_data['estimates']))\n",
    "        else:\n",
    "            # Use the first available metric to infer length\n",
    "            available_metrics = [m for m in metrics if m in first_data]\n",
    "            if available_metrics:\n",
    "                time_steps = np.arange(len(first_data[available_metrics[0]]))\n",
    "            else:\n",
    "                # Default fallback\n",
    "                time_steps = np.arange(10)\n",
    "    \n",
    "    n_metrics = len(metrics)\n",
    "    fig, axes = plt.subplots(n_metrics, 1, figsize=figsize)\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        for method_name, method_data in methods_data.items():\n",
    "            if metric in method_data:\n",
    "                ax.plot(time_steps, method_data[metric], \n",
    "                       label=method_name, marker='o')\n",
    "        \n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.set_title(f'{title} - {metric.upper()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "def viz_rbe_summary(rbe_result, true_states=None, title='RBE Summary',\n",
    "                   figsize=(15, 10)):\n",
    "    \"Create comprehensive summary visualization of RBE results\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Layout: 2x2 grid\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Top left: Final particle distribution\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    final_particles = rbe_result['particles'][-1]\n",
    "    final_weights = rbe_result['weights'][-1]\n",
    "    \n",
    "    if final_particles.shape[1] >= 2:\n",
    "        ax1.scatter(final_particles[:, 0], final_particles[:, 1], \n",
    "                   s=final_weights*1000, alpha=0.6)\n",
    "        ax1.set_xlabel('State Dim 1')\n",
    "        ax1.set_ylabel('State Dim 2')\n",
    "    else:\n",
    "        ax1.hist(final_particles.flatten(), weights=final_weights, bins=30, alpha=0.6)\n",
    "        ax1.set_xlabel('State')\n",
    "        ax1.set_ylabel('Density')\n",
    "    ax1.set_title('Final Particle Distribution')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top right: Estimates over time\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    estimates = rbe_result['estimates']\n",
    "    time_steps = np.arange(len(estimates))\n",
    "    \n",
    "    if estimates.ndim == 2 and estimates.shape[1] >= 2:\n",
    "        ax2.plot(time_steps, estimates[:, 0], 'b-', label='Dim 1', marker='o')\n",
    "        ax2.plot(time_steps, estimates[:, 1], 'r-', label='Dim 2', marker='s')\n",
    "        if true_states is not None:\n",
    "            true_states = np.array(true_states)\n",
    "            ax2.plot(time_steps, true_states[:, 0], 'b--', alpha=0.7, label='True Dim 1')\n",
    "            ax2.plot(time_steps, true_states[:, 1], 'r--', alpha=0.7, label='True Dim 2')\n",
    "        ax2.legend()\n",
    "    else:\n",
    "        ax2.plot(time_steps, estimates.flatten(), 'b-', marker='o', label='Estimate')\n",
    "        if true_states is not None:\n",
    "            ax2.plot(time_steps, np.array(true_states).flatten(), 'r--', alpha=0.7, label='True')\n",
    "        ax2.legend()\n",
    "    \n",
    "    ax2.set_xlabel('Time Step')\n",
    "    ax2.set_ylabel('State Value')\n",
    "    ax2.set_title('Estimates Over Time')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bottom left: Effective sample size\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    eff_sizes = [pf_effective_size(w) for w in rbe_result['weights']]\n",
    "    ax3.plot(np.arange(len(eff_sizes)), eff_sizes, 'g-', marker='o')\n",
    "    ax3.axhline(len(rbe_result['weights'][0])/2, color='r', linestyle='--', alpha=0.7, label='N/2')\n",
    "    ax3.set_xlabel('Time Step')\n",
    "    ax3.set_ylabel('Effective Sample Size')\n",
    "    ax3.set_title('Particle Filter Health')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bottom right: Error metrics (if true states provided)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    if true_states is not None:\n",
    "        errors = np.array(true_states) - estimates\n",
    "        if errors.ndim == 2:\n",
    "            error_norms = np.linalg.norm(errors, axis=1)\n",
    "        else:\n",
    "            error_norms = np.abs(errors)\n",
    "        \n",
    "        ax4.plot(time_steps, error_norms, 'r-', marker='o')\n",
    "        ax4.set_xlabel('Time Step')\n",
    "        ax4.set_ylabel('Estimation Error')\n",
    "        ax4.set_title('Error Over Time')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No true states\\nprovided', ha='center', va='center', \n",
    "                transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.set_title('Error Analysis')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visualization functions (basic functionality)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Create test data\n",
    "particles = rng.normal(0, 1, (100, 2))\n",
    "weights = rng.exponential(1, 100)\n",
    "weights = prob_normalize(weights)\n",
    "\n",
    "# Test particle visualization\n",
    "fig, ax = viz_particles(particles, weights)\n",
    "assert fig is not None\n",
    "plt.close(fig)\n",
    "\n",
    "# Test belief visualization\n",
    "beliefs = np.random.random((10, 3))\n",
    "fig, ax = viz_beliefs(beliefs)\n",
    "assert fig is not None\n",
    "plt.close(fig)\n",
    "\n",
    "# Test comparison visualization\n",
    "methods_data = {\n",
    "    'Method A': {\n",
    "        'estimates': np.random.random(10), \n",
    "        'mse': np.random.random(10), \n",
    "        'mae': np.random.random(10)\n",
    "    },\n",
    "    'Method B': {\n",
    "        'estimates': np.random.random(10), \n",
    "        'mse': np.random.random(10), \n",
    "        'mae': np.random.random(10)\n",
    "    }\n",
    "}\n",
    "fig, axes = viz_comparison(methods_data)\n",
    "assert fig is not None\n",
    "plt.close(fig)\n",
    "\n",
    "# Test RBE summary visualization\n",
    "# Use the RBE result from previous test\n",
    "true_states = [np.array([i * 0.1, i * 0.05]) for i in range(10)]\n",
    "observations = [state + rng.normal(0, 0.1, 2) for state in true_states]\n",
    "result = rbe_estimator(observations, test_transition, test_likelihood, \n",
    "                      n_particles=50, rng=rng)  # Smaller for faster test\n",
    "\n",
    "fig = viz_rbe_summary(result, true_states)\n",
    "assert fig is not None\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Functions\n",
    "\n",
    "Define all functions to be exported from this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "__all__ = [\n",
    "    # Probability utilities\n",
    "    'prob_normalize', 'prob_sample', 'prob_entropy', 'prob_kl_div',\n",
    "    \n",
    "    # Bayesian core\n",
    "    'bayes_update', 'bayes_sequential', 'bayes_posterior_predictive',\n",
    "    \n",
    "    # Particle filter foundation\n",
    "    'pf_init', 'pf_predict', 'pf_update', 'pf_resample', 'pf_effective_size', 'pf_step',\n",
    "    \n",
    "    # RBE estimator\n",
    "    'rbe_estimator', 'rbe_adaptive', 'rbe_metrics',\n",
    "    \n",
    "    # Visualization helpers\n",
    "    'viz_particles', 'viz_beliefs', 'viz_comparison', 'viz_rbe_summary'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module provides the complete foundation for Recursive Bayesian Estimation following fast.ai coding principles:\n",
    "\n",
    "- **Short, clear names**: `prob_normalize`, `bayes_update`, `pf_init`\n",
    "- **Function-first approach**: Minimal classes, comprehensive functions\n",
    "- **Liberal imports**: Full `__all__` definition for `from rbe.core import *`\n",
    "- **Inline testing**: Comprehensive tests using `test_eq` and `test_close`\n",
    "- **Mathematical focus**: Functions optimized for interactive exploration\n",
    "- **Modular design**: Mix-and-match functionality for different use cases\n",
    "\n",
    "The module supports the entire RBE blog series with:\n",
    "- Core probability and Bayesian inference functions\n",
    "- Complete particle filter implementation\n",
    "- Ready-to-use RBE estimators\n",
    "- Comprehensive visualization tools\n",
    "- Performance metrics and analysis functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
