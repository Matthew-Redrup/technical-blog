{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Core\n",
    "\n",
    "> Core Bayesian inference functions - updates, sequential processing, and posterior predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rbe.bayes_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from typing import Optional, Union, List, Callable\n",
    "from fastcore.test import test_eq, test_close\n",
    "from fastcore.all import *\n",
    "from technical_blog.rbe.probability import normalize, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Bayesian Updates\n",
    "\n",
    "The heart of Bayesian inference - updating beliefs with evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `update()` function is the core implementation of **Bayes' theorem** - it's how we mathematically update our beliefs when we receive new evidence. Let me break it down:\n",
    "\n",
    "### What Bayes' Theorem Does\n",
    "\n",
    "Bayes' theorem tells us how to revise our beliefs (prior) when we observe new evidence:\n",
    "\n",
    "$$P(H|E) = \\frac{P(E|H) \\times P(H)}{P(E)}$$\n",
    "\n",
    "Where:\n",
    "- **P(H|E)** = posterior (updated belief after seeing evidence)\n",
    "- **P(E|H)** = likelihood (how probable the evidence is under each hypothesis)\n",
    "- **P(H)** = prior (our initial belief before seeing evidence)\n",
    "- **P(E)** = evidence (total probability of observing this evidence)\n",
    "\n",
    "## How the Function Works\n",
    "\n",
    "```python\n",
    "def update(prior, likelihood, evidence=None):\n",
    "    # Returns: (prior * likelihood) / evidence\n",
    "```\n",
    "\n",
    "**Step 1: Input Validation**\n",
    "- Ensures prior and likelihood have the same shape\n",
    "- Checks for non-negative values (probabilities can't be negative)\n",
    "- Auto-normalizes the prior if it doesn't sum to 1\n",
    "\n",
    "**Step 2: Calculate Evidence**\n",
    "If not provided, evidence is computed as: `evidence = sum(prior * likelihood)`\n",
    "\n",
    "This represents the total probability of seeing the evidence across all possible hypotheses.\n",
    "\n",
    "**Step 3: Apply Bayes' Rule**\n",
    "Returns `(prior * likelihood) / evidence`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def update(prior, # Prior probabilities\n",
    "           likelihood, # Likelihood of evidence given hypothesis\n",
    "           evidence=None # Optional evidence, defaults to sum(prior * likelihood)\n",
    "           ):\n",
    "    \"\"\"Update prior beliefs with likelihood using Bayes' theorem.\"\"\"\n",
    "    prior = np.asarray(prior, dtype=np.float64)\n",
    "    likelihood = np.asarray(likelihood, dtype=np.float64)\n",
    "    \n",
    "    # Validate inputs\n",
    "    if prior.shape != likelihood.shape: raise ValueError(f\"Prior and likelihood shapes don't match: {prior.shape} vs {likelihood.shape}\")\n",
    "    if np.any(prior < 0) or np.any(likelihood < 0): raise ValueError(\"Prior and likelihood must be non-negative\")\n",
    "    # Normalize prior if needed (common in practice)\n",
    "    if not np.isclose(np.sum(prior), 1.0): prior = normalize(prior)\n",
    "    # Compute evidence if not provided\n",
    "    if evidence is None: evidence = np.sum(prior * likelihood)\n",
    "    # Check for impossible observation\n",
    "    if evidence == 0: raise ValueError(\"Impossible observation: zero evidence\")\n",
    "    # Numerical stability check\n",
    "    if evidence < 1e-15:\n",
    "        import warnings\n",
    "        warnings.warn(\"Very small evidence value - numerical instability possible\")\n",
    "    \n",
    "    return (prior * likelihood) / evidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyber Security Example\n",
    "\n",
    "Imagine you're detecting network intrusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10714286, 0.66666667, 0.22619048])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prior beliefs about network state\n",
    "prior = [0.9, 0.08, 0.02]  # [normal, suspicious, attack]\n",
    "\n",
    "# New evidence: unusual port scanning detected\n",
    "# Likelihood of seeing port scans under each hypothesis\n",
    "likelihood = [0.01, 0.7, 0.95]  # Very unlikely if normal, likely if attack\n",
    "\n",
    "# Update beliefs\n",
    "posterior = update(prior, likelihood)\n",
    "# Result: attack probability increases significantly!\n",
    "posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Features for Robust Applications\n",
    "\n",
    "**Automatic Normalization**: Handles unnormalized priors (common when combining multiple sources)\n",
    "\n",
    "**Error Handling**: \n",
    "- Detects impossible observations (zero evidence)\n",
    "- Warns about numerical instability\n",
    "- Validates input shapes and non-negativity\n",
    "\n",
    "**Numerical Stability**: Uses float64 precision to handle the tiny probabilities common in some applications\n",
    "\n",
    "The beauty is that this single function encapsulates the mathematical foundation of all Bayesian learning - whether you're tracking individual threats or updating complex network models, it all comes down to this core update rule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Bayesian updates\n",
    "prior = np.array([0.3, 0.7])\n",
    "likelihood = np.array([0.8, 0.2])\n",
    "posterior = update(prior, likelihood)\n",
    "test_close(np.sum(posterior), 1.0)\n",
    "assert posterior[0] > prior[0]  # First hypothesis should increase\n",
    "# Test with unnormalized prior (common in practice)\n",
    "unnorm_prior = [3, 7]  # Sums to 10, not 1\n",
    "likelihood = [0.8, 0.2]\n",
    "posterior = update(unnorm_prior, likelihood)\n",
    "test_close(np.sum(posterior), 1.0)\n",
    "\n",
    "# Test numerical stability with tiny values\n",
    "tiny_prior = [1e-10, 1-1e-10]\n",
    "tiny_likelihood = [1e-10, 1-1e-10]\n",
    "posterior = update(tiny_prior, tiny_likelihood)\n",
    "test_close(np.sum(posterior), 1.0)\n",
    "\n",
    "# Test shape mismatch error\n",
    "try:\n",
    "    update([0.5, 0.5], [0.8, 0.2, 0.1])\n",
    "    assert False, \"Should raise ValueError for shape mismatch\"\n",
    "except ValueError as e:\n",
    "    assert \"shapes don't match\" in str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "The `sequential` function implements **sequential Bayesian updating** - it's how you process multiple observations one after another, updating your beliefs with each new piece of evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sequential(priors,  # prior probabilities of hypotheses \n",
    "               likelihoods, # likelihoods of observations given hypotheses\n",
    "               evidences=None # evidence for each observation\n",
    "               ):\n",
    "    \"\"\"Sequential Bayesian updates with multiple observations.\"\"\"\n",
    "    priors = np.asarray(priors, dtype=np.float64)\n",
    "    likelihoods = np.asarray(likelihoods, dtype=np.float64)\n",
    "    \n",
    "    # Validate inputs\n",
    "    if len(likelihoods) == 0:\n",
    "        return np.array([priors])\n",
    "    \n",
    "    if likelihoods.ndim != 2:\n",
    "        raise ValueError(\"Likelihoods must be 2D array (n_observations, n_hypotheses)\")\n",
    "    \n",
    "    if likelihoods.shape[1] != len(priors):\n",
    "        raise ValueError(f\"Likelihood shape {likelihoods.shape} incompatible with prior length {len(priors)}\")\n",
    "    \n",
    "    if evidences is None:\n",
    "        evidences = [None] * len(likelihoods)\n",
    "    elif len(evidences) != len(likelihoods):\n",
    "        raise ValueError(\"Number of evidences must match number of likelihoods\")\n",
    "    \n",
    "    # Perform sequential updates\n",
    "    posterior = priors.copy()\n",
    "    posteriors = [posterior.copy()]\n",
    "    \n",
    "    for likelihood, evidence in zip(likelihoods, evidences):\n",
    "        posterior = update(posterior, likelihood, evidence)\n",
    "        posteriors.append(posterior.copy())\n",
    "    \n",
    "    return np.array(posteriors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What It Does\n",
    "\n",
    "Instead of updating beliefs with just one observation (like the basic `update` function), `sequential` handles a whole series of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95      , 0.04      , 0.01      ],\n",
       "       [0.7421875 , 0.1875    , 0.0703125 ],\n",
       "       [0.14615385, 0.59076923, 0.26307692],\n",
       "       [0.07483261, 0.45372194, 0.47144545],\n",
       "       [0.49414824, 0.33289987, 0.17295189]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with initial beliefs\n",
    "prior = [0.95, 0.04, 0.01]  # [normal, suspicious, attack]\n",
    "\n",
    "# Process multiple observations over time\n",
    "observations = [\n",
    "    [0.1, 0.6, 0.9],   # High anomaly score\n",
    "    [0.05, 0.8, 0.95], # Even higher anomaly  \n",
    "    [0.2, 0.3, 0.7],   # Moderate anomaly\n",
    "    [0.9, 0.1, 0.05]   # Back to normal\n",
    "]\n",
    "\n",
    "timeline = sequential(prior, observations)\n",
    "# Returns: array of beliefs after each observation\n",
    "timeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### How It Works\n",
    "\n",
    "The function performs these steps:\n",
    "\n",
    "1. **Starts with your prior beliefs**\n",
    "2. **For each observation**:\n",
    "   - Takes current beliefs as the \"prior\" for this update\n",
    "   - Applies Bayes' theorem using the observation's likelihood\n",
    "   - The resulting posterior becomes the prior for the next observation\n",
    "3. **Returns the complete timeline** of how beliefs evolved\n",
    "\n",
    "Mathematically, it's chaining Bayes' updates:\n",
    "- After obs 1: `P(H|obs1) = P(obs1|H) × P(H) / P(obs1)`\n",
    "- After obs 2: `P(H|obs1,obs2) = P(obs2|H) × P(H|obs1) / P(obs2)`\n",
    "- And so on...\n",
    "\n",
    "#### Key Features for Cyber Security\n",
    "\n",
    "**Timeline Tracking**: You get the complete evolution of beliefs, not just the final result. This lets you see:\n",
    "- When threat probability peaked\n",
    "- How quickly beliefs changed\n",
    "- Whether the system is converging or oscillating\n",
    "\n",
    "**Robust Error Handling**: \n",
    "- Validates that likelihoods are properly shaped (2D array)\n",
    "- Ensures evidence counts match observations\n",
    "- Handles edge cases like empty observation sequences\n",
    "\n",
    "**Memory Efficiency**: Processes observations one at a time rather than requiring all data in memory simultaneously.\n",
    "\n",
    "#### Cyber Security Example\n",
    "\n",
    "In a network anomaly detection scenario:\n",
    "\n",
    "```python\n",
    "# Each row represents likelihood of observation under each network state\n",
    "# [normal_likelihood, suspicious_likelihood, attack_likelihood]\n",
    "network_observations = [\n",
    "    [0.1, 0.6, 0.9],   # Suspicious traffic pattern\n",
    "    [0.05, 0.8, 0.95], # Even more suspicious\n",
    "    [0.9, 0.1, 0.05]   # Returns to normal\n",
    "]\n",
    "\n",
    "belief_timeline = sequential(network_prior, network_observations)\n",
    "```\n",
    "\n",
    "This gives you a complete picture of how your RBE's confidence in different threat levels evolved as new network data arrived - essential for understanding both the current threat state and the system's decision-making process.\n",
    "\n",
    "The function essentially turns your single-shot Bayesian update into a learning system that accumulates evidence over time!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Edge Cases & Error Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequential updating\n",
    "priors = [0.5, 0.5]\n",
    "likelihoods = [[0.9, 0.1], [0.8, 0.2], [0.7, 0.3]]\n",
    "posteriors = sequential(priors, likelihoods)\n",
    "assert posteriors.shape == (4, 2)  # Initial + 3 updates\n",
    "test_close(np.sum(posteriors, axis=1), 1.0)  # All normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty observations (should return just the prior)\n",
    "empty_result = sequential([0.6, 0.4], [])\n",
    "test_eq(empty_result.shape, (1, 2))\n",
    "test_close(empty_result[0], [0.6, 0.4])\n",
    "\n",
    "# Single observation (common case)\n",
    "single_result = sequential([0.5, 0.5], [[0.8, 0.2]])\n",
    "test_eq(single_result.shape, (2, 2))\n",
    "\n",
    "# Test with custom evidences\n",
    "priors = [0.4, 0.6]\n",
    "likelihoods = [[0.9, 0.1], [0.7, 0.3]]\n",
    "evidences = [0.5, 0.8]  # Custom evidence values\n",
    "result = sequential(priors, likelihoods, evidences)\n",
    "# Should use provided evidences instead of computing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Cyber Security Specific Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic network anomaly scenario\n",
    "network_prior = [0.95, 0.04, 0.01]  # [normal, suspicious, attack]\n",
    "\n",
    "# Sequence of observations over time\n",
    "observations = [\n",
    "    [0.1, 0.6, 0.9],   # High anomaly score\n",
    "    [0.05, 0.8, 0.95], # Even higher anomaly\n",
    "    [0.2, 0.3, 0.7],   # Moderate anomaly\n",
    "    [0.9, 0.1, 0.05]   # Back to normal\n",
    "]\n",
    "\n",
    "timeline = sequential(network_prior, observations)\n",
    "\n",
    "# Verify attack probability peaks and then decreases\n",
    "attack_probs = timeline[:, 2]  # Extract attack column\n",
    "peak_idx = np.argmax(attack_probs[1:]) + 1  # Skip initial prior\n",
    "assert attack_probs[peak_idx] > attack_probs[0], \"Attack probability should increase\"\n",
    "assert attack_probs[-1] < attack_probs[peak_idx], \"Should decrease after normal observation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Stability Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very small likelihoods (rare events)\n",
    "tiny_likelihoods = [[1e-15, 1-1e-15], [1e-14, 1-1e-14]]\n",
    "result = sequential([0.5, 0.5], tiny_likelihoods)\n",
    "assert np.all(np.isfinite(result)), \"Should handle tiny values\"\n",
    "\n",
    "# Extreme confidence updates\n",
    "extreme_likes = [[0.999, 0.001], [0.001, 0.999]]\n",
    "result = sequential([0.5, 0.5], extreme_likes)\n",
    "# Should handle rapid belief changes without numerical issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Input Validation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong likelihood dimensions\n",
    "try:\n",
    "    sequential([0.5, 0.5], [0.8, 0.2])  # 1D instead of 2D\n",
    "    assert False, \"Should reject 1D likelihoods\"\n",
    "except ValueError as e:\n",
    "    assert \"2D array\" in str(e)\n",
    "\n",
    "# Mismatched evidence count\n",
    "try:\n",
    "    sequential([0.5, 0.5], [[0.8, 0.2]], evidences=[0.5, 0.6])  # 2 evidences, 1 likelihood\n",
    "    assert False, \"Should reject mismatched evidence count\"\n",
    "except ValueError as e:\n",
    "    assert \"match number of likelihoods\" in str(e)\n",
    "\n",
    "# Incompatible shapes\n",
    "try:\n",
    "    sequential([0.5, 0.5], [[0.8, 0.2, 0.1]])  # 3 hypotheses vs 2 in prior\n",
    "    assert False, \"Should reject shape mismatch\"\n",
    "except ValueError as e:\n",
    "    assert \"incompatible\" in str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convergence and Learning Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test convergence with consistent evidence\n",
    "consistent_evidence = [[0.9, 0.1]] * 10  # Same observation repeated\n",
    "result = sequential([0.5, 0.5], consistent_evidence)\n",
    "\n",
    "# Should converge toward first hypothesis\n",
    "final_belief = result[-1, 0]\n",
    "assert final_belief > 0.95, \"Should strongly favor consistent hypothesis\"\n",
    "\n",
    "# Test belief oscillation with conflicting evidence\n",
    "conflicting = [[0.9, 0.1], [0.1, 0.9]] * 5  # Alternating evidence\n",
    "result = sequential([0.5, 0.5], conflicting)\n",
    "# Final belief shouldn't be too extreme in either direction\n",
    "assert 0.2 < result[-1, 0] < 0.8, \"Conflicting evidence should maintain uncertainty\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive\n",
    "\n",
    "Sample from the posterior predictive distribution - what future observations might look like given our current beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predictive` function implements **posterior predictive sampling** - a key technique in Bayesian inference that answers the question: \"Given what I've learned so far, what kinds of observations might I see in the future?\"\n",
    "\n",
    "#### What It Does\n",
    "\n",
    "The function generates synthetic future observations by combining:\n",
    "1. **Your current beliefs** (posterior distribution over parameters/hypotheses)\n",
    "2. **The observation model** (likelihood function that maps parameters to observation probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predictive(posterior, likelihood_fn, n_samples=1000, rng=None):\n",
    "    \"\"\"Vectorized posterior predictive sampling.\"\"\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    posterior = normalize(posterior)\n",
    "    param_samples = sample(posterior, n_samples, rng)\n",
    "    \n",
    "    # Group samples by parameter for efficient batch processing\n",
    "    unique_params, counts = np.unique(param_samples, return_counts=True)\n",
    "    \n",
    "    predictions = []\n",
    "    for param_idx, count in zip(unique_params, counts):\n",
    "        obs_dist = normalize(likelihood_fn(param_idx))\n",
    "        obs_samples = sample(obs_dist, count, rng)\n",
    "        predictions.extend(obs_samples)\n",
    "    \n",
    "    # Shuffle to remove parameter ordering bias\n",
    "    rng.shuffle(predictions)\n",
    "    return np.array(predictions, dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### How It Works\n",
    "\n",
    "The algorithm follows a two-step process that mirrors the generative story of Bayesian models:\n",
    "\n",
    "**Step 1: Sample Parameters**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "param_samples = sample(posterior, n_samples, rng)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This draws parameter values according to your current beliefs. If you're 70% confident in \"normal network state\" and 30% confident in \"attack state\", about 70% of the samples will be \"normal\".\n",
    "\n",
    "**Step 2: Generate Observations**\n",
    "For each sampled parameter, use the likelihood function to determine what observations that parameter would generate:\n",
    "```python\n",
    "obs_dist = normalize(likelihood_fn(param_idx))\n",
    "obs_samples = sample(obs_dist, count, rng)\n",
    "```\n",
    "\n",
    "## Clever Optimization\n",
    "\n",
    "The function uses **vectorized batch processing** for efficiency:\n",
    "\n",
    "```python\n",
    "unique_params, counts = np.unique(param_samples, return_counts=True)\n",
    "```\n",
    "\n",
    "Instead of processing 1000 individual samples, it groups them: \"I need 700 observations from parameter 0 and 300 from parameter 1.\" This is much faster than generating observations one by one.\n",
    "\n",
    "## Cyber Security Applications\n",
    "\n",
    "**1. Threat Forecasting**\n",
    "```python\n",
    "# Current beliefs about network state\n",
    "network_posterior = [0.6, 0.3, 0.1]  # [normal, suspicious, attack]\n",
    "\n",
    "def network_observations(state_idx):\n",
    "    if state_idx == 0:  # Normal state\n",
    "        return [0.9, 0.08, 0.02]  # [normal_traffic, anomaly, alert]\n",
    "    elif state_idx == 1:  # Suspicious state  \n",
    "        return [0.4, 0.5, 0.1]\n",
    "    else:  # Attack state\n",
    "        return [0.1, 0.3, 0.6]\n",
    "\n",
    "# What kinds of network events should we expect?\n",
    "future_events = predictive(network_posterior, network_observations, n_samples=1000)\n",
    "```\n",
    "\n",
    "**2. Anomaly Detection Validation**\n",
    "Generate synthetic data that matches your current model, then compare with actual observations to detect model drift or new attack patterns.\n",
    "\n",
    "**3. Alert System Tuning**\n",
    "Predict how many alerts different threshold settings would generate under your current threat model.\n",
    "\n",
    "**4. Resource Planning**\n",
    "Estimate future computational or analyst workload based on predicted event distributions.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "**Numerical Stability**: Normalizes both posterior and likelihood distributions to ensure valid probabilities.\n",
    "\n",
    "**Reproducibility**: Uses controlled random number generation for consistent results across runs.\n",
    "\n",
    "**Bias Removal**: Shuffles final predictions to remove any ordering artifacts from the batch processing.\n",
    "\n",
    "**Type Safety**: Returns integer indices (not floats) since observations are typically categorical.\n",
    "\n",
    "## Example Output Interpretation\n",
    "\n",
    "If you get predictions like `[0, 0, 1, 0, 0, 2, 0, ...]`, this means:\n",
    "- Most future observations will be type 0 (normal)\n",
    "- Occasional type 1 observations (suspicious)  \n",
    "- Rare type 2 observations (attacks)\n",
    "\n",
    "The relative frequencies tell you what to expect: if 80% are type 0, your model predicts the network will be normal 80% of the time.\n",
    "\n",
    "This is invaluable for **proactive security planning** - instead of just reacting to threats, you can anticipate what's likely to happen and prepare accordingly!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test posterior predictive\n",
    "posterior = [0.6, 0.4]\n",
    "def simple_likelihood(param_idx):\n",
    "    if param_idx == 0:\n",
    "        return [0.8, 0.2]  # Biased toward observation 0\n",
    "    else:\n",
    "        return [0.3, 0.7]  # Biased toward observation 1\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "predictions = predictive(posterior, simple_likelihood, n_samples=100, rng=rng)\n",
    "assert len(predictions) == 100\n",
    "assert np.all((predictions >= 0) & (predictions <= 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = [0.7, 0.3]\n",
    "\n",
    "def likelihood_fn(param_idx):\n",
    "    if param_idx == 0:\n",
    "        return [0.9, 0.1]  # Parameter 0 strongly predicts observation 0\n",
    "    else:\n",
    "        return [0.2, 0.8]  # Parameter 1 strongly predicts observation 1\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "predictions = predictive(posterior, likelihood_fn, n_samples=1000, rng=rng)\n",
    "\n",
    "# Check output format\n",
    "assert predictions.shape == (1000,), \"Should return 1D array\"\n",
    "assert predictions.dtype == int, \"Should return integer indices\"\n",
    "assert np.all((predictions >= 0) & (predictions <= 1)), \"All predictions should be valid indices\"\n",
    "\n",
    "# Check statistical properties\n",
    "# Since posterior favors param 0 (0.7 vs 0.3), and param 0 favors obs 0 (0.9 vs 0.1),\n",
    "# we should see more 0s than 1s in predictions\n",
    "obs_0_count = np.sum(predictions == 0)\n",
    "obs_1_count = np.sum(predictions == 1)\n",
    "assert obs_0_count > obs_1_count, \"Should predict observation 0 more often\"\n",
    "\n",
    "# Rough check: expect about 70% * 90% + 30% * 20% = 69% observation 0\n",
    "expected_ratio = 0.7 * 0.9 + 0.3 * 0.2  # ≈ 0.69\n",
    "actual_ratio = obs_0_count / 1000\n",
    "assert abs(actual_ratio - expected_ratio) < 0.05, f\"Expected ~{expected_ratio:.2f}, got {actual_ratio:.2f}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Factors\n",
    "\n",
    "Compare evidence for different hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bayes_factor` function is a powerful tool for **comparing the evidence** that supports one hypothesis versus another. Let me break it down:\n",
    "\n",
    "#### What Bayes Factors Do\n",
    "\n",
    "A Bayes factor quantifies how much more likely the observed data is under one hypothesis compared to another:\n",
    "\n",
    "$$BF_{12} = \\frac{P(\\text{data}|H_1)}{P(\\text{data}|H_2)}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- `BF > 1`: Evidence favors hypothesis 1\n",
    "- `BF < 1`: Evidence favors hypothesis 2  \n",
    "- `BF = 1`: Data is equally likely under both hypotheses\n",
    "- `BF = ∞`: Hypothesis 2 considers the data impossible\n",
    "- `BF = 0`: Hypothesis 1 considers the data impossible\n",
    "\n",
    "#### How the Function Works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def bayes_factor(likelihood1, # likelihood of hypothesis 1\n",
    "                 likelihood2, # likelihood of hypothesis 2\n",
    "                 data, # data to use for the calculation\n",
    "                 eps=1e-15 # epsilon for numerical stability\n",
    "                 ):\n",
    "    \"\"\"Calculate Bayes factor for hypothesis 1 vs 2 given data.\"\"\"\n",
    "    likelihood1 = np.asarray(likelihood1)\n",
    "    likelihood2 = np.asarray(likelihood2)\n",
    "    \n",
    "    # Validate inputs\n",
    "    if len(likelihood1) != len(likelihood2):\n",
    "        raise ValueError(\"Likelihood arrays must have same length\")\n",
    "    \n",
    "    # For single observation\n",
    "    if np.isscalar(data):\n",
    "        if likelihood2[data] == 0:\n",
    "            return np.inf if likelihood1[data] > 0 else np.nan\n",
    "        return likelihood1[data] / likelihood2[data]\n",
    "    \n",
    "    # For multiple observations (assuming independence)\n",
    "    # Use log-space computation for numerical stability\n",
    "    log_bf = 0.0\n",
    "    for obs in data:\n",
    "        if likelihood2[obs] == 0:\n",
    "            if likelihood1[obs] > 0:\n",
    "                return np.inf  # Decisive evidence for H1\n",
    "            else:\n",
    "                return np.nan  # Both hypotheses say impossible\n",
    "        \n",
    "        if likelihood1[obs] == 0:\n",
    "            return 0.0  # Decisive evidence for H2\n",
    "        \n",
    "        # Accumulate in log space\n",
    "        log_bf += np.log(likelihood1[obs]) - np.log(likelihood2[obs])\n",
    "    \n",
    "    return np.exp(log_bf)\n",
    "\n",
    "def interpret_bf(bf):\n",
    "    \"Interpret Bayes factor strength\"\n",
    "    if bf < 1/100:\n",
    "        return \"Decisive evidence against H1\"\n",
    "    elif bf < 1/10:\n",
    "        return \"Strong evidence against H1\"\n",
    "    elif bf < 1/3:\n",
    "        return \"Moderate evidence against H1\"\n",
    "    elif bf < 1:\n",
    "        return \"Weak evidence against H1\"\n",
    "    elif bf < 3:\n",
    "        return \"Weak evidence for H1\"\n",
    "    elif bf < 10:\n",
    "        return \"Moderate evidence for H1\"\n",
    "    elif bf < 100:\n",
    "        return \"Strong evidence for H1\"\n",
    "    else:\n",
    "        return \"Decisive evidence for H1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function handles two key scenarios:\n",
    "\n",
    "**Single Observation:**\n",
    "```python\n",
    "if np.isscalar(data):\n",
    "    return likelihood1[data] / likelihood2[data]\n",
    "```\n",
    "Simply divides the probability each hypothesis assigns to the observed data.\n",
    "\n",
    "**Multiple Observations:**\n",
    "For numerical stability with many observations, it uses **log-space computation**:\n",
    "```python\n",
    "log_bf += np.log(likelihood1[obs]) - np.log(likelihood2[obs])\n",
    "return np.exp(log_bf)\n",
    "```\n",
    "\n",
    "This prevents overflow/underflow when multiplying many small probabilities.\n",
    "\n",
    "#### Cyber Security Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(101250.00000000001)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two competing models of network behavior\n",
    "normal_model = [0.9, 0.08, 0.02]    # [normal, suspicious, attack]\n",
    "attack_model = [0.1, 0.3, 0.6]      # Attack-focused model\n",
    "\n",
    "# Observe sequence of mostly attack indicators\n",
    "observations = [2, 2, 1, 2]  # [attack, attack, suspicious, attack]\n",
    "\n",
    "bf = bayes_factor(attack_model, normal_model, observations)\n",
    "# Result: Large BF means attack_model explains data much better\n",
    "bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Robust Edge Case Handling\n",
    "\n",
    "The function handles critical edge cases that would crash simpler implementations:\n",
    "\n",
    "**Impossible Data Under H2:**\n",
    "```python\n",
    "if likelihood2[obs] == 0:\n",
    "    return np.inf  # Decisive evidence for H1\n",
    "```\n",
    "\n",
    "**Impossible Data Under H1:**\n",
    "```python\n",
    "if likelihood1[obs] == 0:\n",
    "    return 0.0  # Decisive evidence for H2\n",
    "```\n",
    "\n",
    "**Impossible Under Both:**\n",
    "```python\n",
    "return np.nan  # Neither hypothesis can explain the data\n",
    "```\n",
    "\n",
    "## Interpretation Helper\n",
    "\n",
    "The `interpret_bf` function provides human-readable interpretations:\n",
    "\n",
    "```python\n",
    "interpret_bf(4.5)   # \"Moderate evidence for H1\"\n",
    "interpret_bf(0.1)   # \"Strong evidence against H1\"\n",
    "interpret_bf(150)   # \"Decisive evidence for H1\"\n",
    "```\n",
    "\n",
    "## Why It's Crucial for RBE\n",
    "\n",
    "In your Recursive Bayesian Estimator for network anomaly detection:\n",
    "\n",
    "1. **Model Selection**: Compare different threat models to see which best explains current data\n",
    "2. **Anomaly Scoring**: Quantify how much more likely data is under \"attack\" vs \"normal\" hypotheses\n",
    "3. **Adaptive Thresholds**: Use BF strength to automatically adjust alert sensitivity\n",
    "4. **Forensic Analysis**: Provide quantitative evidence for security decisions\n",
    "\n",
    "The key advantage is that Bayes factors give us a **principled, quantitative measure** of evidence strength, rather than arbitrary scores or binary classifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Bayes factors\n",
    "# Basic functionality\n",
    "like1 = [0.9, 0.1]  # H1: mostly generates observation 0\n",
    "like2 = [0.2, 0.8]  # H2: mostly generates observation 1\n",
    "\n",
    "bf_single = bayes_factor(like1, like2, 0)\n",
    "test_close(bf_single, 4.5)  # 0.9/0.2\n",
    "\n",
    "bf_multiple = bayes_factor(like1, like2, [0, 0, 1])\n",
    "test_close(bf_multiple, 4.5 * 4.5 * 0.125)  # (0.9/0.2)^2 * (0.1/0.8)\n",
    "\n",
    "# Test edge cases that would crash the original\n",
    "\n",
    "# Case 1: H2 assigns zero probability to observed data\n",
    "like1_safe = [0.8, 0.15, 0.05]  # Normal traffic model\n",
    "like2_zero = [0.0, 0.3, 0.7]     # Attack model that never sees normal traffic\n",
    "\n",
    "bf_inf = bayes_factor(like1_safe, like2_zero, 0)  # Observe normal traffic\n",
    "assert bf_inf == np.inf, \"Should return infinity when H2 impossible\"\n",
    "\n",
    "bf_inf_multi = bayes_factor(like1_safe, like2_zero, [0, 1])  # Mixed observations\n",
    "assert bf_inf_multi == np.inf, \"Should return infinity if any observation impossible under H2\"\n",
    "\n",
    "# Case 2: H1 assigns zero probability to observed data  \n",
    "like1_zero = [0.0, 0.4, 0.6]     # Model that never sees normal traffic\n",
    "like2_safe = [0.7, 0.2, 0.1]     # Model that can see normal traffic\n",
    "\n",
    "bf_zero = bayes_factor(like1_zero, like2_safe, 0)  # Observe normal traffic\n",
    "test_close(bf_zero, 0.0)  # Decisive evidence for H2\n",
    "\n",
    "# Case 3: Both hypotheses assign zero probability (impossible data)\n",
    "like1_impossible = [0.0, 0.5, 0.5]\n",
    "like2_impossible = [0.0, 0.3, 0.7]\n",
    "\n",
    "bf_nan = bayes_factor(like1_impossible, like2_impossible, 0)\n",
    "assert np.isnan(bf_nan), \"Should return NaN when both hypotheses say impossible\"\n",
    "\n",
    "# Test numerical stability with many observations\n",
    "like1_stable = [0.6, 0.4]\n",
    "like2_stable = [0.4, 0.6]\n",
    "many_obs = [0] * 100 + [1] * 100  # 100 of each observation\n",
    "\n",
    "bf_stable = bayes_factor(like1_stable, like2_stable, many_obs)\n",
    "assert np.isfinite(bf_stable), \"Should handle many observations without overflow/underflow\"\n",
    "\n",
    "# Expected value: (0.6/0.4)^100 * (0.4/0.6)^100 = (1.5)^100 * (2/3)^100 = 1\n",
    "test_close(bf_stable, 1.0, eps=1e-10)\n",
    "\n",
    "# Test input validation\n",
    "try:\n",
    "    bayes_factor([0.5, 0.5], [0.3, 0.3, 0.4], 0)  # Mismatched lengths\n",
    "    assert False, \"Should raise ValueError for mismatched lengths\"\n",
    "except ValueError as e:\n",
    "    assert \"same length\" in str(e)\n",
    "\n",
    "# Test with extreme values (numerical stability)\n",
    "like1_extreme = [1e-100, 1-1e-100]\n",
    "like2_extreme = [1-1e-100, 1e-100]\n",
    "\n",
    "bf_extreme = bayes_factor(like1_extreme, like2_extreme, 0)\n",
    "assert np.isfinite(bf_extreme), \"Should handle extreme values\"\n",
    "assert bf_extreme < 1e-90, \"Should be very small but finite\"\n",
    "\n",
    "# Cyber security scenario test\n",
    "normal_model = [0.9, 0.08, 0.02]    # [normal, suspicious, attack]\n",
    "attack_model = [0.1, 0.3, 0.6]      # Attack-focused model\n",
    "\n",
    "# Sequence of observations suggesting attack\n",
    "attack_sequence = [2, 2, 1, 2, 1]   # Mostly attack/suspicious observations\n",
    "\n",
    "bf_attack = bayes_factor(attack_model, normal_model, attack_sequence)\n",
    "assert bf_attack > 1, \"Attack model should be favored for attack sequence\"\n",
    "\n",
    "# Normal sequence\n",
    "normal_sequence = [0, 0, 0, 1, 0]    # Mostly normal observations\n",
    "\n",
    "bf_normal = bayes_factor(attack_model, normal_model, normal_sequence)\n",
    "assert bf_normal < 1, \"Normal model should be favored for normal sequence\"\n",
    "\n",
    "# Test interpretation\n",
    "assert \"Strong evidence for\" in interpret_bf(50)\n",
    "assert \"Weak evidence against\" in interpret_bf(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Priors\n",
    "\n",
    "Helper functions for common conjugate prior-likelihood pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Priors\n",
    "\n",
    "**Conjugate priors** are a mathematical convenience in Bayesian inference where the prior and posterior distributions belong to the same family of probability distributions. This creates a beautiful mathematical symmetry that makes Bayesian updating much simpler.\n",
    "\n",
    "### The Mathematical Beauty\n",
    "\n",
    "When you have a conjugate prior-likelihood pair:\n",
    "- **Prior**: Some distribution family (e.g., Beta)\n",
    "- **Likelihood**: Compatible distribution (e.g., Binomial)\n",
    "- **Posterior**: Same family as prior (Beta again!)\n",
    "\n",
    "The key insight is that you can update your beliefs using **simple arithmetic** instead of complex integration.\n",
    "\n",
    "### Beta-Binomial: The Classic Example\n",
    "\n",
    "The `beta_binomial_update` function implements the most famous conjugate pair:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def beta_binomial_update(alpha, # prior alpha\n",
    "                         beta, # prior beta\n",
    "                         successes, # number of successes\n",
    "                         failures # number of failures\n",
    "                         ):\n",
    "    \"Update Beta prior with binomial data\"\n",
    "    return alpha + successes, beta + failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "\n",
    "**Prior Beliefs**: Beta(α, β) distribution represents your initial beliefs about a probability p\n",
    "- α represents \"pseudo-successes\" you've already seen\n",
    "- β represents \"pseudo-failures\" you've already seen\n",
    "- Your initial belief about p has mean α/(α+β)\n",
    "\n",
    "**New Data**: You observe some binomial data (successes and failures)\n",
    "\n",
    "**Posterior Update**: Simply add the new data to your prior parameters!\n",
    "- New α = old α + observed successes\n",
    "- New β = old β + observed failures\n",
    "\n",
    "### Cyber Security Example\n",
    "\n",
    "Imagine you're tracking the success rate of a particular attack method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial belief: uniform prior (no prior knowledge)\n",
    "alpha_prior, beta_prior = 1, 1  # Beta(1,1) = Uniform(0,1)\n",
    "\n",
    "# Observe attack attempts: 7 successes, 3 failures\n",
    "alpha_post, beta_post = beta_binomial_update(alpha_prior, beta_prior, 7, 3)\n",
    "# Result: Beta(8, 4) - attack success rate ≈ 8/(8+4) = 67%\n",
    "\n",
    "# More data arrives: 2 more successes, 5 more failures  \n",
    "alpha_final, beta_final = beta_binomial_update(alpha_post, beta_post, 2, 5)\n",
    "# Result: Beta(10, 9) - refined estimate ≈ 10/(10+9) = 53%\n",
    "alpha_final, beta_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why Conjugate Priors Are Powerful\n",
    "\n",
    "**1. Computational Efficiency**: No complex integrals - just arithmetic!\n",
    "\n",
    "**2. Interpretable Parameters**: α and β have clear meanings (pseudo-counts)\n",
    "\n",
    "**3. Sequential Learning**: Each update gives you a new prior for the next observation\n",
    "\n",
    "**4. Uncertainty Quantification**: The Beta distribution naturally captures uncertainty about the probability\n",
    "\n",
    "## Applications in Your RBE System\n",
    "\n",
    "**Threat Success Rates**: Track how often different attack types succeed\n",
    "```python\n",
    "# Each attack type gets its own Beta distribution\n",
    "malware_params = beta_binomial_update(1, 1, detected_malware, missed_malware)\n",
    "phishing_params = beta_binomial_update(1, 1, detected_phishing, missed_phishing)\n",
    "```\n",
    "\n",
    "**Sensor Reliability**: Model how often each sensor correctly identifies threats\n",
    "```python\n",
    "sensor_reliability = beta_binomial_update(5, 2, correct_alerts, false_alarms)\n",
    "```\n",
    "\n",
    "**Adaptive Thresholds**: Use the posterior distribution to set confidence-based thresholds\n",
    "\n",
    "## The Magic Formula\n",
    "\n",
    "For Beta-Binomial conjugacy:\n",
    "- **Prior**: Beta(α, β)\n",
    "- **Likelihood**: Binomial(n, p) with k successes\n",
    "- **Posterior**: Beta(α + k, β + n - k)\n",
    "\n",
    "This means your `beta_binomial_update` function is implementing one of the most fundamental results in Bayesian statistics - turning complex probability calculations into simple addition!\n",
    "\n",
    "The beauty is that this same pattern (prior parameters + data = posterior parameters) appears throughout Bayesian statistics with different conjugate pairs, making Bayesian learning both principled and computationally tractable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conjugate updates\n",
    "# Beta-Binomial\n",
    "alpha_post, beta_post = beta_binomial_update(1, 1, 7, 3)\n",
    "test_eq(alpha_post, 8)\n",
    "test_eq(beta_post, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal-Normal Conjugate Update\n",
    "\n",
    "The `normal_normal_update` function implements another classic conjugate prior-likelihood pair: **Normal prior with Normal likelihood**. This is particularly powerful for continuous parameter estimation in cyber security applications.\n",
    "\n",
    "## The Mathematical Framework\n",
    "\n",
    "When you have:\n",
    "- **Prior**: Normal(μ₀, σ₀²) - your initial belief about some parameter\n",
    "- **Likelihood**: Normal data with known variance - observations from that parameter\n",
    "- **Posterior**: Normal(μ₁, σ₁²) - updated belief after seeing data\n",
    "\n",
    "The magic is that the posterior is also Normal, and we can compute it using **precision weighting**.\n",
    "\n",
    "#### How the Function Works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def normal_normal_update(prior_mean, prior_var, data_mean, data_var, n_obs):\n",
    "    \"Update Normal prior with Normal likelihood\"\n",
    "    # Precision weighting\n",
    "    prior_prec = 1 / prior_var\n",
    "    data_prec = n_obs / data_var\n",
    "    \n",
    "    post_prec = prior_prec + data_prec\n",
    "    post_mean = (prior_prec * prior_mean + data_prec * data_mean) / post_prec\n",
    "    post_var = 1 / post_prec\n",
    "    \n",
    "    return post_mean, post_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Precision Weighting Insight\n",
    "\n",
    "The key insight is working with **precision** (1/variance) instead of variance:\n",
    "- **High precision** = low variance = high certainty\n",
    "- **Low precision** = high variance = high uncertainty\n",
    "\n",
    "The algorithm:\n",
    "1. **Convert to precisions**: More precise sources get more weight\n",
    "2. **Add precisions**: Total precision = prior precision + data precision  \n",
    "3. **Weight the means**: Final mean is precision-weighted average\n",
    "4. **Convert back**: Final variance = 1/total precision\n",
    "\n",
    "## Intuitive Understanding\n",
    "\n",
    "Think of it as **combining two sources of information**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95.07874015748031, 0.39370078740157477)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Estimating average response time\n",
    "prior_mean, prior_var = 100, 25    # Prior: 100ms ± 5ms (uncertain)\n",
    "data_mean, data_var = 95, 4        # Data: 95ms ± 2ms (10 observations)\n",
    "\n",
    "post_mean, post_var = normal_normal_update(100, 25, 95, 4, 10)\n",
    "# Result: ~96ms with much lower variance\n",
    "post_mean, post_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The data gets more weight because:\n",
    "- It has lower variance (more precise)\n",
    "- It has more observations (n_obs = 10)\n",
    "\n",
    "## Cyber Security Applications\n",
    "\n",
    "**1. Response Time Modeling**\n",
    "```python\n",
    "# Track system response times under different threat levels\n",
    "normal_response = normal_normal_update(50, 100, 48, 16, 20)    # Normal traffic\n",
    "attack_response = normal_normal_update(50, 100, 150, 400, 5)   # During attack\n",
    "```\n",
    "\n",
    "**2. Anomaly Score Calibration**\n",
    "```python\n",
    "# Calibrate anomaly detection thresholds\n",
    "baseline_score = normal_normal_update(0, 1, 0.1, 0.25, 100)   # Normal baseline\n",
    "current_score = normal_normal_update(*baseline_score, 2.5, 0.5, 10)  # Recent data\n",
    "```\n",
    "\n",
    "**3. Sensor Drift Detection**\n",
    "```python\n",
    "# Track how sensor readings change over time\n",
    "sensor_baseline = normal_normal_update(1000, 50, 995, 25, 50)  # Calibrated reading\n",
    "current_reading = normal_normal_update(*sensor_baseline, 1020, 30, 10)  # Recent drift\n",
    "```\n",
    "\n",
    "**4. Performance Metrics**\n",
    "```python\n",
    "# Model network latency under different conditions\n",
    "latency_model = normal_normal_update(10, 4, 12, 2, 25)  # ms latency\n",
    "```\n",
    "\n",
    "## Key Properties\n",
    "\n",
    "**Precision Accumulation**: Each update increases total precision (reduces uncertainty)\n",
    "```python\n",
    "# More data = more certainty\n",
    "assert post_var < min(prior_var, data_var/n_obs)\n",
    "```\n",
    "\n",
    "**Weighted Compromise**: The posterior mean balances prior belief and data\n",
    "```python\n",
    "# If data is much more precise, posterior approaches data_mean\n",
    "# If prior is much more precise, posterior stays near prior_mean\n",
    "```\n",
    "\n",
    "**Sequential Learning**: You can chain updates naturally\n",
    "```python\n",
    "# Day 1 update\n",
    "post1 = normal_normal_update(prior_mean, prior_var, day1_mean, day1_var, n1)\n",
    "\n",
    "# Day 2 update (using Day 1 posterior as new prior)\n",
    "post2 = normal_normal_update(*post1, day2_mean, day2_var, n2)\n",
    "```\n",
    "\n",
    "## The Test Case Breakdown\n",
    "\n",
    "```python\n",
    "post_mean, post_var = normal_normal_update(0, 1, 2, 0.5, 10)\n",
    "```\n",
    "\n",
    "- **Prior**: Mean=0, Var=1 (precision=1)\n",
    "- **Data**: Mean=2, Var=0.5, n=10 (precision=10/0.5=20)\n",
    "- **Result**: Data precision (20) >> Prior precision (1), so posterior ≈ data\n",
    "\n",
    "The posterior mean (≈1.9) is close to the data mean (2) because the data is much more precise than the prior. The posterior variance (≈0.048) is much smaller than either source alone because precisions add up.\n",
    "\n",
    "## Why This Matters for RBE\n",
    "\n",
    "In our Recursive Bayesian Estimator, this conjugate update allows us to:\n",
    "- **Efficiently track continuous parameters** (response times, scores, etc.)\n",
    "- **Quantify uncertainty** in your estimates\n",
    "- **Adapt to new data** without storing historical observations\n",
    "- **Balance prior knowledge with evidence** in a principled way\n",
    "\n",
    "The mathematical elegance is that complex Bayesian inference reduces to simple arithmetic with means and precisions!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal-Normal  \n",
    "post_mean, post_var = normal_normal_update(0, 1, 2, 0.5, 10)\n",
    "# Should be weighted toward data due to more observations\n",
    "assert 1.5 < post_mean < 2.0\n",
    "assert post_var < 0.5  # Should be more certain than either alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "__all__ = [\n",
    "    # Core updates\n",
    "    'update', 'sequential',\n",
    "    \n",
    "    # Posterior predictive\n",
    "    'predictive',\n",
    "    \n",
    "    # Model comparison\n",
    "    'bayes_factor', 'interpret_bf',\n",
    "    \n",
    "    # Conjugate priors\n",
    "    'beta_binomial_update', 'normal_normal_update'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
