{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "> Core probability utilities for RBE - normalization, sampling, entropy, and divergence measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rbe.probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from typing import Optional, Union, List\n",
    "from fastcore.test import test_eq, test_close\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations\n",
    "\n",
    "Core probability operations following fast.ai style - short names, clear purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def normalize(probs):\n",
    "    \"Normalize `probs` to sum to 1\"\n",
    "    probs = np.asarray(probs)\n",
    "    s = np.sum(probs)\n",
    "    if s == 0: raise ValueError(\"Cannot normalize zero probabilities\")\n",
    "    return probs / s\n",
    "\n",
    "def sample(probs, n=1, rng=None):\n",
    "    \"Sample `n` indices from `probs` distribution\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    probs = np.asarray(probs)\n",
    "    if np.any(probs < 0): raise ValueError(\"Probabilities must be non-negative\")\n",
    "    probs = normalize(probs)\n",
    "    return rng.choice(len(probs), size=n, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic operations\n",
    "probs = [1, 2, 3]\n",
    "normed = normalize(probs)\n",
    "test_close(np.sum(normed), 1.0)\n",
    "test_close(normed, [1/6, 2/6, 3/6])\n",
    "\n",
    "# Test sampling\n",
    "rng = np.random.default_rng(42)\n",
    "samples = sample([0.1, 0.7, 0.2], n=1000, rng=rng)\n",
    "assert len(samples) == 1000\n",
    "assert np.all((samples >= 0) & (samples <= 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Measures\n",
    "\n",
    "Entropy and divergence measures for quantifying uncertainty and comparing distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def entropy(probs, base=2):\n",
    "    \"Calculate entropy of `probs` distribution in given `base`\"\n",
    "    probs = normalize(probs)\n",
    "    probs = probs[probs > 0]  # Remove zeros to avoid log(0)\n",
    "    if base == 2:\n",
    "        return -np.sum(probs * np.log2(probs))\n",
    "    elif base == 'e':\n",
    "        return -np.sum(probs * np.log(probs))\n",
    "    else:\n",
    "        return -np.sum(probs * np.log(probs)) / np.log(base)\n",
    "\n",
    "def kl_div(p, q, eps=1e-10):\n",
    "    \"KL divergence from `q` to `p`\"\n",
    "    p, q = normalize(p), normalize(q)\n",
    "    # Add epsilon to avoid log(0)\n",
    "    return np.sum(p * np.log((p + eps) / (q + eps)))\n",
    "\n",
    "def js_div(p, q):\n",
    "    \"Jensen-Shannon divergence between `p` and `q`\"\n",
    "    p, q = normalize(p), normalize(q)\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test entropy\n",
    "uniform = [0.5, 0.5]\n",
    "certain = [1.0, 0.0]\n",
    "assert entropy(uniform) > entropy(certain)\n",
    "test_close(entropy(uniform), 1.0)  # Maximum entropy for 2 outcomes\n",
    "\n",
    "# Test KL divergence\n",
    "p = [0.5, 0.5]\n",
    "q = [0.5, 0.5]\n",
    "test_close(kl_div(p, q), 0.0, eps=1e-10)  # Same distributions\n",
    "\n",
    "# Test JS divergence (symmetric)\n",
    "test_close(js_div(p, q), js_div(q, p))  # Should be symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective Sample Size\n",
    "\n",
    "Measure of particle filter health - how many particles are effectively contributing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eff_size(weights):\n",
    "    \"Calculate effective sample size of normalized `weights`\"\n",
    "    weights = normalize(weights)\n",
    "    return 1.0 / np.sum(weights**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test effective sample size\n",
    "uniform_weights = np.ones(100) / 100\n",
    "skewed_weights = np.zeros(100)\n",
    "skewed_weights[0] = 1.0\n",
    "\n",
    "test_close(eff_size(uniform_weights), 100.0)  # All particles contribute\n",
    "test_close(eff_size(skewed_weights), 1.0)     # Only one particle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Distribution Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def categorical(probs, labels=None):\n",
    "    \"Create categorical distribution from `probs` with optional `labels`\"\n",
    "    probs = normalize(probs)\n",
    "    if labels is None:\n",
    "        labels = list(range(len(probs)))\n",
    "    return dict(zip(labels, probs))\n",
    "\n",
    "def uniform(n):\n",
    "    \"Create uniform distribution over `n` outcomes\"\n",
    "    return np.ones(n) / n\n",
    "\n",
    "def from_counts(counts):\n",
    "    \"Create probability distribution from `counts`\"\n",
    "    counts = np.asarray(counts)\n",
    "    if np.any(counts < 0):\n",
    "        raise ValueError(\"Counts must be non-negative\")\n",
    "    return normalize(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test categorical utilities\n",
    "cat_dist = categorical([1, 2, 3], ['A', 'B', 'C'])\n",
    "test_eq(cat_dist['A'], 1/6)\n",
    "test_eq(cat_dist['B'], 2/6)\n",
    "test_eq(cat_dist['C'], 3/6)\n",
    "\n",
    "# Test uniform\n",
    "u = uniform(4)\n",
    "test_close(u, [0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "# Test from_counts\n",
    "probs = from_counts([10, 20, 30])\n",
    "test_close(probs, [1/6, 2/6, 3/6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "__all__ = [\n",
    "    # Basic operations\n",
    "    'normalize', 'sample',\n",
    "    \n",
    "    # Information measures\n",
    "    'entropy', 'kl_div', 'js_div',\n",
    "    \n",
    "    # Effective sample size\n",
    "    'eff_size',\n",
    "    \n",
    "    # Categorical utilities\n",
    "    'categorical', 'uniform', 'from_counts'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
