{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBE Estimator\n",
    "\n",
    "> Main recursive Bayesian estimator implementations with performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rbe.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from typing import Optional, Callable, Dict, List, Tuple\n",
    "from fastcore.test import test_eq, test_close\n",
    "from fastcore.all import *\n",
    "from technical_blog.rbe.particle_filter import run as pf_run, init as pf_init, step as pf_step\n",
    "from technical_blog.rbe.probability import eff_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard RBE Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def estimate(observations, transition_fn, likelihood_fn, \n",
    "            n_particles=1000, init_fn=None, rng=None):\n",
    "    \"Main RBE estimator for `observations` with particle filter\"\n",
    "    return pf_run(observations, transition_fn, likelihood_fn,\n",
    "                  n_particles, init_fn, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic estimator\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Simple tracking problem\n",
    "def test_transition(particle, rng):\n",
    "    return particle + rng.normal(0, 0.05, particle.shape)\n",
    "\n",
    "def test_likelihood(particle, observation):\n",
    "    diff = np.linalg.norm(particle - observation)\n",
    "    return np.exp(-0.5 * (diff / 0.1)**2)\n",
    "\n",
    "# Generate test data\n",
    "true_states = [np.array([i * 0.1, i * 0.05]) for i in range(10)]\n",
    "observations = [state + rng.normal(0, 0.1, 2) for state in true_states]\n",
    "\n",
    "# Run estimator\n",
    "result = estimate(observations, test_transition, test_likelihood, \n",
    "                  n_particles=100, rng=rng)\n",
    "\n",
    "assert result['estimates'].shape == (10, 2)\n",
    "assert 'particles' in result\n",
    "assert 'weights' in result\n",
    "assert 'eff_sizes' in result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive RBE Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adaptive(observations, transition_fn, likelihood_fn, \n",
    "            adapt_fn=None, n_particles=1000, init_fn=None, rng=None):\n",
    "    \"Adaptive RBE estimator that adjusts parameters based on performance\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    if adapt_fn is None:\n",
    "        # Default adaptation: adjust resampling threshold based on ESS\n",
    "        def adapt_fn(ess_history, current_threshold):\n",
    "            avg_ess = np.mean(ess_history[-5:]) if len(ess_history) >= 5 else ess_history[-1]\n",
    "            if avg_ess < 0.3 * n_particles:\n",
    "                return min(0.9, current_threshold + 0.1)\n",
    "            elif avg_ess > 0.7 * n_particles:\n",
    "                return max(0.1, current_threshold - 0.1)\n",
    "            return current_threshold\n",
    "    \n",
    "    # Initialize\n",
    "    state_dim = len(observations[0]) if hasattr(observations[0], '__len__') else 1\n",
    "    particles, weights = pf_init(n_particles, state_dim, init_fn, rng)\n",
    "    \n",
    "    # Adaptive parameters\n",
    "    resample_threshold = 0.5\n",
    "    \n",
    "    # Storage\n",
    "    estimates = []\n",
    "    ess_history = [eff_size(weights)]\n",
    "    threshold_history = [resample_threshold]\n",
    "    \n",
    "    for obs in observations:\n",
    "        # Adapt threshold\n",
    "        resample_threshold = adapt_fn(ess_history, resample_threshold)\n",
    "        threshold_history.append(resample_threshold)\n",
    "        \n",
    "        # Standard step with adaptive threshold\n",
    "        particles, weights = pf_step(particles, weights, obs,\n",
    "                                    transition_fn, likelihood_fn,\n",
    "                                    resample_threshold, rng)\n",
    "        \n",
    "        # Record\n",
    "        estimate = np.average(particles, weights=weights, axis=0)\n",
    "        estimates.append(estimate)\n",
    "        ess_history.append(eff_size(weights))\n",
    "    \n",
    "    return {\n",
    "        'estimates': np.array(estimates),\n",
    "        'ess_history': np.array(ess_history),\n",
    "        'threshold_history': np.array(threshold_history),\n",
    "        'final_particles': particles,\n",
    "        'final_weights': weights\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test adaptive estimator\n",
    "adaptive_result = adaptive(observations, test_transition, test_likelihood,\n",
    "                          n_particles=100, rng=rng)\n",
    "\n",
    "assert adaptive_result['estimates'].shape == (10, 2)\n",
    "assert len(adaptive_result['ess_history']) == 11  # Initial + 10 steps\n",
    "assert len(adaptive_result['threshold_history']) == 11\n",
    "print(f\"Threshold adapted from {adaptive_result['threshold_history'][0]:.2f} to {adaptive_result['threshold_history'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def metrics(true_states, estimates):\n",
    "    \"Calculate performance metrics for RBE estimates\"\n",
    "    true_states = np.array(true_states)\n",
    "    estimates = np.array(estimates)\n",
    "    \n",
    "    # Ensure same shape\n",
    "    if true_states.shape != estimates.shape:\n",
    "        raise ValueError(f\"Shape mismatch: true {true_states.shape} vs estimates {estimates.shape}\")\n",
    "    \n",
    "    errors = true_states - estimates\n",
    "    \n",
    "    # Mean squared error\n",
    "    mse = np.mean(errors**2)\n",
    "    \n",
    "    # Root mean squared error\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Mean absolute error\n",
    "    mae = np.mean(np.abs(errors))\n",
    "    \n",
    "    # Maximum absolute error\n",
    "    max_ae = np.max(np.abs(errors))\n",
    "    \n",
    "    # Per-dimension metrics if multidimensional\n",
    "    if errors.ndim > 1:\n",
    "        dim_mse = np.mean(errors**2, axis=0)\n",
    "        dim_rmse = np.sqrt(dim_mse)\n",
    "        dim_mae = np.mean(np.abs(errors), axis=0)\n",
    "    else:\n",
    "        dim_mse = mse\n",
    "        dim_rmse = rmse\n",
    "        dim_mae = mae\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'max_absolute_error': max_ae,\n",
    "        'n_samples': len(estimates),\n",
    "        'dim_mse': dim_mse,\n",
    "        'dim_rmse': dim_rmse,\n",
    "        'dim_mae': dim_mae\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "perf_metrics = metrics(true_states, result['estimates'])\n",
    "assert 'mse' in perf_metrics\n",
    "assert 'rmse' in perf_metrics\n",
    "assert 'mae' in perf_metrics\n",
    "assert perf_metrics['n_samples'] == 10\n",
    "assert perf_metrics['rmse'] == np.sqrt(perf_metrics['mse'])\n",
    "\n",
    "print(f\"RMSE: {perf_metrics['rmse']:.4f}\")\n",
    "print(f\"MAE: {perf_metrics['mae']:.4f}\")\n",
    "print(f\"Per-dimension RMSE: {perf_metrics['dim_rmse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online vs Batch Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compare_online_batch(observations, transition_fn, likelihood_fn,\n",
    "                        n_particles=1000, rng=None):\n",
    "    \"Compare online (recursive) vs batch processing performance\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    # Online/recursive processing\n",
    "    online_result = estimate(observations, transition_fn, likelihood_fn,\n",
    "                            n_particles, rng=rng)\n",
    "    \n",
    "    # Batch processing (process all observations at once)\n",
    "    # For particle filter, this means running forward-backward smoothing\n",
    "    # Here we'll approximate with a simple batch mean for comparison\n",
    "    state_dim = len(observations[0]) if hasattr(observations[0], '__len__') else 1\n",
    "    particles, weights = pf_init(n_particles * 10, state_dim, rng=rng)  # More particles for batch\n",
    "    \n",
    "    # Apply all observations at once (simplified batch processing)\n",
    "    for obs in observations:\n",
    "        # Update weights based on all observations\n",
    "        for i, particle in enumerate(particles):\n",
    "            weights[i] *= likelihood_fn(particle, obs)\n",
    "    \n",
    "    weights = weights / np.sum(weights) if np.sum(weights) > 0 else np.ones_like(weights) / len(weights)\n",
    "    batch_estimate = np.average(particles, weights=weights, axis=0)\n",
    "    \n",
    "    # For fair comparison, repeat batch estimate for all time steps\n",
    "    batch_estimates = np.tile(batch_estimate, (len(observations), 1))\n",
    "    \n",
    "    return {\n",
    "        'online': online_result,\n",
    "        'batch': {\n",
    "            'estimates': batch_estimates,\n",
    "            'final_estimate': batch_estimate\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare online vs batch\n",
    "comparison = compare_online_batch(observations, test_transition, test_likelihood,\n",
    "                                 n_particles=50, rng=rng)\n",
    "\n",
    "online_metrics = metrics(true_states, comparison['online']['estimates'])\n",
    "batch_metrics = metrics(true_states, comparison['batch']['estimates'])\n",
    "\n",
    "print(f\"Online RMSE: {online_metrics['rmse']:.4f}\")\n",
    "print(f\"Batch RMSE: {batch_metrics['rmse']:.4f}\")\n",
    "print(f\"Online better by: {(batch_metrics['rmse'] - online_metrics['rmse']) / batch_metrics['rmse'] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model RBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def multi_model(observations, models, model_probs=None, n_particles=1000, rng=None):\n",
    "    \"RBE with multiple competing models\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    n_models = len(models)\n",
    "    \n",
    "    if model_probs is None:\n",
    "        model_probs = np.ones(n_models) / n_models\n",
    "    \n",
    "    # Run each model\n",
    "    model_results = []\n",
    "    model_likelihoods = []\n",
    "    \n",
    "    for model in models:\n",
    "        result = estimate(observations, model['transition'], model['likelihood'],\n",
    "                         n_particles // n_models, rng=rng)\n",
    "        model_results.append(result)\n",
    "        \n",
    "        # Compute model likelihood (simplified - product of weights)\n",
    "        likelihood = 1.0\n",
    "        for weights in result['weights'][1:]:  # Skip initial\n",
    "            likelihood *= np.mean(weights)  # Simplified\n",
    "        model_likelihoods.append(likelihood)\n",
    "    \n",
    "    # Update model probabilities\n",
    "    model_likelihoods = np.array(model_likelihoods)\n",
    "    model_probs = model_probs * model_likelihoods\n",
    "    model_probs = model_probs / np.sum(model_probs) if np.sum(model_probs) > 0 else np.ones(n_models) / n_models\n",
    "    \n",
    "    # Combine estimates using model probabilities\n",
    "    combined_estimates = np.zeros_like(model_results[0]['estimates'])\n",
    "    for i, (result, prob) in enumerate(zip(model_results, model_probs)):\n",
    "        combined_estimates += prob * result['estimates']\n",
    "    \n",
    "    return {\n",
    "        'estimates': combined_estimates,\n",
    "        'model_probs': model_probs,\n",
    "        'model_results': model_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-model estimator\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Low noise',\n",
    "        'transition': lambda x, rng: x + rng.normal(0, 0.01, x.shape),\n",
    "        'likelihood': test_likelihood\n",
    "    },\n",
    "    {\n",
    "        'name': 'High noise', \n",
    "        'transition': lambda x, rng: x + rng.normal(0, 0.1, x.shape),\n",
    "        'likelihood': test_likelihood\n",
    "    }\n",
    "]\n",
    "\n",
    "multi_result = multi_model(observations[:5], models, n_particles=100, rng=rng)\n",
    "assert multi_result['estimates'].shape == (5, 2)\n",
    "assert len(multi_result['model_probs']) == 2\n",
    "test_close(np.sum(multi_result['model_probs']), 1.0)\n",
    "print(f\"Model probabilities: {multi_result['model_probs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "__all__ = [\n",
    "    # Estimators\n",
    "    'estimate', 'adaptive', 'multi_model',\n",
    "    \n",
    "    # Analysis\n",
    "    'metrics', 'compare_online_batch'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
