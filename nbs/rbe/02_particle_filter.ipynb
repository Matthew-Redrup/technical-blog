{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filter\n",
    "\n",
    "> Monte Carlo methods for recursive Bayesian filtering in non-linear, non-Gaussian systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rbe.particle_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from typing import Optional, Callable, Tuple, List\n",
    "from fastcore.test import test_eq, test_close\n",
    "from fastcore.all import *\n",
    "from technical_blog.rbe.probability import normalize, sample, eff_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Particle Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def init(n_particles, state_dim, init_fn=None, rng=None):\n",
    "    \"Initialize particle filter with `n_particles` and `state_dim`\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    if init_fn is None:\n",
    "        # Default: uniform initialization in [0, 1]\n",
    "        particles = rng.uniform(0, 1, size=(n_particles, state_dim))\n",
    "    else:\n",
    "        particles = init_fn(n_particles, state_dim, rng)\n",
    "    \n",
    "    weights = np.ones(n_particles) / n_particles\n",
    "    return particles, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initialization\n",
    "rng = np.random.default_rng(42)\n",
    "particles, weights = init(100, 2, rng=rng)\n",
    "assert particles.shape == (100, 2)\n",
    "assert len(weights) == 100\n",
    "test_close(np.sum(weights), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Update Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict(particles, weights, transition_fn, rng=None):\n",
    "    \"Prediction step: apply `transition_fn` to `particles`\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    new_particles = np.zeros_like(particles)\n",
    "    for i, particle in enumerate(particles):\n",
    "        new_particles[i] = transition_fn(particle, rng)\n",
    "    \n",
    "    return new_particles, weights  # Weights unchanged in prediction\n",
    "\n",
    "def update(particles, weights, observation, likelihood_fn):\n",
    "    \"Update step: weight `particles` using `observation` and `likelihood_fn`\"\n",
    "    new_weights = np.zeros_like(weights)\n",
    "    \n",
    "    for i, particle in enumerate(particles):\n",
    "        new_weights[i] = weights[i] * likelihood_fn(particle, observation)\n",
    "    \n",
    "    # Normalize weights\n",
    "    if np.sum(new_weights) > 0:\n",
    "        new_weights = normalize(new_weights)\n",
    "    else:\n",
    "        # If all weights are zero, reset to uniform\n",
    "        new_weights = np.ones_like(weights) / len(weights)\n",
    "    \n",
    "    return particles, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predict and update\n",
    "def simple_transition(particle, rng):\n",
    "    return particle + rng.normal(0, 0.1, size=particle.shape)\n",
    "\n",
    "def simple_likelihood(particle, observation):\n",
    "    # Gaussian likelihood\n",
    "    diff = np.linalg.norm(particle - observation)\n",
    "    return np.exp(-0.5 * diff**2)\n",
    "\n",
    "# Test prediction\n",
    "new_particles, new_weights = predict(particles, weights, simple_transition, rng)\n",
    "assert new_particles.shape == particles.shape\n",
    "test_close(new_weights, weights)  # Weights unchanged\n",
    "\n",
    "# Test update\n",
    "observation = np.array([0.5, 0.5])\n",
    "particles, weights = update(particles, weights, observation, simple_likelihood)\n",
    "test_close(np.sum(weights), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def resample(particles, weights, method='systematic', rng=None):\n",
    "    \"Resample `particles` using `weights` with specified `method`\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    n_particles = len(particles)\n",
    "    \n",
    "    if method == 'systematic':\n",
    "        # Systematic resampling\n",
    "        positions = (np.arange(n_particles) + rng.uniform()) / n_particles\n",
    "        cum_weights = np.cumsum(weights)\n",
    "        indices = np.searchsorted(cum_weights, positions)\n",
    "    elif method == 'multinomial':\n",
    "        # Multinomial resampling\n",
    "        indices = sample(weights, n_particles, rng)\n",
    "    elif method == 'stratified':\n",
    "        # Stratified resampling\n",
    "        positions = (np.arange(n_particles) + rng.uniform(size=n_particles)) / n_particles\n",
    "        cum_weights = np.cumsum(weights)\n",
    "        indices = np.searchsorted(cum_weights, positions)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown resampling method: {method}\")\n",
    "    \n",
    "    new_particles = particles[indices]\n",
    "    new_weights = np.ones(n_particles) / n_particles\n",
    "    \n",
    "    return new_particles, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test resampling methods\n",
    "for method in ['systematic', 'multinomial', 'stratified']:\n",
    "    resampled_particles, resampled_weights = resample(particles, weights, method=method, rng=rng)\n",
    "    assert resampled_particles.shape == particles.shape\n",
    "    test_close(np.sum(resampled_weights), 1.0)\n",
    "    test_close(resampled_weights, np.ones(100)/100)  # Should be uniform after resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Particle Filter Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def step(particles, weights, observation, transition_fn, likelihood_fn, \n",
    "         resample_threshold=0.5, rng=None):\n",
    "    \"Complete particle filter step: predict, update, and conditionally resample\"\n",
    "    # Prediction\n",
    "    particles, weights = predict(particles, weights, transition_fn, rng)\n",
    "    \n",
    "    # Update\n",
    "    particles, weights = update(particles, weights, observation, likelihood_fn)\n",
    "    \n",
    "    # Conditional resampling\n",
    "    if eff_size(weights) < resample_threshold * len(particles):\n",
    "        particles, weights = resample(particles, weights, rng=rng)\n",
    "    \n",
    "    return particles, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete step\n",
    "particles, weights = init(100, 2, rng=rng)\n",
    "observation = np.array([0.5, 0.5])\n",
    "\n",
    "particles, weights = step(particles, weights, observation, \n",
    "                         simple_transition, simple_likelihood, rng=rng)\n",
    "\n",
    "assert particles.shape == (100, 2)\n",
    "test_close(np.sum(weights), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter Runner\n",
    "\n",
    "High-level interface for running particle filters on sequences of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run(observations, transition_fn, likelihood_fn, \n",
    "        n_particles=1000, init_fn=None, resample_threshold=0.5, rng=None):\n",
    "    \"Run particle filter on sequence of `observations`\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    \n",
    "    # Infer state dimension from first observation\n",
    "    state_dim = len(observations[0]) if hasattr(observations[0], '__len__') else 1\n",
    "    \n",
    "    # Initialize particles\n",
    "    particles, weights = init(n_particles, state_dim, init_fn, rng)\n",
    "    \n",
    "    # Store results\n",
    "    estimates = []\n",
    "    particle_history = [particles.copy()]\n",
    "    weight_history = [weights.copy()]\n",
    "    eff_sizes = [eff_size(weights)]\n",
    "    \n",
    "    for obs in observations:\n",
    "        # Particle filter step\n",
    "        particles, weights = step(particles, weights, obs, \n",
    "                                 transition_fn, likelihood_fn, \n",
    "                                 resample_threshold, rng)\n",
    "        \n",
    "        # Estimate (weighted mean)\n",
    "        estimate = np.average(particles, weights=weights, axis=0)\n",
    "        estimates.append(estimate)\n",
    "        \n",
    "        # Store history\n",
    "        particle_history.append(particles.copy())\n",
    "        weight_history.append(weights.copy())\n",
    "        eff_sizes.append(eff_size(weights))\n",
    "    \n",
    "    return {\n",
    "        'estimates': np.array(estimates),\n",
    "        'particles': particle_history,\n",
    "        'weights': weight_history,\n",
    "        'eff_sizes': np.array(eff_sizes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test particle filter runner\n",
    "# Generate synthetic data\n",
    "true_states = [np.array([i * 0.1, i * 0.05]) for i in range(10)]\n",
    "observations = [state + rng.normal(0, 0.1, 2) for state in true_states]\n",
    "\n",
    "# Run filter\n",
    "result = run(observations, simple_transition, simple_likelihood, \n",
    "             n_particles=100, rng=rng)\n",
    "\n",
    "assert result['estimates'].shape == (10, 2)\n",
    "assert len(result['particles']) == 11  # Initial + 10 steps\n",
    "assert len(result['weights']) == 11\n",
    "assert len(result['eff_sizes']) == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Particle Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def auxiliary_pf_step(particles, weights, observation, transition_fn, \n",
    "                     likelihood_fn, auxiliary_fn=None, rng=None):\n",
    "    \"Auxiliary particle filter step for improved proposal distribution\"\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    if auxiliary_fn is None:\n",
    "        auxiliary_fn = likelihood_fn  # Default to standard PF\n",
    "    \n",
    "    n_particles = len(particles)\n",
    "    \n",
    "    # Compute auxiliary weights\n",
    "    aux_weights = np.zeros(n_particles)\n",
    "    for i, particle in enumerate(particles):\n",
    "        # Look-ahead: what would the likelihood be after transition?\n",
    "        predicted = transition_fn(particle, rng)\n",
    "        aux_weights[i] = weights[i] * auxiliary_fn(predicted, observation)\n",
    "    \n",
    "    # Resample based on auxiliary weights\n",
    "    if np.sum(aux_weights) > 0:\n",
    "        aux_weights = normalize(aux_weights)\n",
    "        indices = sample(aux_weights, n_particles, rng)\n",
    "        particles = particles[indices]\n",
    "    \n",
    "    # Standard predict and update\n",
    "    return step(particles, np.ones(n_particles)/n_particles, observation,\n",
    "                transition_fn, likelihood_fn, resample_threshold=2.0, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test auxiliary particle filter\n",
    "particles, weights = init(100, 2, rng=rng)\n",
    "particles, weights = auxiliary_pf_step(particles, weights, observation,\n",
    "                                      simple_transition, simple_likelihood, rng=rng)\n",
    "assert particles.shape == (100, 2)\n",
    "test_close(np.sum(weights), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "__all__ = [\n",
    "    # Core operations\n",
    "    'init', 'predict', 'update', 'resample', 'step',\n",
    "    \n",
    "    # High-level interface\n",
    "    'run',\n",
    "    \n",
    "    # Advanced filters\n",
    "    'auxiliary_pf_step'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
